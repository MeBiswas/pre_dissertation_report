
I will start by extracting key content from the provided PDFs to inform the literature review and methodology sections. I‚Äôll use the file URLs you shared and parse their contents to identify relevant models, datasets, and findings to reference in your report.
{"file_content":"Global Transitions Proceedings 3 (2022) 31‚Äì37 Contents lists available at ScienceDirect Global Transitions Proceedings journal homepage: http://www.keaipublishing.com/en/journals/global-transitions-proceedings/ Exploratory analysis of credit card fraud detection using machine learning techniques M a J Madhurya , , H L Gururaj a ‚àó ,   B C Soundaryaa,   K P Vidyashreea\n\n### , A B Rajendraa\n\n\na Vidyavardhaka College of Engineering, Mysuru, India a r t i c l e i n f o a b s t r a c t Keywords: In today‚Äôs world, a lot of processes are carried over the Internet to make our lives easier. But, on the other Class imbalance hand, many unauthorized and illegitimate activities that take place over it are causing major trouble for the Data-driven model growth of the economy. One of them being the fraud cases that misguide people and lead to financial losses. Data prediction Major frauds reported recently occur through the malicious techniques that are made to work on Credit cards Illegitimate that are used for financial transactions over online platforms. Hence, it is the need of the hour to investigate this Malicious Vector problem. Several companies have started their study in this regard and have formulated data driven models that machine use various Machine Learning algorithms and models on datasets to analyse false activity. Several techniques used are Support Vector Machine, Gradient Boost, Random Forest and their mixtures. In this comparative study, the anomaly of class imbalance and ways to implement its solutions are analysed to prove certain results. The effectiveness of the algorithms varies on the set of data and the instance in which it is used. They prove that all algorithms despite of all the calculations show certain imbalance at some point in the study The limitations have also been evaluated and highlighted to help in future. In this study, it is found that although logistic regression had more accuracy but when the learning curves were plotted it signified that the majority of the algorithm under fit while KNN has the ability only to learn. Hence KNN is better classifier for the credit card fraud detection. 1 r a t d  a t  h b  f g  a 2 p  1 i o s  b t  I b  e t i  1 t e w  t s  m f  i u  F h A 2 B . Introduction The word ‚ÄòFraud‚Äô, it could be understood as the act of intentional eception and dishonesty intended towards personal gain. Now, with he Internet taking over our lives, many people and businesses have ecome the target for fraudulent activities. Several reports claim that rowth of commercial fraud attempts has risen in 2018 compared to\n\n016. Frauds in those years have unstripped each other by a whop-\ning 83%. The E-commerce Fraud Index has claimed that fraud rate n stores have risen from 0.06% in 2016 to 0.23% in 2017. 10% f all frauds are considered exclusively of Credit cards that have re- ulted in huge financial losses that worry companies. Since much of he transactions are digitized there has been an increase in the num- er of cards that are active, and their transaction data has been mul- iplying more than ever. Therefore, the amount of data to be exam- ned during the detection process has become voluminous. The main ools used by researchers are ML Algorithms, Neural Networking mod- ls, Classification and Clustering techniques. Many researchers are also orking on early or pre-detection of credit card frauds. Other research cholars have also investigated feasible and efficient methods and ways or Fraud detections. ML and other correlated approaches are usually sed, for example, ANN (Artificial Neural Network), the method of‚àó Corresponding author. E-mail address: gururaj1711@vvce.ac.in (H.L. Gururaj) . ttps://doi.org/10.1016/j.gltp.2022.04.006 vailable online 6 April 2022 666-285X/¬© 2022 The Authors. Publishing Services by Elsevier B.V. on behalf of Ke Y-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) ule induction system, Logistic Regression (LR), Decision Trees (DT), nd the Support Vector Machine etc. ML algorithms are AI techniques hat have the ability to solve various problems from diverse disciplines nd fields that usually possess large amounts of data. Though there ave been many ideas and solutions proposed to prevent and detect raud, there is still a big need to apply and analyze the strength of ML lgorithm. .1. Clustering Here, the large group of data is divided into smaller and similar ones ased on the similarities that they have in their nature and form clusters. tems in different clusters may not have the property of other cluster lements as pictorially represented in Fig. 1 . .2. Classification and Methods When certain values are given as an input from a huge set of data, hese algorithms find a common interest or a conclusion on its basis, it eans these methods try to extract one or more output from input that s given. ML algorithms are useful while performing these activities. The ig. 2 below shows the classifications applied in Machine Learning. Ai Communications Co. Ltd. This is an open access article under the CC\n\n\n\n---\n\nM.J. Madhurya, H.L. Gururaj, B.C. Soundarya et al. Global Transitions Proceedings 3 (2022) 31‚Äì37 Fig. 1. Clustering technique. Fig. 3. The flow process diagram for developing a machine learning model. b c d\n\n## L\n\n\nu f Fig.\n\n2. Classification methods of Machine Learning.\np a 1 a y m r  a m  a\n\n### v\n\n\nh  N o  p a  v a  t o  t s  t T  i p  e l  a o  e S  c e  [ e  u s  f\n\n### s\n\n\ne 2 a c m m  t s  L d  u u  a a  e s  c.3. Construction of references This is a supervised learning technique that finds out the output di- ectly depending on the input data that is given by the user. This is ostly used in continuous data that cannot have any particular discrete alues. ML algorithms are used in such predictions. The main aspect be- ind this paper is to analyze the performance, accuracy, and efficiency f various Machine Learning classifiers while using it on the prediction nalysis and preventive analysis of certain algorithms. Key factors such s additive techniques like oversampling, binary classification etc. It is bserved that traditional classifier algorithms of ML while paired with uch techniques give better results and increase efficiency of the process. he remaining part of the paper is organized and divided into sections ertaining to the research. In Section 2 , there is a summary on the re- ated works of the problem. In Section 3 , the architecture and method- logical study with details is given by inferring various classifiers. In ection 4 , the results along with graphical representations have been stablished to show the analyzed test results and their values. In the nd, in section 5 , we draw some conclusions and throw light on future cope of the problem and its limitations to be focused upon in further tudies. . Related work Credit card transactions are either classified as fraudulent or legiti- ate transactions and are mainly a binary classification problem. Ba- ically, data mining classification comprises problems such as Fraud etection that is used to figure out credit card transactions as fraud- lent or legitimate. Some Additional techniques and factor methods part from data mining which are involved in fraud detection are Web- ervices based collaborative schemes in which the private bodies like32 anks can share the information about the fraud patterns and frequen- ies for the enhancement of the fraud detection capability and to re- uce the financial loss. The basic procedure in developing any Machine earning model [1] is given below in Fig. 3 . Numerous Machine Learning techniques have been developed and sed in the various experimental studies to approach the problem of raud detection in Credit cards. In the [1] , a data driven approach to set up fraud alerts has been roposed that runs on select features like Oversampling under sizes nd SMOTE technique. A few authors in their studies [2] have come cross the comparison among various models and their resultant anal- sis, like XGBOOST, Random Forest, Decision Trees etc. These are the ost widely used techniques so far in detection of frauds. There has lso been a study of new techniques like Adaboost and Majority Voting pproaches that add or enhance the ML algorithm performance. [ 3 , 4 ] Feature Selection (FS) with optimization used in Artificial Neural etwork (ANN) for the selection of the required features while the im- lementation of the algorithms has been seen [5] . When more than one alid parameter is present, it becomes important to select the best effec- ive feature. Since most models are irrelevant in transaction sequencing, hey cannot learn by information at a single level, hence a new struc- ured sequenced learning ensemble classifier that improves performance s also seen [ 6 , 7 ]. In another paper [8] by S. Venkata Suryanarayana t al. Many classifications and their metrics and performance have been nalysed. This gives an idea as to how many metrics can be consid- red while finding a proper algorithm. Adaptive features selection pro- esses by comparative study of 5 different techniques can be analysed 9] , these adaptive features make it easier to bifurcate and eliminate the nimportant ones. The figure below shows the accuracy and precision ound in [ 10 , 11 ]. The organized complete study on the application of Random For- st has been discussed by Priya Gupta et al in the paper that in detail nalyses the key factors of RF and its limitations [12] . Also, and the con- ept of real time deep learning and binary data classification by various ethods has been discussed in the paper cited [13] which compara- ively analyses these techniques. A strong and new tool of bidirectional ong short-term memory (BiLSTM) and bidirectional Gated recurrent nit (BiGRU) is looked upon by Hassan Najadat and others who have lso applied various other six strategies like Ada Boost etc [14] . to help nhance the performance [15] . The Fig. 4 represents the normal credit ard transaction.\n\n\n\n---\n\nM.J. Madhurya, H.L. Gururaj, B.C. Soundarya et al. Global Transitions Proceedings 3 (2022) 31‚Äì37 Fig. 4. Working of a credit card fraud detection model [5] .\n\n### Fig. 5. Na√Øve Bayes algorithm.\n\n\no o  fi t R f v  g h  a a D\n\n### o 3\n\n\ni\n\n## N\n\n\n### g\n\n\n### c t\n\n\n### P i\n\n\n### ( a\n\n\n### M i\n\n\n### p T\n\n\n### d p\n\n\n### i\n\n\n### h o\n\n\n### i o\n\n\n### s E\n\n\nc  ùëÉ M ‚àï =\n\n## 3 =\n\n\np  i c  q  The Novel strategies that effectively address the skew distribution f data are assessed by ML technique along with the implementation f the API (Application Programming Interface) module to decide if a ransaction is fraudulent or legitimate are studied [ 16 , 17 ]. In the study by Aadhya Kaul and others it is seen it that compares the arious techniques for the most suitable method, a certain ML criterion as been used on the dataset and F1-score of those are calculated to nalyse it, [18] and Naive Bayes is introduced in arbitrary classification. ata processing techniques and their abilities are compared with those f the Machine Learning techniques and the prediction is done based on\n\nt. [19] also discusses the supervised based classification of dataset using\normalization and Principal element analysis and is seen 95% accuracy. The hybrid approach that is made of combining the strength and redibility of three sub-methods [20] , the GridSearchCV for Hyper- arameters Optimization (HPO), the Recursive Feature Elimination RFE) for the selection of useful predictive features, and the Synthetic inority Oversampling (SMOTE) to overcome the imbalanced or dispro- ortionate data problem is also proposed [21] . An auto-encoder based eep learning technique and restricted Boltzmann Machine (RBM) are mplemented in hidden layers to find patterns [22] and anomalies in the uge set of data has been proposed by Apapan Pumsiriratin and others n their paper for the detection processes. The results show the mean quared error and area under the normal curve. This paper strictly fo- used on the comparison between various classifiers and techniques in achine Learning and their performance accuracy. . Discussions Fraud detection in simple words is a simple binary classification roblem in which any particular transaction or exchange will be either lassified as fraud or legit only. In this study, a few standard classi-33 cation techniques like Naive Bayes, K-Nearest Neighbor and Logistic egression methods, Random Forest Classifiers, Decision Trees. For ef- ective usage of these algorithms, different stages are included such as athering the data, cleaning data, researching and visualization of data nd training the classifier algorithms and finally evaluating the result. .1. Na√Øve Bayes algorithm It is a theory based on two assumptions. Initially, all features in a iven entry that needs to be bifurcated contribute equally. Secondly, all he given attributes are statistically independent of each other, which mplies that the values that attribute present do not show us any points bout the attributes. This might not be true in all cases and Bayes rule s used in such a situation to find out if it‚Äôs either fraudulent or legit. he class that is associated with the higher probability is taken as the redicted class for instance. Refer Fig. 5 . Naive Bayes has a base of restrictive independence among the vari- us characters present in the dataset and the resultant classifier is based n restrictive probabilities of the matching options and is provided in\n\nq. (1) .\n[ ùê∂ ( i )| ùëì ( k )]  √ó ùëÉ [ ( i )] ùëÉ [ ùëì ( k )] ( i) ùëÉ [ ùëì ( k )| ùê∂ ( i )] Œ† ( ùëÉ [ ùëì ( k )| ùê∂ ( i )] 1) ùëò 1 ..., ùëõ ‚à∂ ùëñ = 1 , 2 etc. Here, unknown probabilities are identified only with the already ex- sting known values by making use of Bayesian concepts which will re- uire prior knowledge to ensure logics. The flowchart given below in\n\n\n\n---\n\nM.J. Madhurya, H.L. Gururaj, B.C. Soundarya et al. Global Transitions Proceedings 3 (2022) 31‚Äì37 Fig. 6. Structure of a decision tree. F i\n\n## 3\n\n\nc\n\n### s Fig. 7. KNN flowchart.\n\n\no o s\n\n### t\n\n\nt p a c m t\n\n## 3\n\n\ni\n\n### o Fig. 8. Fold validation process.\n\n\ni a 3 t\n\n### a\n\n\na w f c d [\n\n### l\n\n\nl\n\n### t\n\n\n### o\n\n\n## F\n\n\n\nig. 5 [23] represents the structural working of this algorithm with var- ous predictions. .2. Decision tree algorithm In this method, there are two types of namely regression trees and lassification trees. Here, a training dataset is used to construct a deci- ion tree. This decision tree has separate nodes that form the structure f the tree, the topmost node of the tree is known as the root node. The ther non-leaf nodes represent the test done on the attribute, each re- ulting branch denotes the outcome of the test and each leaf node on he tree denotes a class label. These leaf nodes also show the classes that are returned if reached as he final prediction by the model. So, one can find out the prediction by roperly traversing through the decision tree. A few of the decision tree lgorithms include the C4.5, CART, ID3. This algorithm manages the onstant set of data and uses the divide and solve approach to solve the ain problem into subproblems through its repeated usage. The struc- ure of a Decision Tree is as follows in Fig. 6 [12] . .3. K-NN algorithm This works on a simple logic that it plots all the existing training nstances and later classifies the unlabelled instance based on the idea f their nearest neighbour present. Unlike the decision trees, here the nstances are directly used to analyse. But it is also known that here, ll existing algorithms are already instance-based as they are built on raining models. Here, in this case, the unlabelled instance is classified nd divided based on calculating the distances between the instances nd by using the metric. The one which has the majority class is labelled or unlabelled class. Let us visualize how this algorithm works in steps epicted in Fig. 7 .\n\n- First, load from the data and then assign a certain value of K into the\nrequired group of neighbours.\n\n- Secondly, we find the Euclidean distance between the test data set\nand the training data set. This will make the instance into a well- organized collection.\n\n- Now, we must sort that ordered collection of indices and then order\nthem in ascending order of the distance.\n\n- The value of K is now initialized and the labels for chosen K entries.\n- In the end, the values of mean and mode for K labels for classificationand regression.\n34 .4. Random forest classifiers This algorithm is nothing but a Bayes classifier of Random Forest hich is actually a simple implementation of Decision trees [12] . It ould also be considered as a part of the Logistic Regression process 6] . It is a new approach to ensemble-tree based algorithms and is a earning algorithm. It is mainly selected from a series of randomly se- ected subsets of the taken training set for the experiment. So, as the rail goes on, objects of the class are decided upon the votes of all the ther trees present in it. Let us understand the working of the Random orest classifier algorithm in steps and depicted in Fig. 8 .\n\n- Before any of the test processes, the Python libraries are imported,\nand the required data set is loaded into the data frame.\n\n- Now, that data is segregated into train and test dataset accordingly.\n- The Random Forest Regression model is later applied on the training\ndata that exists.\n\n\n\n---\n\nM.J. Madhurya, H.L. Gururaj, B.C. Soundarya et al. Global Transitions Proceedings 3 (2022) 31‚Äì37\n\n### Table 1\n\n\nF1 score table. Method Accuracy F1-Score 3 Logistic Regression 78.83 .015 Decision Tree 72.66 0.369 Random Forest Classifier 80.16 0.446 [  Na√Øve Bayes (Gaussian) 61.5 0.443\n\n### r Na√Øve Bayes (Bernoulli) 75.83 0.491\n\n\n### 1 K- Neighbour‚Äôs Classifier 72.5 0.239\n\n\n## ANN-DL 77.63 0.44\n\nn m p m a p a 3\n\n## S\n\n\no\n\n### w\n\n\nt  c a  d o  t t  l a f  S m\n\n## 4\n\n\n### p\n\n\n### l o\n\n\nm A\n\n### n\n\n\n4 i a t d  o c  m c  h b t 4\n\n## 4\n\n\na f  m a  f r  d T  p\n\n### f l\n\n\n4 f r t n  d t t  D t  e t  n [  f p a 1 s\n\n- After these procedures, the test outcome is gauged and predicted,\nand the suitable confusion matrix is made. .5. Support vector machine SVMs are particularly depended on the structural risk minimization 6 , 12 ] unlike the other neural networks which focus on the empirical isk minimization. This technique was brought into light by Vapnik in 992[12] to debug and solve only the binary classification problem, but ow it is extended towards the non-linear regression also. These SVMs ap a certain data to a pre-existing very high dimensional space via a articular kernel function and thereby finds the hyperplane that maxi- izes the margin between any two classes. The Solution SVM problems re based on those data points that exist particularly at the margin. Such oints are called support vectors [6] . .6. Artificial neural network Artificial neural networks have been in existence and developed vertime by many scientists which are inspired by biological neural net- orks (neurons). These networks are then utilized to and approximate hose functions/factors that can be largely determined by an input that re unknown. An artificial neural network is constructed in the nodes ne by one in a stacked manner. They are stacked between a coating of he target vector and the feature vector [12] . ANN is developed to have similar kind of interpretation just like our human brain that learns rom various activities. [6] ANN has been successfully implemented in any of the model is seen. . Result and discussions This paper is aimed at looking for a suitable algorithm to tackle the arge amounts of data that are taken as the input for a fraud detection odel and therefore a factual comparison of the Machine Learning tech- iques has been done on a credit card dataset considered. .1. F1 strategy To estimate the following models and evaluate their results, the ataset is broken into training and testing data. Let us investigate one onclusion drawn on the hold of accuracy and F1-score which was cal- ulated using the confusion matrix. Now, let us gauge the algorithms ased on the other three parameters to get a better understanding of heir efficiency discussed in Section 4.4 . .2. Data analysis and pre-processing The raw dataset taken for the study was sorted and pre-processed or the sole intention of improving the performance of the classifiers nd reducing their training and operating time. If not, the data would equire lots of time in between to sorted based on their common features. he pre-processing also includes the work of investigating the dataset eature space and handling the imbalance nature of the dataset [6] . .3. Performance metrics Many of the parameters can be used while comparing all the tech- iques and to report their performances including the confusion set ma- rix, Sensitivity, Specificity, False positive rate and balanced classifica- ion rate or even the Matthews Correlation coefficient. A confusion ma- rix is a table showing all the possible instances or the no. of instances hat are classified correctly/ incorrectly in each of the prescribed classes. 6 , 12 ] Table represents the confusion matrix of a binary classifier. In the roblem of fraud detection, positive means the legitimate transactions nd negative represents the fraudulent transactions. The three parameters discussed here are 35 ‚Ä¢ Specificity: It is considered as the number of frauds that get predicted into the ctual total number of fraud cases as described in equation (2) . pecif icity = TN‚àï(  TN + FP)  (2)\n\n- Sensitivity:\nThis is understood as the number or the count of legit predictions ompared to the sum/total number of legit transactions. But, in fraud etection, the most significant feature is the specificity or fraud detec- ion rate. It is taken as having a value of recall means a lowest financial oss to the company as shown in equation (3) . ensit ivit y = TP‚àï(  TP + FN)  (3)\n\n- Accuracy:\nThis is a parameter which gives the overall accuracy [6] of the pro- osed system. It gives the total number of predictions to the total number f cases considered as shown in Equation (4) . ccuracy =(  TP + TN)  ‚àï ( TP + TN + FP + FN)  (4) Sometimes the correctness of the model can be very misleading as n case of Credit card frauds as the number of fraudulent transactions re less compared to the total sum in whole. This makes the dataset otally imbalanced. Also, selecting the right metric depends on the goal r business objective that we are looking for. Sometimes one strategy ay help one achieve customer satisfaction while others might have a igher ability to prevent the financial losses. .4. Experimental results For our understanding and convenience let us take up four models nd train and test them using Weka which stands for ‚ÄúWaikato Environ- ent and Knowledge Analysis ‚Äù. [6] It is a workbench, or a platform used or Machine Learning that has the capacity to implement many of the ata mining techniques. It can also help apply the various early and pre- rocessing and sampling techniques. Weka was developed in the Java anguage in New Zealand by the University of Waikato. [ 6 , 12 ] In this experiment we have used 0-fold,5-folds,10-folds,15-folds, 20- olds cross validation processes. It is made so to ensure the equal rep- esentation of all data as training and test data. Then, the average of hese responses is taken to find out the result of a specific parameter as epicted in Fig. 8 . Now, coming to the dataset, Table 2 shows the output result of the ecision Tree algorithm. Table 3 represents the performance of K near- st neighbour while Table 4 illustrates the performance of the Neural etwork algorithm. And finally, we have Table 5 representing the per- ormance of the Logistic Regression. Now, let us analyse the graphical representation of the values with 5 folds and 20 folds and compare the result depicted in Figs. 9 & 1 re- pectively.\n\n\n\n---\n\nM.J. Madhurya, H.L. Gururaj, B.C. Soundarya et al. Global Transitions Proceedings 3 (2022) 31‚Äì37 Table 2 . This table gives the recorded values of accuracy, sensitivity and speci- ficity at various folded experiments using the Decision Tree algorithm. 2.Results of Decision Trees across different fraud rates Folds used Accuracy measured Sensitivity found Specificity found 0 0.94119706 94.03% 76% 5 0.94293 94.68% 71% 10 0.94476 94.70% 74% 15 0.9417 94.94% 65% 20 0.94776 94.77% 75% Table 3 This table interprets the recorded values of accuracy, sensitivity and specificity at various folded experiments using the K-Nearest Neighbour algorithm.\n\n3. Results of K-Nearest Neighbour across different fraud rates\nFolds used Accuracy measured Sensitivity found Specificity found 0 0.958897945 96.50% 76.90% 5 0.95838 96.70% 74.44% 10 0.95838 96.70% 74.40% 15 0.9585 96.71% 74.50% Fig. 10. Graph 2 with 20 folds. 20 0.95838 96.70% 74.44% Table 4 This table represents the recorded values of accuracy, sensitivity and 5 specificity at various folded experiments using the Neural Network algo-\n\n### rithm.\n\n\n### 4.Results of Neural Network across different fraud rates e\n\n\nb Folds used Accuracy measured Sensitivity found Specificity found d 0 0.769688 99.30% 69.70% a 5 0.84369 97.70% 44.70% o 10 0.93995 95.70% 68.50%\n\n### 15 0.9372 96.40% 56.31% u\n\n\n### 20 0.92813 96.61% 54.33% c\n\n\nc Table 5 w This table accounts the recorded values of accuracy, sensitivity and speci- t ficity at various folded experiments using the Logistic Regression algo-\n\n### rithm. a\n\n\nl Results of Logistic Regression across different fraud rates t Folds used Accuracy measured Sensitivity found Specificity found a\n\nt 0 0.963198 96.90% 82.00%\n\n### 5 0.96238 96.80% 80.49% r\n\n\n10 0.963198 96.80% 80.40% u 15 0.9624 96.85% 80.62% m 20 0.96238 96.85% 80.49% r\n\n### d\n\n\n\n\n\n## A\n\n\nt\n\n## R\n\n\n\n\n\nFig. 9. Graph 1 with 15 folds. 36 . Conclusion Though the above used techniques and algorithms are proven to be fficient and accurate while being implemented individually or in com- inations, there could be a chance of having dissimilar and irregular ataset. Hence, some pre-processing and sampling algorithms must be dded to the raw dataset to classify it before being subjected to any f the Random Forest, Linear Regression, AdaBoost or ANN techniques sed to analyse it. Failure of such models could happen mainly because redit card transactions are very confidential and hence one must take are while developing a model such that it doesn‚Äôt leak any key data hile in the process. Also, the total number of frauds is always lesser han the total data considered hence the variation of the subset is high nd may vary at different instances causing a model to fail in a particu- ar situation. The real limitation of any Machine Learning Algorithm is he fact that real-time actions must take place. Or while using predictive nalytic methods, the algorithm must be adaptive in nature in order to ackle the imbalance or variations found in huge datasets from various esources. So, there doesn‚Äôt exist any data mining technique that can be niversally better in all cases and therefore studies in the future work ust look upon these factors in order to increase the prediction accu- acy. The limitation of this paper are as follows on which work can be one in future:\n\n1. The performance of other machine learning algorithm can be\nchecked for credit card fraud detection.\n\n2. The accuracy of XG Boost, Random Forest etc. machine learning al-\ngorithm should also be tested more subjectively on other data sets for credit card fraud detection.\n\n3. The performance of other algorithms can also be tested on other data\nsets of different domains and varied patterns. cknowledgments No financial support was received to perform the research work of his manuscript. eferences [1] Vinod Jain, Mayank Agrawal, Anuj Kumar, ‚ÄúPerformance Analysis of Machine Learn- ing Algorithms in Credit Cards Fraud Detection, 2020 at 8th International Confer- ence on Reliability, Infocom Technologies and Optimization (Trends and Future Di- rections) (ICRITO), 2022 . [2] Roopesh Akula in ‚ÄúFraud identification of credit card using ML techniques at, Int. J. Comput. Artif. Intell. 1 (2) (2020) 31‚Äì33 .\n\n\n\n---\n\nM.J. Madhurya, H.L. Gururaj, B.C. Soundarya et al. Global Transitions Proceedings 3 (2022) 31‚Äì37\n\n## [\n\n\n\n## [\n\n\n## [\n\n\n\n\n## [\n\n\n\n## [\n\n\n\n## [\n\n\n\n## [\n\n\n## [  [\n\n\n\n[\n\n## [\n\n\n## [\n\n\n[\n\n## [\n\n\n[3] A. Krishnaiah, P.B. Divakarachari, Automatic Music Mood Classification using Mul- ti-class Support Vector Machine based on Hybrid Spectral Features, 2022 . [4] Gurumurthy Krishnamurthy Arun‚àó , Kaliyappan Venkatachalapathy, Intelligent fea- ture selection with social spider optimisation based Artificial Neural Network Model for Credit card Fraud detection, in: | Arun & Venkatachalapathy, 11, IIOABJ, 2020, pp. 85‚Äì91. |||| . [5] T.G. Nguyen, T.V. Phan, D.T. Hoang, T.N. Nguyen, C. So-In, Efficient SDN-based traf- fic monitoring in IoT networks with double deep Q-network, in: International confer- ence on computational data and social networks, Springer, Cham, 2020, pp. 26‚Äì38 . [6] Xurui Li, Wei Yu, Tianyu Luwang Jianbin Zheng, Xuetao Qiu, Jintao Zhao, Lei Xia Yujiao Li in ‚Äú Transaction Fraud detection using GRU-Centered Sandwich-structured Model at, in: Proceedings of the 2018 IEEE 22nd International Conference on Com- puter Supported Cooperative Work in Design, 2022 . [7] K. Yu, L. Lin, M. Alazab, L. Tan, B. Gu, Deep learning-based traffic safety solution for a mixture of autonomous and manual vehicles in a 5G-enabled intelligent trans- portation system, IEEE Trans. Intell. Transp. Syst. 22 (7) (2020) 4337‚Äì4347 . [8] S. Venkata Suryanarayana ‚àó , G.N. Balaji, G. Venkateswara Rao in ‚ÄúMachine learning approaches for credit card fraud detection ‚Äù at, Int. J. Eng. Technol. 7 (2) (2018)\n\n## 917‚Äì920 .\n\n[9] Ajeet Singh, Anurag Jain, Adaptive Credit Card Fraud Detection Techniques Based on Feature Selection Method, University Grants Commission (UGC), Delhi, India, 2022 in fellowship research problem at . 10] Vaishnave Jonnalagadda, Priya Gupta, Eesita Sen in ‚ÄúCredit card fraud detection using Random Forest Algorithm ‚Äù in Jonnalagadda Vaishnave et al.; International Journal of Advance Research, Ideas and Innovations in Technology, Volume 5, Issue 2. 11] B.D. Parameshachari, K.M. Keerthi, T.R. Kruthika, A. Melvina, R. Pallavi, K.S. Poonam, Intelligent Human Free Sewage Alerting and Monitoring System, in: 3rd International Conference on Integrated Intelligent Computing Communication & Security (ICIIC 2021), Atlantis Press, 2021, pp. 480‚Äì486 . 12] Youness Abakarim, Mohamed Lahby, Abdelbaki Attioui, ‚ÄúAn Efficient Real Time Model For Credit Card Fraud Detection Based On Deep Learning ‚Äù, SITA‚Äô18, 2018 October 24‚Äì25Rabat, Morocco . 37 13] Hassan Najadat, et al., Credit Card Fraud Detection Based on Machine and Deep Learning, 2020 11th International Conference on Information and Communication Systems (ICICS), 2022 . 14] R.K. Dash, T.N. Nguyen, K. Cengiz, A. Sharma, Fine-tuned support vector regression model for stock predictions, Neural Comput. Appl. (2021) 1‚Äì15 . 15] Anuruddha Thennakoon, Chee Bhagyani, Sasitha Premadasa, Shalitha Mihiranga, Nuwan Kuruwitaarachchi in ‚ÄúReal-time Credit Card Fraud Detection Using Machine Learning, 9th International Conference on Cloud Computing, Data Science & Engi- neering (Confluence), 2022 . 16] Z. Guo, K. Yu, A. Jolfaei, A.K. Bashir, A.O. Almagrabi, N. Kumar, Fuzzy detection system for rumors through explainable adaptive learning, IEEE Trans. Fuzzy Syst. 29 (12) (2021) 3650‚Äì3664 . 17] Aadhya Kaula, Mihika Chhabraa, Prachi Sachdevaa, Rachna Jaina, Preeti Nagratha in ‚ÄúCredit Card Fraud Detection Using Different ML and DL Techniques ‚Äù in their research works. 18] G.B. Rajendran, U.M. Kumarasamy, C. Zarro, P.B. Divakarachari, S.L. Ullo, Land-use and land-cover classification using a human group-based particle swarm optimiza- tion algorithm with an LSTM Classifier on hybrid pre-processing remote-sensing im- ages, Remote Sensing 12 (24) (2020) 4135 . 19] Hamzah Ali Shukur, Sefer Kurnaz in, Credit card fraud detection using machine learning methodology, Int. J. Comput. Sci. Mob. Comput. 8 (3) (2019) 257‚Äì260 Vol.Issuepg . 20] Z. Guo, K. Yu, Y. Li, G. Srivastava, J.C.W. Lin, Deep learning-embedded social inter- net of things for ambiguity-aware social recommendations, IEEE Trans. Netw. Sci. Eng. (2021) . 21] Naoufal Rtayli‚àó  , Nourddine Enneya in ‚ÄúEnhanced credit card fraud detection based on SVM-recursive feature elimination and hyper-parameters optimization, J. Inf. Se- cur. Appl. 55 (2020) 102596 . 22] D.L. Vu, T.K. Nguyen, T.V. Nguyen, T.N. Nguyen, F. Massacci, P.H. Phung, A convo- lutional transformation network for malware classification, in: 2019 6th NAFOSTED conference on information and computer science (NICS), IEEE, 2019, pp. 234‚Äì239 . 23] Apapan Pumsirirat, Liu Yan, Credit card fraud detection using deep learning based on auto-encoder and restricted boltzmann machine ‚Äù in (IJACSA), Int. J. Adv. Com- put. Sci. Appl. 9 (1) (2018) .","knowledge_id":"79dc8203-1df2-4e2c-ac7e-3e38a35d3fc5","title":"1-s2_0-S2666285X22000425-main.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/27c81383-6aac-4750-afc0-1c3f8e72e364/1-s2_0-S2666285X22000425-main.pdf"}
{"file_content":"Alexandria Engineering Journal 104 (2024) 171‚Äì192 A 1 B Contents lists available at ScienceDirect Alexandria Engineering Journal journal homepage: www.elsevier.com/locate/aej Original article Credit card fraud detection using the brown bear optimization algorithm Shaymaa E. Sorour a,b,‚àó, Khalied M. AlBarrak a, Amr A. Abohany c, Amr A. Abd El-Mageed d a Department of Management Information Systems, School of Business, King Faisal University, Al-Ahsa 31982, Saudi Arabia b Faculty of Specific Education, Kafrelsheikh University, 33511, Egypt c Faculty of Computers and Information, Kafrelsheikh University, Kafrelsheikh, 33511, Egypt d Department of Information Systems, Sohag University, Sohag 82511, Egypt\n\n### A R T I C L E I N F O A B S T R A C T\n\nKeywords: Fraud detection in banking systems is crucial for financial stability, customer protection, reputation man- E-commerce agement, and regulatory compliance. Machine Learning (ML) is vital in improving data analysis, real-time Online transaction fraud fraud detection, and developing fraud techniques by learning from data and adjusting detection strategies Fraud Detection (FD) accordingly. Feature Selection (FS) is essential for enhancing fraud detection through ML to achieve optimal Feature Selection (FS) Brown Bear optimization (BBO) algorithm model accuracy. This is because it helps to eliminate the negative impact of redundant and irrelevant attributes. Meta-heuristics To enhance the accuracy of the given dataset, the researchers utilized multiple methods to determine the most fitting features. However, it is important to note that when implementing these methods on datasets with larger feature sizes, they may encounter issues with local optimality. Despite this, the researchers continue to work on improving the effectiveness of these methods. This study presents an effective methodology based on the Brown-Bear Optimization (BBO) algorithm to enhance the capacity to accurately identify financial CCF transactions by recognizing pertinent features. BBO has balanced capabilities to reduce dimensionality while enhancing classification accuracy. It is improved by adjusting the positions randomly to enhance exploration and exploitation capabilities, and then it is cloned into a binary variant named Binary BBOA (BBBOA). The Support Vector Machine (SVM), k-nearest Neighbor (ùëò-NN), and Xgb-tree are the ML classifiers used with the suggested methodology. On the Australian credit dataset, the proposed methodology is compared with the basic BBOA and ten current optimizers, such as Binary African Vultures Optimization (BAVO), Binary Salp Swarm Algorithm (BSSA), Binary Atom Search Optimization (BASO), Binary Henry Gas Solubility Optimization (BHGSO), Binary Harris Hawks Optimization (BHHO), Binary Bat Algorithm (BBA), Binary Particle Swarm Optimization (BPSO), Binary Grasshopper Optimization Algorithm (BGOA), and Binary Sailfish Optimizer (BSFO). Regarding Wilcoxon‚Äôs rank-sum test (ùõº = 0.05), the superiority and effective consequence of the presented methodology are clear on the utilized dataset and got an accuracy of classification up to 91% in the utilized dataset combined with an attribute reduction length down to 67%. The proposed methodology is further validated using 10 benchmark datasets and outperformed its competitors in most utilized datasets regarding different performance measures. In the end, the proposed methodology is further validated using ten benchmark datasets from the UCI repository. It outperformed its competitors in most of the utilized datasets regarding different performance measures.1. Introduction adaptation to market fluctuations, and various payment methods. These aspects have notably contributed to economic resilience, particularly Electronic Commerce (E-commerce) denotes the digital transaction during pandemic-induced governmental restrictions and mandates for of goods or services. This advancement in conventional business frame- home confinement, such as those experienced during pandemic periods works has facilitated home-based purchasing options for consumers [1, (COVID-19) [3,4]. According to eMarketer, global E-commerce sales 2]. E-commerce platforms provide multiple benefits, including acceler- grew by 27.6% in 2020 and were predicted to increase by 14.3% in ated procurement procedures, decreased expenditures, enhanced con- 2021, reaching nearly 5 trillion.1 sumer convenience, facilitation of product and price comparisons, rapid ‚àó Correspondence to: King Faisal University, Saudi Arabia. E-mail addresses: ssorour@kfu.edu.sa (S.E. Sorour), kalbarrak@kfu.edu.sa (K.M. AlBarrak), amrabohany8@gmail.com (A.A. Abohany), amr.atef@commerce.sohag.edu.eg (A.A.A. El-Mageed). 1 https://www.insiderintelligence.com/content/worldwide-ecommerce-will-approach-5-trillion-this-year.https://doi.org/10.1016/j.aej.2024.06.040 Received 24 December 2023; Received in revised form 13 May 2024; Accepted 13 vailable online 24 June 2024 110-0168/¬© 2024 The Authors. Published by Elsevier B.V. on behalf of Faculty of E Y-NC-ND license (h ttp://creativecommons.org/licenses/by-nc-nd/4.0/ ). June 2024 ngineering, Alexandria University. This is an open access article under the CC\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 Fig. 1. The growth of E-commerce revenues [5].Fig. 1 demonstrates the continuous rise in E-commerce revenue. Be- fore the 2008‚Äì2009 financial crisis, the annual growth in E-commerce revenues accelerated from 15% to 25% but slowed to about 3% in 2009 due to the economic downturn. This growth rate was signifi- cantly more stable than the decline experienced in traditional retail commerce during the same period. After 2009, E-commerce growth rates rebounded, increasing at over 10% annually, a rate substantially higher than total retail sales. The COVID-19 pandemic in 2020, while negatively affecting digital travel sales, also led to a significant increase in retail E-commerce revenue, prompted by a consumer shift to online shopping, a trend that has continued post-pandemic [5]. The persistent COVID-19 pandemic has increased online demand for various products, including essential items. This has also heightened the utilization of electronic payment methods and triggered a notable rise in fraudulent financial activities [6]. Given the evolution of E- commerce, nearly all businesses, whether small or large-scale, have adopted credit card payments. However, this surge in credit card usage, particularly through online transactions, has provided criminals with new opportunities to misappropriate consumer credit card information. With its broad implications, financial fraud poses a severe chal- lenge within academic, commercial, and regulatory domains, detrimen- tally impacting service providers and their customers. This issue is of paramount concern within the financial arena, infiltrating everyday economic interactions on a global scale. It entails the unauthorized appropriation of assets or capital for self-enrichment, which dimin- ishes trust in fiscal institutions and increases the cost of living. The categorization of financial fraud is multifaceted, encompassing the adulteration of fiscal statements, deceit in insurance practices, banking subterfuge, telecommunications fraud, and fraudulent activities in the stocks and commodities markets [7,8]. The ripple effects of such fraud- ulent conduct have precipitated a significant upheaval in the global financial infrastructure, accelerating the transition toward digitized financial services and ushering in a new era of challenges. Reports indicate a sharp rise in fraud losses associated with credit and debit cards, growing exponentially from 2000 to 2015. Research by Saia and Carta (2019) [9] highlights that unauthorized purchases and fake credit cards, although constituting only 10%‚Äì15% of total fraud cases, are responsible for a staggering 75%‚Äì80% of the financial losses due to fraud. As a result, private and public sectors are ramping up their investments in research and development to create increasingly sophisticated systems for detecting fraud. The substantial financial flow172 in E-commerce attracts fraudulent activities, posing a risk of significant financial losses. Juniper Research reported an increase in fraud from $ 17.5 billion in 2020 to $ 20 billion in 2021. These figures underscore the necessity for E-commerce and financial entities to develop fraud detection and prevention mechanisms [10]. Fraud constitutes an illegitimate act characterized by deception. Credit card fraud (CCF) entails unlawfully acquiring a cardholder‚Äôs information via telephone communication, text messaging, or cyber hacking to facilitate unauthorized financial transactions. Such fraud- ulent acts are often conducted using software under the perpetrator‚Äôs control. The procedure to identify CCF commences when a consumer executes a transaction using their credentials. This transaction must be validated to confirm its legitimacy [10‚Äì12]. Efforts in research are geared toward developing detection systems that leverage methodologies like Machine Learning (ML), Deep Learn- ing (DL), and Data Mining (DM). These systems scrutinize transactional data to differentiate between authentic and fraudulent activities. The complexity of detecting CCF is increasing as the characteristics of fraud- ulent transactions become more akin to legitimate ones. Consequently, credit card companies are compelled to implement more advanced fraud detection technologies. An efficient fraud detection system is essential for accurately identifying and detecting fraudulent activities in real-time transactions, broadly categorized into anomaly detection and misuse detection systems [11]. Financial institutions that issue credit cards or manage online trans- actions must use automated fraud detection systems. This reduces losses while increasing customer trust. With the rise of big data and artificial intelligence, new possibilities for using advanced ML models to detect fraud have emerged [13]. Today‚Äôs fraud detection systems, which rely on advanced DM, ML, and DL methods, are extremely effective. A binary classification model that can distinguish between normal and fraudulent transactions is built using a data set that includes labeled transactions (normal and fraudulent). The developed model determines whether incoming transactions are legitimate or fraudulent. Detecting fraudulent transactions using classification techniques presents many challenges [14,15]. Automated systems for detecting fraud are essential for financial entities involved in issuing credit cards or handling online transactions, as they help minimize losses and bolster customer confidence. The emergence of big data and artificial intelligence has opened up fresh possibilities in employing sophisticated ML models for fraud detection.\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 Fig. 2. Fraud detection in E-Commerce transactions.According to Bao et al. (2022) [13], the latest fraud detection systems leveraging advanced DM, ML, and DL techniques have proven highly effective. The current research investigates CCF, a significant subset of bank- ing fraud. Credit cards, a dominant electronic payment method world- wide, have simplified online transactions and increased fraudulent activities by cybercriminals. Unauthorized use of credit card systems or information, often without the owner‚Äôs knowledge, is a growing concern, impacting numerous banks and financial entities globally. Detection of CCF involves automated analysis of transactional data stored in service provider databases. Common fraud types include application and transaction fraud, spanning physical and digital realms. Categories of fraud encompass card-not-present situations, counterfeit cards, and identity theft. Fig. 2 delineates the step-by-step progression of online financial transactions within an e-commerce framework. This diagram identifies critical points where fraud prevention and detection mechanisms are integrated into the consumer‚Äôs experience. Banks play a pivotal role in scrutinizing transactions for fraud before payment authorization, notably by checking if the involved website is on a block list. Transactions linked to blocked sites are rejected as a fraud prevention measure. After bank approval, the process transitions from fraud prevention to a focus solely on fraud detection. Preventive measures for application fraud include validation of per- sonal details and detection methods like communal and spike detection. Tools like address verification services, card verification value, 3D security, and encryption are used for transaction fraud. The significance of dimensional attributes in datasets is a pressing issue because they173 can obstruct the effectiveness of classification procedures, potentially causing overfitting or ambiguity in algorithms [16]. FS, a vital preprocessing phase, addresses this issue. This stage entails identifying and eliminating extraneous attributes that adversely affect the performance of the ML model. FS aims to reduce the dimen- sionality of feature sets while preserving performance accuracy. Vari- ous methods have been developed for dataset classification, but Meta- Heuristic have garnered significant attention for their effectiveness in addressing a wide range of optimization problems [17]. MHOA are optimization techniques designed to find approximate solutions for various optimization problems. These algorithms are derivative-free, showcasing ease of use, adaptability, and the ability to circumvent local optima. MHOAs operate stochastically, initiating their optimization process with randomly generated solutions, unlike gradi- ent search methods that require the computation of derivatives in the search space. Their simplicity and straightforwardness, stemming from basic concepts and easy implementation, make these algorithms highly flexible and easily modifiable to suit specific problems. A key charac- teristic of MHOAs is their exceptional capacity to prevent premature convergence. Their stochastic nature allows them to function effectively as a ‚Äòblack box‚Äô, skirting local optima and thoroughly exploring the search space [18‚Äì23]. Utilizing MHOAs to handle FS issues can effectively address var- ious challenges in data analysis. FS is crucial for extracting relevant features from unbalanced datasets, particularly before classifying CCF cases in extensive datasets. The primary benefits of FS include sim- plified data interpretation, reduced training duration, and resolving\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 1 t b a q X s s s 2 v c i t a a t M r c a t I i a n f issues related to high dimensionality. Bio-inspired algorithms, adept at tackling complex and combinatorial problems, have been applied effec- tively in detecting CCF. Notable among these is the binary brown bear optimization (BBO) algorithm, a renowned bio-inspired optimization method known for its dual capability of local and global search through exploration and exploitation techniques. This paper employs the BBO algorithm due to its comprehensive search characteristics and provides efficient solutions to computational problems. As a result, extensive research has been conducted to detect finan- cial fraud, yielding a plethora of proposed methodologies, as exten- sively cataloged in the literature (e.g., referenced in Table 2). These methods are predominantly ML, DM, and DL techniques. This study aims to scrutinize the limitations inherent in existing class methodolo- gies, focusing on ML techniques. An empirical investigation utilizing the BBO algorithm, integrated with FS, has been carried out to address the classification dilemma. The aim is to unearth and address the pivotal issues required to develop an efficient and accurate solution for the class conundrum. 1.1. Motivations To solve the challenges of FS in supervised learning, researchers propose a binary version of the BBO algorithm, which is a novel meta- heuristic developed by Prakash et al. [18] and is designed to handle continuous optimization problems based on natural inspirations. The BBO algorithm draws inspiration from the natural behaviors of brown bears, particularly their ability to discriminate between the smell of a pedal and to sniff. The proposed methodology addresses FS problems in supervised learning by considering each brown bear‚Äôs unique pedal smell differentiation behavior. This behavior‚Äôs main fea- tures include distinct walking styles, cautious stepping, and twisting feet on ground depressions. Additionally, brown bears exhibit sniffing pedal differentiation behavior, which can be leveraged for exploration. The BBO algorithm utilizes these behaviors for the exploitation and exploration phases. The proposed algorithm was verified on the Aus- tralian credit dataset, and significant modifications were demonstrated. This study presents a new strategy for optimization that could improve the accuracy and efficiency of supervised learning classification with a minimization of the number of selected features. The BBO algorithm recommended in this study has distinct ad- vantages that differentiate it from other meta-heuristics and validate its implementation. Firstly, the initial configuration of the BBO al- gorithm is unique, requiring only two hyperparameters: population size and stopping criteria. It eliminates the need for extra parameters, making applying to various optimization problems convenient. This is especially significant for meta-heuristic as coefficients can signifi- cantly impact algorithm performance, and adapting them for various applications can be time-consuming. Moreover, the BBO algorithm depends on the existing best solutions of the brown bears‚Äô group and the information generated by other group members during updates. This feature distinguishes it from most other meta-heuristic, which only depend on existing best solutions, enhancing its overall performance. The Australian credit dataset from the UCI-ML repository was used to determine the algorithm‚Äôs effectiveness in selecting essential at- tributes. Three standard ML classifiers, SVM, ùëò-NN, and Xgb-tree, were employed to calculate the average classification accuracy through 30 separate algorithm executions. 1.2. Contributions The proposed approach offers the following significant contribu- tions that highlight its uniqueness:174 1. An effective methodology based on the BBBO algorithm and machine learning techniques, namely SVM, ùëò-NN, and Xgb-tree, is suggested in this paper for improving the ability to pre- cisely detect financial CCF transactions occurring during card- holder transactions by identifying relevant features from the provided credit dataset. By focusing on the most pertinent in- formation for FD and reducing the dimensionality of the data, this methodology enhances the detection model‚Äôs effectiveness.\n\n2. The introduced methodology relies on the BBO algorithm, which\nis a novel meta-heuristic that no author in any literature has applied before to identify CCF transactions. As a result, its capacity to address this problem has not been investigated.\n\n3. For boosting the methodology‚Äôs efficiency by generating a new\npopulation that maintains the main structure but has more ap- propriate positions and avoids falling into a local optimum, the BBO is integrated with an efficient technique called adjusting the positions randomly.\n\n4. A comparison study has been conducted between the suggested\nmethodology and other recent methodologies over the Aus- tralian credit dataset using a variety of evaluation metrics, including mean fitness value, classification accuracy, the se- lected number of features, precision, recall, F-score, ROC_AUC, Kappa, specificity, and MCC.\n\n5. The suggested methodology is characterized from other FD\nmethodologies by its reliance on three well-known ML classifiers to accurately classify, namely SVM, ùëò-NN, and Xgb-tree, which confirm its dominance compared to its counterparts using a Wilcoxon‚Äôs non-parametric rank-sum test (5% significance level).\n\n6. The practical experiments prove the excellence of the proposed\nmethodology and its ability to produce fewer attributes chosen while efficiently enhancing the classification accuracy. Addi- tionally, it is employed to decrease misclassified financial CCF transactions and maximize correctly classified CCF transactions. .3. Structure The rest of the paper is structured as follows. Section 2 presents he current studies that handle fraud detection. Section 3 introduces a rief exposition of the original BBO algorithm; Section 3 elaborates and nalyzes the proposed BBBO algorithm to tackle FS tasks, as well as a uick explanation of the two utilized ML classifiers: SVM, ùëò-NN, and gb-tree; Section 4 shows the empirical results and comparisons with ome recent approaches, and the conclusion as well as issues for future tudy in Section 5. The abbreviations used throughout this paper are ummarized in Table 1. . Related work In recent years, numerous articles have been published, and sur- eys and reviews of existing FD and prevention strategies have been onducted in the literature. This section delineates various scholarly nvestigations focused on FD. Particular attention is accorded to studies hat have explored FD within the context of class challenges. A diverse rray of methodologies is employed in uncovering fraudulent financial ctivities. Key methodologies can be classified into distinct categories to horoughly examine the most pertinent literature in this field, including L, Cross-Channel Fraud (CCF) detection strategies, ensemble, feature anking methods, and user authentication techniques. Zojaji et al. [24] ategorized credit card FD methods into two main types, supervised nd unsupervised, and offered a detailed taxonomy of the various echniques identified in the literature, emphasizing these categories. t additionally examined the datasets used in each approach, though t did not propose any specific methodology. Conversely, Adewumi nd Akinyelu [25] conducted a survey on online credit card FD using ature-inspired and ML techniques. Their survey covered literature rom 1997 to 2016 and did not elaborate on the methodology used\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 Table 1 Table of abbreviations. Abbreviation Nomenclature ùê¥ùê¥ùëÅùëÅ Auto Associative Neural Network ùê¥ùëÅùëÅ Artificial Neural Network ùê¥ùê∑ùê¥ùëÜùëåùëÅ Adaptive Synthetic Sampling ùê¥ùê∂ùê∂ Accuracy ùê¥ùê∂ùëÇ Ant Colony Optimization ùêµùêµùëÇ Brown-Bear Optimization ùêµùêµùêµùëÇ Binary Brown-Bear Optimization ùêµùê¥ùëâ ùëÇ Binary African vultures ùêµùëÜùëÜùê¥ Binary Salp Swarm Algorithm ùêµùê¥ùëÜùëÇ Binary Atom Search Optimization ùêµùêªùê∫ùëÜùëÇ Binary Henry Gas Solubility Optimization ùêµùêªùêªùëÇ Binary Harris Hawks Optimization ùêµùêµùê¥ Binary Bat Algorithm ùêµùëÉùëÜùëÇ Binary Particle Swarm Optimization ùêµùê∫ùëÇùê¥ Binary Grasshopper Optimization Algorithm ùêµùëÜùêπùëÇ Binary Sailfish Optimizer ùê∂ùê∂ùêπ Credit Card Fraud ùê∂ùëÇùê¥ Cuckoo Optimization Algorithm ùê∂ùëÜ Cost-Sensitive ùê∑ùê∂ùëÅùëÅ Deep 1D-Convolutional Neural Network ùê∑ùê∏ Differential Evolution ùê∑ùëÄ Data Mining ùê∑ùêø Deep Learning ùêπùê∑ Fraud Detection ùêπùëÜ Feature Selection ùêπùëÖùëÅùëÅ Fuzzy Rough Nearest Neighbors ùêπùêø Federated Learning ùê∫ùëÅùëÅùë† Graph Neural Networks ùê∫ùê¥ùêπùëÜ Genetic Algorithm Feature Selection ùêºùëÖ Information Retrieval ùëñùëìùëúùëüùëíùë†ùë° I solution Forest ùêøùëÜùëáùëÄ Long Short-Term Memory ùêøùëÇùêπ Local Outlier Factor ùêøùëÖ Logistic Regression ùëÄùêª Meta-Heuristic ùëÄùêø Machine Learning ùëÅùêµ Na√Øve Bayes ùëÅùêøùëÉ Natural Language Process ùëÄùêøùëÉ Multi-Layer Perceptron ùëÜùëâùëÄ Support Vector Machine ùëÇùê∂ùê∂ ONE-Class Classification ùëÇùê∂ùê∂ùëÜùëâùëÄ One-Class Classification SVM ùëÉ Precision ùëÉùêºùê∑ Proportional‚ÄìIntegral‚ÄìDerivative ùëÖ Recall ùëÖùêπ Random Forest ùëÖùëÇ Random Oversampling ùëÖùêπùê∂ Random Forest Classifier ùëÖùëÇùëÜ Random Over Sampling ùëÜùëÄùëÇ Sequential Minimal optimization ùëÜùëÇùëÄ Self Organising Map ùëÜùëÄùëÇùëáùê∏ Synthetic Minority Oversampling Technique ùëÜùëÜùê¥ Salp Swarm Algorithm ùëäùëÇùê¥ Whale Optimization Algorithm but summarized key techniques and algorithms prevalent from 2010 to 2015. Similarly, Chilaka et al. [26] explored credit card FD methods in the electronic finance and banking sector, selecting relevant articles from 2014 to 2019 and outlining the strategies used. Their focus was on solutions prioritizing rapid response and efficiency, but their review was not systematic and lacked a taxonomy. Aziz and Ghous [27] explored various DM techniques tailored for detecting CCF, focusing on multiple ML strategies such as RF, SVM, Hybrid Methods (HM), DT, and DL. These methods were utilized to identify typical consumer behavior patterns based on past activities. Their review of ML techniques highlighted significant inconsistencies among different studies and suggested areas for future research. Singh et al. [7] proposed a CCF detection model that combines two sequential levels of an SVM with an optimization algorithm inspired by the firefly bio-inspired. The subset of features was optimized in the first level using the firefly algorithm and the CfsSubsetEval feature section 175 method, and the training model for detecting CCF cases was constructed in the second level using the SVM classifier. The model successfully classified 591 transactions with an accuracy of 85.65%. Nguyen et al. [49] suggested a DL method based on convolutional neural networks and Long Short-Term Memory (LSTM) for detecting CCF on European, Small, and Tall cards across three distinct financial datasets. Sampling techniques were used in the study to address the issue of class imbalance, which resulted in a dramatic reduction in performance on newly unseen data but an improvement on existing examples. The suggested approaches can be used to identify CCF in real-world scenarios, as evidenced by experimental results showing that the suggested DL methods perform better at detecting CCF than conventional ML models. With an F1 score of 84.85%, the LSTM with 50 blocks emerged as the best algorithm when all of the algorithms were compared. For employing various FS algorithms to improve accuracy out- comes, Khushaba et al. [50] introduced a Differential Evolution (DE) optimization method for FS. This method‚Äôs effectiveness is evaluated against Genetic Algorithms (GA) and Particle Swarm Optimization (PSO). Ahmed et al. [51] detailed the application of FS in detecting intrusions within wireless sensor networks, employing PSO and PCA space, and compared its performance with GA. AL et al. [52] designed an approach to efficiently navigate the exploration and exploitation phases in various optimization challenges. [37] elaborated on an en- hanced credit card risk identification method that utilized algorithms, including DF and SVM, for fraud risk detection. Misra et al. [53] and Schl√∂r et al. [54] implemented under-sampling techniques to create balanced datasets for training DL-based detection models. A notable limitation of under-sampling is its tendency to omit useful data instances from the training set, which could adversely affect detection accuracy. On the other hand, isolation methods have been employed to estimate the data distribution and develop a generative model using various mixture components. This approach to outlier detection has been effectively used in fraud detection, as demonstrated by Buschj√§ger et al. [55]. Nonetheless, there is a noticeable gap in the literature regarding thoroughly evaluating the latest ML techniques that utilize under-sampling to address class imbalance issues. The potential of hybrid semi-supervised methodologies, which combine supervised learning with unsupervised outlier detection, has not been fully explored. Moreover, the assessment of fraud detection efficiency in mobile payment systems has been limited to standard performance metrics, overlooking the financial impact of such detections. Hajek et al. [56] aimed to develop a fraud detection system based on XGBoost that also considers such systems‚Äô financial impacts. This system was rigorously tested on a dataset exceeding 6 million mo- bile transactions. To assess the effectiveness of the proposed model, they compared it with existing ML techniques tailored for handling imbalanced data and detecting outliers. The findings indicated that the semi-supervised ensemble model, which integrates various un- supervised outlier detection algorithms with an XGBoost classifier, outperformed others regarding standard classification metrics. More- over, combining random under-sampling with the XGBoost method yielded the most significant cost savings. Krim et al. [57] characterized an auto-encoder as a specific type of neural network capable of similarly encoding and decoding data. This technique involves training autoencoders specifically on non- anomalous data points. The method relies on assessing reconstruction errors to determine anomalies, subsequently categorizing them as either ‚Äòfraud‚Äô or ‚Äôno fraud‚Äô. This implies that when encountering un- trained scenarios, the system is likely to identify a higher incidence of anomalies [2]. A nominal increase above the upper threshold value is often flagged as anomalous. This approach has also been applied where an auto-encoder-based network is used for anomaly detection. In the context of ML, a generative adversarial network (GAN) involves two neural networks working in tandem to enhance their predictive capa- bilities. GANs, primarily unsupervised, evolve through a cooperative zero-sum game strategy.\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 Table 2 Comparison of the related work. Ref Year Dataset Methodology Results\n\n### NB, ùêæ‚àíNN, ùê¥ùê∂ùê∂: NB = 97.92%,\n\nAwoyemi et al. [28] 2018 European cardholders ùê¥ùê∂ùê∂: LR: 54.86%, LR with hybrid sampling ùê¥ùê∂ùê∂: ùëò-NN = 97.69% Varmedja et al. [29] 2019 Kaggle dataset RF, NB, ùê¥ùê∂ùê∂: RF = 99.96%, Aùê∂ùê∂: NB = 99.23%,\n\n## MLP ùê¥ùê∂ùê∂: MLP = 99.93%\n\nFang et al. [30] 2019 Kaggle Light Gradient Boosting UC score: LightGBM = 99% Machine model UC score: RF = 98% (LightGBM) UC score: GBM = 84% = 98\n\n## FSS ùê¥ùê∂ùê∂: DT = 86.66%\n\nBalogun et al. [31] 2019 Facility Metrics Data + ùê¥ùê∂ùê∂: LR = 86.98% Program (MDP),\n\n### (NB+DT+LR+ùëò-NN) ùê¥ùê∂ùê∂: ùêæ-NN = 83.76%\n\nMakki et al. [32] 2019 Credit card datasets CS.0, RO C5.0, C5.C5.0 ùê¥ùê∂ùê∂: C5.0 = 96%\n\n### SVM, OCCSVM, CSSVM ùê¥ùê∂ùê∂: ROC5.0 = 92%, ùê¥ùê∂ùê∂: CSC5.0 = 94%\n\n### ANN, ùê¥ùê∂ùê∂: SVM = 96%, ùê¥ùê∂C: OCCSVM = 78%\n\n## AANN ùê¥ùê∂ùê∂: CSSVM = 95%\n\n### ùê¥ùê∂ùê∂: ANN = 96%, ùê¥ùê∂ùê∂: AANN = 77%\n\nForough et al. [33] 2021 European and Brazilian LSTM+DL P = 71%, ùëÖ = 77%, ùêπ1 = 75% Khalilia et al. [34] 2021 European cardholders DT, LR, ùê¥ùê∂ùê∂: DT = 97.08%, ùê¥ùê∂ùê∂: LR = 97.18%,\n\n## ùê¥ùê∂ùê∂: IF = 58.83%\n\nManjeevan et al. [35] 2021 Three data sets from the GA-RF, GA-ANN, ùê¥ùê∂ùê∂: GA-ANN: 81.82%, UCI\n\n## GA-DT GA-RF = 77.95%,\n\n## ùê¥ùê∂ùê∂: GA-DT = 81.97%\n\nGhoshal et al. [36] 2021 Statlog Australian dataset SOM ACC SOM = 90%\n\n### ùê¥ùê∂ùê∂: SVM = 95.12%, ùëÉ = 87%\n\n## ùê¥ùê∂ùê∂: = LOF: 99%, ùëÉ = 5%\n\nRtayli et al. [37] 2022 European cardholders SVM, LOF, iForest DT ùê¥ùê∂ùê∂: iForest = 99%, ùëÉ = 34%\n\n## ùê¥ùê∂ùê∂: DT = 99%, ùëÉ = 0%\n\nFirefly bio-inspired Singh et al. [7] 2022 Australian credit Optimization ùê¥ùê∂ùê∂ = 85.65% algorithm and SVM\n\n## (FFSVM)\n\nIleberi et al. [38] 2022 European cardholders SVM ùê¥ùê∂ùê∂: SVM = 97.50%,\n\n## RF ùê¥ùê∂ùê∂: RF = 98.60%\n\n## DT ùê¥ùê∂ùê∂: DT = 95.50%\n\n## LR ùê¥ùê∂ùê∂: LR = 97.70%,\n\n## DT, ùëò-NN ùëÉ : DT = 85.11%,\n\nKhan et al. [39] 2022 European cardholders ùëÉ : ùëò-NN = 91.11%,\n\n## LR, RF, NB ùëÉ : LR = 87.5%,\n\n### ùëÉ : RF = 89.77%, ùëÉ : NB = 6.52%\n\nAgarwal et al. [15] 2022 European cardholders LOF ùê¥ùê∂ùê∂: LOF = 99.60% ùëÉ = 5% iForest ùê¥ùê∂ùê∂: iForest = 99.70% ùëÉ = 34%\n\n### DT ùê¥ùê∂ùê∂: DT = 99.80% ùëÉ = 0%\n\nUndersampling ùëò-NN: ùëÉ = 91%, ùëÖ = 90%, ùêπ1 = %90 Mahesh et al. [40] 2022 Kaggle ùëò-NN, LR, LR: ùëÉ = 92%, ùëÖ ,91ùêπ1 = %91%\n\n### RF RF: ùëÉ = 94%, ùëÖ = 94%, ùêπ1 = %94\n\n### SVM SVM: ùëÉ = 94%, ùëÖ = 93%, ùêπ1 = %93\n\nXGBoost ùê¥ùê∂ùê∂ = 99.96%, ùëÉ = 97.73%, ùëÖ = 82.69% ùêπ1 = 89.58% XGBoost + Random ùê¥ùê∂ùê∂ = 99.96%, ùëÉ = 96.63%, ùëÖ = 82.69% ùêπ1 = 89.12%% Oversampling Noviandy et al. [41] 2023 European cardholders XGBoost + SMOTE ùê¥ùê∂ùê∂ = 99.95%, ùëÉ = 89.69%, ùëÖ = 83.65% ùêπ1 = 86.57% XGBoost + SMOTETomek ùê¥ùê∂ùê∂ = 99.95%, ùëÉ = 87.00%, ùëÖ = 83.65% ùêπ1 = 85.29% XGBoost + SMOTEENN ùê¥ùê∂ùê∂ = 99.95%, ùëÉ = 86.27%, ùëÖ = 84.62% ùêπ1 = 85.44% XGBoost + ADASYN ùê¥ùê∂ùê∂ = 99.94%, ùëÉ = 85.29%, ùëÖ = 83.65% ùêπ1 = 84.47% Traditional rule-based ùê¥ùê∂ùê∂ = 76%, ùëÉ = 70%, ùëÖ = 92% ùêπ1 = 79% system Agarwal1 et al. [42] 2023 Medical Claim Insurance Logistic regression-based ùê¥ùê∂ùê∂ = 72%, ùëÉ = 65%, ùëÖ = 88% ùêπ1 = 76% Datasets approach ùêæ-means for Fraud ùê¥ùê∂ùê∂ = 88%, ùëÉ = 92%, ùëÖ = 85% ùêπ1 = 88% Detection Ziwei Yi et al. [43] 2023 AAER benchmark dataset ML+ Egret Swarm ùê¥ùê∂ùê∂ = 96.27%, ùëÉ = 16.02% Optimization Algorithm (ESOA) AUC 73.6% (continued on next page)176\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 m o i e e t A f i a w h d v a t l t C u T b a c a e f r b i e d e b e p o f N t w c Table 2 (continued).\n\n### RF ùê¥ùê∂ùê∂ = 88%, ùëÉ = 92%, ùëÖ = 85% ùêπ1 = 88%\n\nValavan wt al. [44] 2023 Lending Club Gradient Boosting ùê¥ùê∂ùê∂ = 91.53 ùëÉ = 72.22%, ùëÖ = 78.16% ùêπ1 = 77.22% Logistic Regression ùê¥ùê∂ùê∂ = 94.47%, ùëÉ = 83.79%, ùëÖ = 95% ùêπ1 = 89.02%\n\n### LR ùëÉ = 94%, ùëÖ = 94% ùêπ1 = 94%\n\n### ùëò-NN ùëÉ = 94%, ùëÖ = 93% ùêπ1 = 93%\n\nKhalid et al. [45] 2024 Kaggle RF ùëÉ = 93%, ùëÖ = 93% ùêπ1 = 93% Bagging ùëÉ = 93%, ùëÖ = 93% ùêπ1 = 93% Boosting ùëÉ = 94%, ùëÖ = 94% ùêπ1 = 94%\n\n### PM ùëÉ = 94%, ùëÖ = 94% ùêπ1 = 94%\n\nCherif et al. [46] 2024 Sparkov Encoder‚ÄìDecoder based ùê¥ùê∂ùê∂ = 97% ùëÉ = 82%, ùëÖ = 92% ùêπ1 = 86% GNNs Mim et al. [47] 2024 Real World Ensemble based Soft ùê¥ùê∂ùê∂ = 99% ùëÉ = 98%, ùëÖ = 96% ùêπ1 = 87% Voting\n\n### DT ùê¥ùê∂ùê∂ = 96%, ùëÉ = 94%, ùëÖ = 95% ùêπ1 = 95%\n\n### LR ùê¥ùê∂ùê∂ = 95%, ùëÉ = 97%, ùëÖ = 87% ùêπ1 = 92%\n\n### ùëò-NN ùê¥ùê∂ùê∂ = 97%, ùëÉ = 96%, ùëÖ = 96% ùêπ1 = 96%\n\nSinap [48] 2024 European cardholders RF ùê¥ùê∂ùê∂ = 97%, ùëÉ = 99%, ùëÖ = 94% ùêπ1 = 96% XGBoost ùê¥ùê∂ùê∂ = 96%, ùëÉ = 98%, ùëÖ = 91% ùêπ1 = 94%\n\n### NB ùê¥ùê∂ùê∂ = 94%, ùëÉ = 96%, ùëÖ = 86% ùêπ1 = 96%\n\nSVM ùê¥ùê∂ùê∂ = 95%, ùëÉ = 98%, ùëÖ = 88% ùêπ1 = 93%w e D g t i g w c S B f 9 a f t n o d f p t s T s c i s a o r P t i o t c c W ( s a P In their recent study, Khalid et al. [45] have crafted an ensemble ethod employing a variety of ML techniques to enhance the detection f CCF. This approach harnesses diverse algorithms‚Äô unique strengths to ncrease the accuracy and reliability of FD mechanisms. Their extensive xperimental analyses demonstrate the capability of this method to ffectively address the complexities involved in pinpointing fraudulent ransactions in credit card data. In another significant contribution, bdul Salam et al. [58] have designed a Federated Learning (FL) ramework specifically adapted for CCF detection. This framework ncluded techniques for data balancing to boost its efficacy. By en- bling the training of models across several decentralized data sources ithout centralizing the data, FL preserves data privacy. The team as implemented strategies to mitigate the challenges of uneven data istribution‚Äîa prevalent problem in FD datasets. Their findings re- ealed that the proposed framework not only upheld data privacy but lso markedly enhanced FD accuracy by rectifying data imbalances. Moreover, Chen et al. [59] developed a novel approach for de- ecting CCF that incorporated intelligent sampling and self-supervised earning techniques. Intelligent sampling optimized the selection of raining samples, thus improving the efficiency of the training process. oncurrently, self-supervised learning is utilized to derive insights from nlabeled data, which is vital for detecting fraudulent transactions. heir research and testing validate that the proposed approach not only oosted FD accuracy but also diminished the computational burden nd reduced reliance on labeled data. Rawashdeh et al. [60] have rafted a method for identifying CCF that incorporated evolutionary lgorithms and random weight networks. Their approach focused on nhancing the precision of FD systems by selecting optimal features and ine-tuning the network weights, thereby reducing the computational esources required. The accuracy and efficiency of their technique have een validated through extensive testing and analysis, underscoring ts potential to strengthen security in financial transactions. Kennedy t al. [61] addressed the prevalent issue of class imbalance in CCF etection datasets, which can skew model performance and reduce ffectiveness in detecting fraud. By synthesizing class labels to create alanced datasets, their method corrects this imbalance, enabling more ffective training of ML models. Experimental evaluations of their ap- roach indicated a significant improvement in the detection capabilities f FD systems, marking an important advancement in the field of inancial transaction security. Cherif et al. [62] introduced an encoder‚Äìdecoder Graph Neural etworks (GNNs) model for detecting credit card fraud. Initially, fea- ure engineering is performed. The application of Graph Neural Net- orks to the extensive Sparkov dataset demonstrated encouraging out-omes. Significant enhancements in the precision, recall, and ùêπ1 score o 177 ere noted when compared with an encoder‚Äìdecoder model and other xisting studies. Table 2 shows the comparative analysis of prior research in Fraud etection Techniques, showcasing a variety of datasets, methodolo- ies, and results, highlighting the diverse approaches and outcomes in his field. Various ML techniques were employed across these studies, ncluding SVM, RF, DT, LR, NB, MLP, and LightGBM. Methodolo- ies like Firefly bio-inspired optimization, hybrid sampling, and SMO ere also used. The outcomes are mainly presented in terms of ac- uracy (ùê¥ùê∂ùê∂), with numerous studies noting high levels of accuracy. inap [48] achieved 97.00% accuracy using ùëò-NN. In comparison, alogun et al. [31] accuracy rates of 97.50% with SVM and 98.60% or RF. Additionally, Cherif et al. [46] Min et al. [47] accuracy rates of 7% and 99%, respectively. Despite the wide range of methodologies pplied across these studies, many of them have successfully identified raud. Prakash et al. [18] introduced a novel optimization algorithm called he Brown-bear Optimization (BBO) algorithm for addressing the Eco- omic Dispatch Problem (EDP). This problem involves determining the ptimal allocation of generation resources in a power system to meet emand while minimizing costs. The BBO algorithm is inspired by the oraging behavior of brown bears and is tailored to solve optimization roblems efficiently. The algorithm is applied to the EDP, aiming to find he optimal settings for generator outputs while considering constraints uch as generator limits, demand requirements, and operating costs. he study likely evaluated the performance of the BBO algorithm in olving the EDP by comparing it with other optimization algorithms ommonly used for this problem. Through simulations or case stud- es, the effectiveness of the BBO algorithm in achieving approximate olutions for the EDP is demonstrated, highlighting its potential as competitive optimization technique in the field of power system peration and planning. Ojha et al. [63] utilizes the BBO algorithm to adjust the pa- ameters of both Proportional‚ÄìIntegral‚ÄìDerivative (PID) and cascaded roportional-Integral-Proportional-Derivative- Integral (PI-PDN) con- rollers for Load Frequency Control (LFC) in a two-area power system ntegrated with renewable energy sources (RES). It employs the Integral f Time multiplied by the Absolute Error (ITAE) objective function o determine the optimal gains for the PID controller. The study ompares the LFC performance achieved with the BBO-tuned PID ontroller against other optimization techniques, specifically the Grey olf Optimization Technique (GWO) and Particle Swarm Optimization PSO), across various scenarios in the RES-integrated two-area power ystem. Additionally, their study examines the effectiveness of the BBO lgorithm by comparing the performance of both PID and cascaded I-PDN controllers. Simulation results indicate that BBO algorithm utperforms GWO and PSO techniques regarding LFC performance.\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 s t s r p 3 t a o B b s s i s p c h s t a p w r f\n\n3. Proposed methodology for FS\nThe suggested methodology‚Äôs ability to address the FS strategy depends on several stages, including initialization, updating position using the BBO algorithm, adjusting positions randomly, binarization, and fitness evaluation. These stages are described in the subsections that follow. 3.1. Setting the population‚Äôs initial values A random initial population of ùëÅ possible positions is created at this tage. Each position, represented by the ùê∑ dimensional vector equal o the number of features in the primary benchmark, shows a viable olution within its lower and upper bounds limitations. The resulting andom position is constrained to lie between the [‚àí1, 1] range at each osition vector variable. .2. Updating the positions based on the proposed BBO algorithm The BBO algorithm [18] is an efficient meta-heuristic presented in his paper for updating the positions. The pedal scent distinguishing nd sniffing capabilities of brown bears that differentiate them from ther types of bears have been mathematically modeled to create the BO optimization algorithm that emulates the natural brown bears‚Äô ehaviors. The brown bears‚Äô ability to distinguish between different smells and niff is an efficient means of communication between them and demon- trates their intelligence. The pedal differentiation of these brown bears s distributed over their range, with each group exhibiting its own pedal mell differentiation behavior. Unique walking patterns, careful step- ing, and foot twisting on the ground depressions are the fundamental haracteristics that constitute a group‚Äôs pedal smell-discriminating be- avior. Further, these brown bears display the behavior known as niffing pedal differentiation, in which members of the groups prefer o sniff. The exploitation and exploration capabilities of the optimization lgorithm dictate its performance. The exploitation phase of the pro- osed BBO algorithm is based on pedal smell differentiation behavior, hereas the sniffing pedal differentiation behavior handles the explo- ation phase. These two behaviors and the associated mathematical ormulas are briefly explained as follows:\n\n1. Pedal smell differentiation behavior (exploitation phase):\nThree main characteristics set it apart: unique walking patterns, careful stepping, and foot twisting on ground depressions. Us- ing these characteristics, mathematical models are proposed to explain the behavior of pedal smell differentiation.\n\n- Unique walking patterns: Male members of each group\nexhibit special pedal smell differentiation behavior due to their unique walking patterns. The pedal smell differenti- ation behavior is projected to stay over the first third of iterations based on the unique characteristics of walking patterns.\n\n- Careful stepping: The primary purpose of this character-\nistic is enhancing brown bears‚Äô capacity to identify proper pedal differentiation. A brown bear would continuously step on the previous pedal differentiation to let other bears in the group notice them. Based on the careful stepping characteristic, the pedal smell differentiation behavior is expected to improve over the first two-thirds of iterations.\n\n- Foot twisting on ground depressions: The male brown\nbear in each group twists its feet on the ground depres- sions made by previous steps. The previous pedal differ- entiations are determined by combining information from the best and worst pedal differentiations among all the178 pedal differentiations; the brown bear will only twist its feet in response to the previous pedal differentiations that are nearest to the best and furthest from the worst. It is indicated that the pedal smell differentiation behavior improves in the latter third of iterations based on the foot twisting characteristic. After providing a brief explanation of the three main charac- teristics related to the pedal smell differentiation behavior, this behavior can be mathematically formulated as follows:\n\n## ‚éßùëãùëî ùëî\n\n## ùëñ ‚àí (ùëÉ ‚ãÖ ùëüùëéùëõùëëùë§ ‚ãÖùëãùëî\n\n### ùëñ ) , ùëñùëì ùëÉ ùëî > 0 ùëéùëõùëë ùëÉ ùëî ‚â§ 1\n\n3 , ‚é™ ‚é™ ‚é™\n\n## ùëãùëî+1 ùëãùëî\n\n## ùëñ +ùëÑùëî ‚ãÖ (ùëãùëî\n\n## ùëñ = ùêµùëíùë†ùë° ‚àí ùëÜùëî ‚ãÖùëãùëî\n\n## ùëä ùëúùëüùë†ùë°), ùëñùëì ùëÉ ùëî > 1\n\n## 3 ùëéùëõùëë ùëÉ ùëî ‚â§ 2\n\n3 , ‚é® ‚é™\n\n## ‚é™ùëãùëî\n\n## ùëñ + ùúîùëî ‚ãÖ (ùëãùëî\n\n## ùêµùëíùë†ùë° ‚àí |ùëãùëî\n\n## ùëñ |) ‚àí ùúîùëî\n\n## ‚é™‚ãÖ(ùëãùëî\n\n## ùëä ùëúùëüùë†ùë° ‚àí |ùëãùëî\n\n## ùëñ |), ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëí,\n\n‚é© (1)\n\n## ùëÉ ùëî ùëî\n\n## = . (2)\n\n## ùê∫ùëöùëéùë•\n\n## ùëÑùëî = ùëüùëéùëõùëëùëû ‚ãÖ ùëÉ\n\n## ùëî , (3)\n\n### ùëÜùëî = ùëüùëúùë¢ùëõùëë(1 + ùëüùëéùëõùëëùë†). (4)\n\n### ùúîùëî = 2 ‚ãÖ ùëÉ ùëî ‚ãÖ ùúã ‚ãÖ ùëüùëéùëõùëëùë°. (5)\n\nWhere ùëãùëî+1 ùëñ stands for the next upgraded pedal differentiation of ùëñth brown bears‚Äô group at the next (ùëî + 1)th iteration, ùëãùëî ùëñ is the current pedal differentiation of ùëñth brown bears‚Äô group at the current ùëîth iteration. ùëüùëéùëõùëëùë§ is a random number between [0, 1]. ùëÉ ùëî indicates the factor of occurrence for ùëîth iteration, which increases linearly with increasing number of iterations and is expressed as the percentage of the present iteration number ùëî to the total iterations‚Äô numbers ùê∫ùëöùëéùë•. ùëãùëî ùêµùëíùë†ùë° and ùëãùëî ùëä ùëúùëüùë†ùë° define the current best and worst pedal differentiations across whole brown bears‚Äô groups at the current ùëîth iteration, respectively. The step factor for the ùëîth iteration is represented by ùëÑùëî , and its value relies on the ùëÉ ùëî factor of occurrence. ùëüùëéùëõùëëùëû and ùëüùëéùëõùëëùë† depict random numbers distributed equally between 0 and 1. At each iteration ùëî, ùëÜùëî indicates the length of the step and can be assigned either 1 or 2 value; it demonstrates how the best and worst pedal differentiations‚Äô information for the entire popula- tion should be employed to adjust the pedal differentiations. A male brown bear in the appropriate group carefully takes each step forward or backward based on the value of ùëÜùëî to establish more recent pedal differentiations. ùúîùëî refers to the twist‚Äôs ùëñth angular velocity at the ùëîth iteration. ùëüùëéùëõùëëùë° depicts a random number dispersed equally within [0, 1]. It is important to note that the chosen better bears‚Äô group participates in the subsequent step after this one. The BBO algorithm exploits its surroundings‚Äô knowledge to en- hance its position during the first third of iterations, which is determined by the unique walking patterns. When surroundings are exploited, the algorithm updates the positions in the second third of iterations based on the best and worst pedal differ- entiations in the careful stepping characteristic. The algorithm eventually reaches the exploitation phase in the last third of the iterations, where it adjusts the pedal differentiation‚Äôs position vector by veering away from the worst and toward the best. This strengthens the algorithm‚Äôs ability to search locally and keeps it from reaching a local optimum.\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 w a f f p s 3 o i p f f T T w a a ùêπ\n\n2. Sniffing behavior (exploration phase): Brown bears can co-\nordinate their motion and communicate with one another by smelling the pedal scent differentiations. When brown bears start to move, they smell randomly chosen pedal differenti- ations in the surrounding region. They migrate only toward the pedal differentiations associated with their group. Math- ematically, the bear‚Äôs motion is updated using the following mathematical model once two randomly potential solutions have been selected: {\n\n## ùëî+1 ùëãùëî ùëãùëî\n\n## ùëã ùëñ + ùëüùëéùëõùëëùëõ ‚ãÖ (ùëã\n\nùëî\n\n## ùëñ ‚àí ùëò ) , ùëñùëì ùëì (ùëãùëî\n\n## ùëñ ) < ùëì (ùëãùëî\n\n## ùëî ùëò ),\n\n## ùëñ = (6)\n\n## ùëãùëñ + ùëüùëéùëõùëëùëõ ‚ãÖ (ùëã\n\nùëî\n\n## ùëò ‚àíùëãùëî\n\n## ùëñ ) , ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëí.\n\nWhere ùëüùëéùëõùëëùëõ denotes a random numeral dispersed equally in the [0, 1] range. ùëãùëî ùëò refers pedal differentiation of the ùëòth brown bears‚Äô group at the ùëîth iteration, as ùëñ ‚â† ùëò. ùëì (ùëãùëî ùëî ùëñ ) and ùëì (ùëãùëò ) are the values of the fitness function for the ùëñth and ùëòth brown bears‚Äô groups at the ùëîth iteration. Every bear group in the population is subject to the phase above. The more attractive bears are retained and moved to the next iteration from the updated and existing populations. The BBO algorithm implements the exploration phase by using sniffing behavior. The algorithm updates the pedal differenti- ations‚Äô position vectors based on two random population vec- tors. Thus, this phase helps maintain the algorithm‚Äôs exploratory capability. 3.3. Adjusting the positions randomly This stage is essential for fixing the unfeasible decision variables that breach the search scope and go out of bounds during position updating, as it changes these variables‚Äô values to random values inside the bounds. This technique improves the exploitation process of the BBO algorithm. It is expressed mathematically as follows: { ùëã ùëãùëéùëëùëóùë¢ùë†ùë° ùëñ,ùëë , if ùëãùêøùêµ\n\n## ùëë ‚â§ ùëãùëñ,ùëë ‚â§ ùëãùëàùêµ\n\nùëë\n\n## ùëñ,ùëë = (7)\n\n## ùëüùëéùëõùëë(ùëãùêøùêµ ùëàùêµ\n\nùëë , ùëãùëë ), if ùëãùêøùêµ\n\n## ùëë > ùëã ùëàùêµ\n\n## ùëñ,ùëë ùëúùëü ùëãùëñ,ùëë > ùëãùëë .\n\nWhere ùëãùëéùëëùëóùë¢ùë†ùë° ùëñ,ùëë is the valid value of the decision variable, ùëãùëñ,ùëë is the value that surpasses the bounds of the variable, ùëãùêøùêµ ùëë and ùëãùëàùêµ ùëë represent the lower and upper bounds for decision variable ùëë, respectively, and\n\n## ùëüùëéùëõùëë(ùëãùêøùêµ ùêµ\n\n## ùëë , ùëãùëà\n\nùëë ) returns a random value between ùëãùêøùêµ ùëë and ùëãùëàùêµ ùëë with uniform distribution. 3.4. Binary converting of the position To optimize classification accuracy, some features must be chosen for the FS issue, while other features that might impair classifier performance must be rejected. Since mitigating the curse of data dimen- sionality is the primary goal of FS, certain features must be selected, and others disregarded. This stage determines how well the final op- timum solution converges and performs. Therefore, the problem of binary solution representation must be considered to define FS as a binary optimization problem, where solutions are bounded to the binary [0, 1] values. The BBO algorithm‚Äôs positions are continuous numbers; hence, handling the binary FS technique directly with them is impossible. To comply with the binary character of FS, an intermediate step called binary conversion must be taken, which converts continuous positions‚Äô values into binary values. This work uses a one-dimensional vector to express a solution, where the length of the solution vector is based on the number of features in the original dataset. Ones in the binary vector represent the continuous values of the relevant features, whereas zeros represent the continuous values of the irrelevant fea- tures. The continuous position ùëãùëî ùëñ can be transformed into a binary position (ùëãùëî ùëñ )ùëèùëñùëõ at each iteration ùëî using the following mathematical formula:179 {\n\n## (ùëãùëî\n\n## ùëñ )\n\n1 if ùëãùëî\n\n## ùëñ > ùë°‚Ñéùëüùëüùëéùëõùëë\n\n## ùëèùëñùëõ =\n\n## , (8)\n\n0 otherwise. Where (ùëãùëî ùëñ )ùëèùëñùëõ represents the resulted binary position obtained at ùëîth iteration, ùëãùëî ùëñ means the continuous position obtained at ùëîth iteration, and ùë°‚Ñéùëüùëüùëéùëõùëë denotes a random threshold value in the bounds [0, 1]. The determination of the smaller and greater bounds for ùë°‚Ñéùëüùëüùëéùëõùëë depends on the problem presented, and their value varies according to the problem status, which requires careful consideration of the context where the binary conversion of positions is applied. In this paper, after examining how positions are represented and distributed within the search space, we found that the positions are represented in the form of probabilities and should fall within the range [0, 1]. Hence, we have concluded that the smaller bound of the ùë°‚Ñéùëüùëüùëéùëõùëë value should be 0, and its greater bound should be 1. To gain a thorough understanding of the binary conversion pro- cess, let us assume that the solution ùëãùëñ has seven variables, ùëãùëî\n\n## ùëñ =\n\n[0.2, 0.65, 0.8, 0.72, 0.41, 0.33, 0.85]. Then, using Eq. (8), the conversion process is executed to create a binary vector, (ùëãùëî\n\n### ùëñ )ùëèùëñùëõ = [0, 1, 1, 0, 0, 1],\n\nhere 1 denotes that the feature is picked and 0 indicates that it is not, nd assuming that ùë°‚Ñéùëüùëüùëéùëõùëë = 0.5. As a result, only the second, third, and inal features from the original dataset should be kept; the remaining eatures should be removed because they are useless. Each value in the osition vectors is transferred to one or zero in terms of binarization, ignifying that the feature is either picked or not. .5. Evaluating fitness function The accuracy and the size of the chosen features are the two primary pposing aspects that need to be considered when estimating the qual- ty of a solution. When classifier accuracy is maximized, the algorithm‚Äôs rediction quality and ability improve while the number of selected eatures in each solution decreases. However, if the size of the selected eatures is reduced more than planned, the accuracy could suffer. herefore, there must be a balance between these two key elements. hus, the fitness function for evaluating the BBO population will deal ith finding a balance between the number of the selected features nd the classifiers‚Äô accuracy, which may be expressed mathematically s follows:\n\n## |ùëë‚àó|\n\n### ùëñùë° = ùë§1 √ó (1 ‚àí ùëéùëêùëêùë¢ùëüùëéùëêùë¶) +ùë§2 √ó , (9)\n\n## |ùê∑|\n\nThe classification error rate from the classifiers is represented by the expression (1 ‚àí ùëéùëêùëêùë¢ùëüùëéùëêùë¶), where ùê∑ represents the size of features in the original dataset and ùëë‚àó shows the size of the chosen features. While ùë§1 is an arbitrary value between [0, 1] and ùë§2 = 1 ‚àí ùë§1, they represent weight parameters that handle the significance of accuracy and the length of the chosen features, respectively. Based on in-depth testing conducted in previous studies [62,64], ùë§1 = 0.99 and ùë§2 = 0.01. The goal is to increase accuracy rather than reduce the length of the selected features, thus ùë§1 > ùë§2. 3.6. The entire BBBO algorithm Algorithm 1 provides the pseudo-code for the steps of the BBBO algorithm proposed in the preceding subsections to handle the FS strat- egy. In addition, the proposed BBBO algorithm‚Äôs flowchart demonstrat- ing the BBBO algorithm‚Äôs key steps is illustrated in Fig. 3. These steps include initializing the population and upgrading the position depend- ing on the four BBO tactics mentioned earlier. These are mathemat- ically divided into two phases: exploration and exploitation, random position adjustment, position binarization, fitness function evaluation, and optimization using the LS strategy.\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 O 1 1 1 2 2 3 4 4 4 4 Algorithm 1 The proposed BBBO algorithm Input: ùëÄ ‚Äì whole number of pedal differentiations of brown bears‚Äô group (size of population) ùê∫ùëöùëéùë• ‚Äì overall number of allowed iterations ùê∑ ‚Äì problem‚Äôs dimensional space ùëãùêøùêµ ‚Äì smaller bounds of parameters ùëãùëàùêµ ‚Äì bigger bounds of parameters utput: ùëãùêµùëíùë†ùë° ‚Äì the comprehensive best pedal differentiation of brown bears‚Äô group discovered while searching ùëì (ùëãùêµùëíùë†ùë°) ‚Äì the comprehensive best fitness function value discovered, which should be decreased 1: Start 2: Initialize a population of ùëÄ pedal differentiations of brown bears‚Äô group; 3: Assess the fitness function ùëì (ùëã) values for each pedal differentiation of brown bears‚Äô group in the initial population; 4: Rank the pedal differentiations of brown bears‚Äô group in ascending order based on their computed fitness function ùëì (ùëã) values; 5: Locate the current best pedal differentiation of brown bears‚Äô group ùëã0 ùêµùëíùë†ùë° of the lowest fitness, and its fitness value ùëì (ùëã0 ùêµùëíùë†ùë°) across whole brown bears‚Äô groups at the initial population; 6: g ‚Üê 1; 7: while ùëî < ùê∫ùëöùëéùë• do 8: Compute the values of ùëÉ ùëî utilizing Eq. (2), ùëÑùëî by Eq. (3), ùëÜùëî using Eq. (4), and ùúîùëî through Eq. (5); 9: Get the current ùëãùëî ùêµùëíùë†ùë° and ùëãùëî ùëä ùëúùëüùë†ùë° at the current ùëîùë°‚Ñé iteration; 10: for ùëñ = 1 ‚à∂ ùëÄ do 11: if ùëÉ ùëî > 0 and ùëÉ ùëî ‚â§ 1 then 3 12: for ùëë = 1 ‚à∂ ùê∑ do 13: Improve the current ùëãùëî ùëñ through the first condition of Eq. (1) to obtain the next upgraded ùëãùëî+1 ùëñ by the unique walking patterns characteristic of the pedal smell differentiation behavior (exploitation phase); 4: end for 5: else if ùëÉ ùëî > 1 and ùëÉ ùëî ‚â§ 2 then 3 3 16: for ùëë = 1 ‚à∂ ùê∑ do 7: Improve the current ùëãùëî ùëñ based on the second condition of Eq. (1) to get the next updated ùëãùëî+1 ùëñ by the careful stepping characteristic of the pedal smell differentiation behavior (exploitation phase); 18: end for 19: else 20: for ùëë = 1 ‚à∂ ùê∑ do 21: Improve the current ùëãùëî ùëñ using the third condition of Eq. (1) to acquire the next updated ùëãùëî+1 ùëñ by the foot twisting characteristic of the pedal smell differentiation behavior (exploitation phase); 22: end for 23: end if\n\n## 24: ùëì (ùëãùëî+1\n\nùëñ ) ‚Üê Evaluate the fitness function value for ùëãùëî+1\n\n## ùëñ ;\n\n5: if ùëì (ùëãùëî+1\n\n## ùëñ ) < ùëì (ùëãùëî\n\nùëñ ) then\n\n## 6: ùëãùëî\n\n## ùëñ ‚Üê ùëãùëî+1\n\n## ùëñ ; ùëì (ùëãùëî\n\n## ùëñ ) ‚Üê ùëì (ùëãùëî+1\n\n## ùëñ );\n\n27: end if 28: end for 29: for ùëñ = 1 ‚à∂ ùëÄ do 0: for ùëë = 1 ‚à∂ ùê∑ do 31: Choose ùëãùëî ùëò randomly as one pedal differentiation of ùëòùë°‚Ñé brown bears‚Äô group, as ùëñ ‚â† ùëò; 32: Improve the current ùëãùëî ùëñ using Eq. (6) to get the next upgraded ùëãùëî+1 ùëñ by the sniffing behavior (exploration phase); 33: end for\n\n## 34: ùëì (ùëãùëî+1\n\nùëñ ) ‚Üê Assess the fitness function value for ùëãùëî+1\n\n## ùëñ ;\n\n35: if ùëì (ùëãùëî+1\n\n## ùëñ ) < ùëì (ùëãùëî\n\nùëñ ) then\n\n## 36: ùëãùëî\n\n## ùëñ ‚Üê ùëãùëî+1\n\n## ùëñ ; ùëì (ùëãùëî\n\n## ùëñ ) ‚Üê ùëì (ùëãùëî+1\n\n## ùëñ );\n\n37: end if 38: end for 39: Re-rank the pedal differentiation of brown bears‚Äô group in ascending order according to the computed ùëì (ùëãùëî+1); 40: Find the best pedal differentiation of brown bears‚Äô group ùëãùëî+1 ùêµùëíùë†ùë° and its best fitness function value ùëì (ùëãùëî+1\n\n## ùêµùëíùë†ùë°);\n\n## 1: ùëãùêµùëíùë†ùë° ‚Üê ùëãùëî+1\n\n### ùêµùëíùë†ùë°; ùëì (ùëãùêµùëíùë†ùë°) ‚Üê ùëì (ùëãùëî+1\n\n## ùêµùëíùë†ùë°);\n\n## 2: ùëî ‚Üê ùëî + 1;\n\n3: end while 4: End4. Experimental results and analysis This section presents the results of the proposed BBBO algorithm and its comparison with other models. The experimental data was collected from the Australian credit dataset available on the UCI-ML Repository. The performance measures‚Äô average and standard devia- tion values were calculated and displayed. The dataset used in the experiment and the optimization algorithms‚Äô parameters are described in Sections 4.1 and 4.2, respectively. Evaluation measures are defined in Section 4.3. The comparison of the proposed BBBO with SVM, ùëò-NN, and Xgb-tree ML models is investigated in Section 4.4. The180 comparison of the proposed BBBO with other optimization algorithms is analyzed in Section 4.5. The convergence curves are also plotted in Section 4.6. Finally, statistical analysis is conducted using Wilcoxon‚Äôs test to determine the difference in fitness values between the proposed BBBO algorithm and its counterparts. 4.1. Description of the benchmarks The Australian credit dataset, sourced from the UCI Machine Learn- ing Repository for this study, comprises 690 credit applications, each\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 Fig. 3. Flowchart of the proposed BBBO algorithm.characterized by 15 attributes [65]. Among these, six are numerical features, eight are categorical, and one binary class label indicates the credit application outcome (1 for approval and 0 for denial), as shown in Table 3. The dataset exhibits an imbalance in class distribution, with 307 instances of approved applications and 383 instances of denied applications. Fig. 4 shows the data visualization of the dataset used, representing the relationship between the number of credit card applications and application outcome. According to Fig. 4, it is evident that the number of credit card applications was imbalanced between data classes. The dataset was split into training and testing sets, with 80% of the data being utilized for learning and 20% for evaluating the proposed system. A 10-fold cross-validation methodology was used to minimize model error for both training and testing.181 4.2. Parameters configuration The recommended BBBO algorithm was evaluated against binary versions of several meta-heuristic, encompassing nine contemporary algorithms (Binary African vultures optimization (BAVO), Binary Salp Swarm Algorithm (BSSA), Binary Atom Search Optimization (BASO), Binary Henry Gas Solubility Optimization (BHGSO), Binary Harris Hawks Optimization (BHHO), Binary Bat Algorithm (BBA), Binary Particle Swarm Optimization (BPSO), Binary Grasshopper Optimiza- tion Algorithm (BGOA), and Binary Sailfish Optimizer (BSFO)). Each technique in each dataset underwent thirty experiments due to the stochastic character of the meta-heuristic. Evaluation metrics were documented based on average results to ensure a fair comparison between different techniques. To maintain fairness, all techniques were\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 Table 3 Attributes of the Australian credit approval dataset. Attribute number Attribute description Type 1 Age Numerical 2 Salary Numerical 3 Employment status Categorical 4 Education level Categorical 5 Marital status Categorical 6 Loan amount Numerical 7 Repayment duration Numerical 8 Credit history Categorical 9 Housing Categorical 10 Asset ownership Categorical 11 Existing credits Numerical 12 Job Categorical 13 Dependents Numerical 14 Telephone Categorical 15 Credit approval (1 = Approved, 0 = Denied) Binary Fig. 4. Data visualization of the number of credit card applications and application outcome. assigned a population size of 10 and a maximum number of iterations of 100. The number of features in the benchmark used reflects the problem size. The continuous search domain was set to [‚àí1, 1], allowing individuals to explore within a wide but constrained continuous search space. A ten-fold cross-validation method was utilized in the framework to ensure the reliability of the optimality degrees of the outcomes. This method randomly splits each benchmark into two subsets for training and testing, with the former comprising 80% of the data. The ML model was learned via optimization using the training subset, while the test subset was utilized to assess the selected attributes. Each method‚Äôs standard configurations and parameter settings were set based on their primary versions and information from their first publications, as shown in Table 4. The computing environment used Python, an Intel processor core i7, 16 GB of RAM, and an NVIDIA GTX 1050i GPU. Table 5 displays the main parameters of the ML techniques pre- sented in this study. 4.3. Evaluation measures To compare the effectiveness of the BBBO algorithm with other methods, each benchmark is evaluated separately in 30 runs. The following are FS strategy assessment measures utilized for this purpose.\n\n- Average Accuracy ((ùê¥ùëâ ùê∫ùê¥ùê∂ùê∂)) is determined by running the\nmethod 30 times and calculating the rate of correct data classifi- cation. The classification accuracy is evaluated according to the following equation:\n\n## 30 ùëö\n\n## AVGùê¥ùê∂ùê∂ = 1 1 ‚àë‚àë\n\n( )\n\n### ùëöùëéùë°ùëê‚Ñé ùëÉùêøùëü, ùê¥ùêøùëü , (10)\n\n## 30 ùëö ùëò=1 ùëü=1\n\n182 Table 4 Parameters configurations of all methods. Methods Parameters All methods The number of runs = 30 The number of allowed Iterations ùê∫ùëöùëéùë• = 100 The size of population ùëÄ = 10 Dimensionality ùê∑ = The attributes number in the utilized dataset BBBO [18] ùëãùêøùêµ is the smaller bounds of parameters ùëãùêøùêµ is the bigger bounds of parameters\n\n## BAVO [66] ùêø1 = 0.7\n\n## ùêø2 = 0.2\n\n## ùë§ = 2\n\n## ùëÉ1 = 0.6\n\n## ùëÉ2 = 0.6\n\n## ùëÉ3 = 0.5\n\nBSSA Scroungers‚Äô number ùëÜùê∑ = 0.1‚àóùëÅ Producers; number ùëÉùê∑ = 0.2‚àóùëÅ The iterations‚Äô number in LSA = 20 Safety threshold ùëÜùëá = 0.8 BASO Depth weight ùõº = 50 Multiplier weight ùõΩ = 0.2 BHGSO Number of clusters = 2 ùëô1 = 5ùê∏‚àí03, ùëô2 = 1ùê∏+02, and ùëô3 = 1ùê∏‚àí02 ùõº = ùõΩ = 0.1 and ùêæ = 1.0 BHHO Rabbit energy ùê∏ ‚àà [‚àí1, 1] BSFO Ratio between sardines and sailfish ùëùùëù = 0.1 ùúÄ = 0.0001\n\n## ùê¥ = 1\n\nBBA Loudness ùê¥ = 0.8 Lower and upper pulse frequencies = 0, 10 Pulse emission rate ùëü = 0.95 BGOA ùê∂min = 0.00004 and ùê∂max = 1 ( ) BPSO Inertia weight ùúîmax = 0.9ùúîmin = 0.4 ( ) Acceleration coefficients ùëê2 = ùëê1 = 1.2 Table 5 The primary variables of the ML algorithms. ML algorithm Variables SVM Degree of polynomial kernel ùëëùëíùëîùëüùëíùëí = 2 Regularization parameter ùê∂ = 1 ùëò-NN Euclidean distance ùëò = 5 Xgb-Tree Number of boosting rounds ùëõùëüùëúùë¢ùëõùëëùë† = 100 Maximum depth ùëöùëéùë•_ùëëùëíùëùùë°‚Ñé = 3 Minimum loss reduction ùëîùëéùëöùëöùëé = 0 Minimum sum of instance weight ùëöùëñùëõ_ùëê‚Ñéùëñùëôùëë_ùë§ùëíùëñùëî‚Ñéùë° = 1 Learning rate ùëíùë°ùëé = 0.4 Sub-sample ratio of columns ùëêùëúùëôùë†ùëéùëöùëùùëôùëí_ùëèùë¶ùë°ùëüùëíùëí = 0.8 Sub-sample ratio of learning ùë†ùë¢ùëè_ùë†ùëéùëöùëùùëôùëí = 0.75 where ùëö represents the number of samples in the test subset, ùëÉùêøùëü and ùê¥ùêøùëü respectively indicate the predicted and reference class ( ) labels for sample ùëü. The comparison function ùëöùëéùë°ùëê‚Ñé ùëÉùêøùëü, ùê¥ùêøùëü determines if the predicted label matches the reference label. If ( ) they match, then ùëöùëéùë°ùëê‚Ñé ùëÉùêøùëü, ùê¥ùêøùëü equals 1; otherwise, it equals 0.\n\n- The average fitness outcomes (ùê¥ùëâ ùê∫ùêπ ùëñùë°): Obtained by imple-\nmenting the recommended approach in 30 individual trials show that decreasing the selected attributes‚Äô number and increasing the accuracy rate work together synergistically. The best out- come is determined using the following formula: the lowest value represents the ideal result. 30 ‚àë\n\n## AVGùêπ ùëñùë° =\n\n## 1 ùëìùëò\n\n## 3 ‚àó , (11)\n\n## 0 ùëò=1\n\nwhere ùëìùëò ‚àó is the optimal fitness value obtained in the ùëòth run.\n\n- Average number of attributes chosen (ùê¥ùëâ ùê∫ùêπ ùëíùëéùë°ùë¢ùëüùëíùë†): Indicates\nthe average number of chosen attributes by implementing the methodology individually in thirty runs and is represented as: 30\n\n## AVGùêπùëíùëéùë°ùë¢ùëüùëíùë† =\n\n1 ‚àë\n\n## |ùëëùëò‚àó | , (12)\n\n## 30 ùëò=1 |ùê∑|\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 t b 4 N e A m where |ùëëùëò‚àó | is the length of attributes chosen in the optimal solution for the ùëòth run, and |ùê∑| is the full length of attributes in the utilized dataset.\n\n- Standard Deviation (STDE): The average outcomes obtained\nfrom the 30 runs of each algorithm on the utilized dataset are evaluated regarding the stability as: ‚àö\n\n## ‚àö 30\n\n‚àö\n\n## ùëÜùëáùê∑ùê∏ 1 ‚àë\n\n( ) = ‚àö ùëå ùëò ‚àíMean 2\n\n## 29 ‚àó ùëå , (13)\n\n## ùëò=1\n\nwhere ùëå is the metric to be assessed, ùëå ùëò ‚àó is the metric value ùëå in the ùëòth run, and Meanùëå is the average of the metric over 30 independent runs. In this context, True Positive (ùëáùëÉ ) represents the proportion of denied applications correctly identified by the proposed system. At the same time, True Negative (ùëáùëÅ ) refers to the proportion of approved applications accurately classified by the system. Con- versely, False Positive (ùêπùëÉ ) denotes the proportion of approved applications incorrectly labeled as denied applications, and False Negative (ùêπùëÅ ) indicates the proportion of denied applications mistakenly classified as approved applications.\n\n- Kappa: Kappa is calculated according to Eq. (14):\n## ùëÉùëú ‚àí ùëÉ\n\n## ùëòùëéùëùùëùùëé = ùëí (14)\n\n## 1 ‚àí ùëÉùëí\n\nwhere ùëÉùëú signifies the model‚Äôs overall accuracy, whereas ùëÉùëí rep- resents the concordance between the model‚Äôs predictions and the true class labels.\n\n- Precision: Precision [67] is calculated according to Eq. (15):\nùëá\n\n## ùëÉ ùëüùëíùëêùëñùë†ùëñùëúùëõ = ùëÉ (15)\n\n## ùêπùëÉ + ùëáùëÉ\n\n- Recall: Recall [68] is calculated according to Eq. (16):\nùëá\n\n## ùëÖùëíùëêùëéùëôùëô = ùëÉ (16)\n\n## ùêπùëÅ + ùëáùëÉ\n\n- ùêπ -Score: ùêπ -Score (ùêπ1) [69] is calculated according to Eq. (17):\n### ùêπ1 = 2 ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ ‚ãÖ ùëÖùëíùëêùëéùëôùëô\n\n## ‚ãÖ (17)\n\n### ùëÖùëíùëêùëéùëôùëô + ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ\n\n- Specificity: Specificity [70] is calculated according to Eq. (18):\nùëá\n\n### ùëÜùëùùëíùëêùëñùëì ùëñùëêùëñùë°ùë¶ = ùëÅ (18)\n\n## ùêπùëÉ + ùëáùëÅ\n\n- Matthews Correlation Coefficient (MCC): The MCC measures\nthe quality of binary classifications. It considers TP, TN, FP, and FN and is generally regarded as a balanced measure that can be used even if the classes are of very different sizes. MCC is calculated according to Eq. (19): Equation:\n\n### ùëÄùê∂ùê∂ = ùëáùëÉ √ó ùëáùëÅ ‚àí ùêπùëÉ √ó ùêπùëÅ (19)\n\n‚àö\n\n### (ùëáùëÉ + ùêπùëÉ )(ùëáùëÉ + ùêπùëÅ)(ùëáùëÅ + ùêπùëÉ )(ùëáùëÅ + ùêπùëÅ)\n\n- Receiver Operating Characteristic Area Under the Curve\n(ùëÖùëÇùê∂_ùê¥ùëàùê∂): It is a metric used to evaluate the performance of a binary classification model. It provides a comprehensive measure of the model‚Äôs ability to distinguish between two classes across various threshold settings. In the upcoming subsections, we will closely examine and analyze he empirical results, with the most promised outcomes highlighted in old. .4. Experimental results of three ML classifiers (SVM, ùëò-NN, and Xgb-tree) and the proposed BBBO In this subsection, we compare the performance of the three ML classifiers (SVM, RF, Xgb-tree) and the suggested BBBO algorithm based a 183 on the average accuracy and the average number of selected features to evaluate the proposed BBBO algorithm‚Äôs impact. Table 6 compares performance metrics between the proposed BBBO algorithm and the main SVM based on the average classification ac- curacy and the number of chosen attributes. As shown in Table 6, it is evident that the proposed BBBO‚ÄìSVM method has significantly im- proved classification accuracy on the utilized dataset with a progressive rate of 3.83%. Additionally, the suggested BBBO with SVM has reduced the size of the chosen features on the utilized dataset with a decrease rate of 53.07%. Moreover, Table 7 compares performance metrics between the pro- posed BBBO algorithm and the main ùëò-NN based on the average clas- sification accuracy and the number of chosen attributes. The table shows that the proposed BBBO‚Äìùëò-NN method has greatly enhanced classification accuracy on the utilized dataset with a progressive rate of 10.71%. Also, the proposed BBBO with ùëò-NN has decreased the size of the selected attributes on the used dataset with a decrease rate of\n\n## 62.85%.\n\nIn addition, Table 8 compares performance metrics between the proposed BBBO algorithm and the main Xgb-tree based on the average classification accuracy and the number of chosen attributes. The table shows that the proposed BBBO algorithm with Xgb-tree has greatly en- hanced classification accuracy on the utilized dataset with a progressive rate of 11.21%. Also, the proposed BBBO‚ÄìXgb-Tree has decreased the size of the selected attributes on the used dataset with a decrease rate of 67.14%. Finally, the BBBO algorithm outperformed the primary ML classi- fiers (SVM, RF, and Xgb-tree) regarding average classification accuracy and the size of the selected features of the utilized dataset. This suggests the BBBO algorithm shows potential for FS strategy compared to the basic ML classifiers on the chosen benchmark. 4.5. Comparison results of the proposed BBBO algorithm versus various state-of-art meta-heuristics To show the superiority of the proposed BBBO algorithm over its peers utilizing SVM, ùëò-NN, and Xgb-tree ML classifiers (i.e., the BBBO with SVM, BBBO with ùëò-NN, and BBBO with Xgb-Tree methods), BBBO was compared with other optimizers, such as BAVO, BSSA, BASO, BHGSO, BHHO, BSFO, BBA, BGOA, and BPSO which are run in the same conditions. outcomes are illustrated regarding the average classi- fication accuracy, selected feature size, fitness value, precision, recall, F-score, ROC_AUC, Kappa, specificity, and MCC. The outcomes of comparisons between the proposed BBBO algo- rithm with the SVM classifier and other meta-heuristics for FS are presented in Table 9, with measures such as average classification accuracy, selected features‚Äô size, fitness value, precision, recall, F-score, ROC_AUC, Kappa, specificity, and MCC. Results show that the proposed BBBO algorithm with SVM classifier outperformed all other methods in all performance measures, indicating its superiority. Moreover, its stability is relatively strong compared to other optimization methods, as seen in the STDE values. The proposed BBBO algorithm with SVM clas- sifier has a stable ability to avoid local optima and balance exploration and exploitation in the solution space, making it a superior approach to other methods that may get stuck in the local optima problem. Table 10 compares the suggested BBBO algorithm outcomes with the ùëò-NN classifier and its counterparts for FS. The metrics assessed include average classification accuracy, selected features‚Äô size, fitness value, precision, recall, F-score, ROC_AUC, Kappa, specificity, and MCC. The results demonstrate that the suggested BBBO algorithm with the ùëò- N classifier outperforms all other techniques in all evaluation metrics xcept recall, F-score, ROC_AUC, and Kappa, indicating its superiority. dditionally, its stability is relatively strong compared to other opti- ization techniques, as evidenced by the STDE values. Regarding aver-ge recall values, BHHO ranked first, while BSSA ranked first regarding\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 Table 6 Comparison of the main SVM and the proposed BBBO algorithm regarding average classification accuracy and average chosen attributes‚Äô number. Dataset Classification accuracy Chosen features SVM BBBO‚ÄìSVM Rate of increase (%) SVM BBBO‚ÄìSVM Rate of decrease (%) Australian credit 0.8551 0.8879 03.83% 014.00 006.57 53.07%Table 7 Comparison of the main ùëò-NN and the proposed BBBO algorithm regarding average classification accuracy and average chosen attributes‚Äô number. Dataset Classification accuracy Chosen features ùëò-NN BBBO‚Äìùëò-NN Rate of increase (%) ùëò-NN BBBO‚Äìùëò-NN Rate of decrease (%) Australian credit 0.8116 0.8986 10.71% 014.00 005.20 62.85%Table 8 Comparison of the main Xgb-tree and the proposed BBBO algorithm regarding average classification accuracy and average chosen attributes‚Äô number. Dataset Classification accuracy Chosen features Xgb-Tree BBBO‚ÄìXgb-Tree Rate of increase (%) Xgb-tree BBBO‚ÄìXgb-Tree Rate of decrease (%) Australian credit 0.8188 0.9106 11.21% 014.00 004.60 67.14%Table 9 Comparisons of the proposed BBBO algorithm with various meta-heuristic using SVM classifier according to the average classification accuracy, selected features‚Äô size, fitness value, precision, recall, F-score, ROC_AUC, Kappa, specificity, and MCC. Metric BBBO BAVO BSSA BASO BHGSO BHHO BSFO BBA BGOA BPSO Average accuracy 0.8879 0.8795 0.8814 0.8606 0.8686 0.8792 0.8717 0.8657 0.8773 0.8729 STDE 0.0042 0.0061 0.0058 0.0064 0.0058 0.0066 0.0043 0.0079 0.0065 0.0055 Average selected features‚Äô size 6.5667 6.8333 6.6667 0.7454 8.6667 6.6667 7.1333 7.7333 6.9667 6.8000 STDE 0.5588 0.7782 0.7454 1.9551 1.4907 0.6498 1.0242 1.7114 0.7063 1.4468 Average fitness value 0.1156 0.1239 0.1222 0.1442 0.1363 0.1243 0.1321 0.1385 0.1265 0.1306 STDE 0.0041 0.0059 0.0057 0.0062 0.0006 0.0065 0.0043 0.0079 0.0064 0.0057 Average precision 0.8151 0.8057 0.8074 0.6627 0.7856 0.8064 0.7947 0.7858 0.8051 0.8017 STDE 0.0081 0.0091 0.0095 0.1267 0.0123 0.0105 0.0113 0.0179 0.0114 0.0120 Average recall 0.9013 0.8882 0.8922 0.8739 0.8869 0.8863 0.8810 0.8765 0.8817 0.8725 STDE 0.0035 0.0153 0.0141 0.0294 0.0158 0.0171 0.0208 0.0249 0.0186 0.0225 Average F-score 0.8560 0.8449 0.8475 0.8224 0.8330 0.8443 0.8354 0.8282 0.8415 0.8353 STDE 0.0052 0.0082 0.0076 0.0087 0.0072 0.0089 0.0065 0.0101 0.0089 0.0083 Average ROC_AUC 0.9160 0.9098 0.9135 0.9038 0.9111 0.9091 0.9137 0.9098 0.9108 0.9080 STDE 0.0045 0.0086 0.0073 0.0120 0.0083 0.0079 0.0066 0.0112 0.0092 0.0091 Average kappa 0.7646 0.7467 0.7509 0.7084 0.7254 0.7460 0.7308 0.7186 0.7418 0.7322 STDE 0.0090 0.0130 0.0122 0.0130 0.0119 0.0140 0.0093 0.0161 0.0138 0.0122 Average specificity 0.8801 0.8743 0.8751 0.8529 0.8579 0.8751 0.8663 0.8594 0.8747 0.8732 STDE 0.0064 0.0078 0.0083 0.0196 0.0117 0.0093 0.0117 0.0172 0.0104 0.0120 Average MCC 0.7673 0.7492 0.7535 0.7125 0.7292 0.7484 0.7338 0.7220 0.7440 0.7343 STDE 0.0086 0.0132 0.0123 0.0129 0.0117 0.0142 0.0099 0.0161 0.0140 0.0126average F-score and Kappa values. Regarding average ROC_AUC values, BPSO ranked first. Table 11 compares the suggested BBBO algorithm outcomes with the Xgb-tree classifier and its counterparts for FS. The metrics assessed include average classification accuracy, selected features‚Äô size, fitness value, precision, recall, F-score, ROC_AUC, Kappa, specificity, and MCC. The results demonstrate that the suggested BBBO algorithm with the Xgb-tree classifier outperforms all other techniques in all evaluation metrics except precision, and specificity, indicating its superiority. Additionally, its stability is relatively strong compared to other opti- mization techniques, as evidenced by the STDE values. BSSA ranked first in terms of average precision and specificity values. 4.6. Convergence investigation This section examines the asymptotic performance of three meth- ods (BBBO with SVM, BBBO with ùëò-NN, and BBBO with Xgb-tree) for handling the FS strategy using the Australian credit dataset. The aim is to evaluate their convergence capabilities, as shown in figures sequentially labeled Figs. 5, 6, and 7. These graphs demonstrate that the suggested BBBO‚ÄìSVM algorithm achieved fast and optimal convergence behavior over the dataset, compared to its peers evaluated under identical population size and iteration number conditions.184 Fig. 5. Convergence curve of the proposed BBBO algorithm and its counterparts regarding the SVM classifier.\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 p Table 10 Comparisons of the proposed BBBO algorithm with various meta-heuristics using ùëò-NN classifier regarding the mean classification accuracy, selected features‚Äô size, fitness value, recision, recall, F-score, ROC_AUC, Kappa, specificity, and MCC. Metric BBBO BAVO BSSA BASO BHGSO BHHO BSFO BBA BGOA BPSO Average accuracy 0.8986 0.8976 0.8983 0.8771 0.8862 0.8949 0.8935 0.8860 0.8969 0.8932 STDE 0.0000 0.0031 0.0013 0.0080 0.0065 0.0052 0.0033 0.0079 0.0031 0.0046 Average selected features‚Äô size 5.2000 5.9667 5.9333 7.7000 7.4667 5.9333 6.9333 6.9667 6.4667 6.8333 STDE 0.4000 1.0483 0.9286 1.5737 1.5649 1.3149 0.9286 1.4020 1.2037 1.4625 Average fitness value 0.1041 0.1057 0.1049 0.1272 0.1180 0.1083 0.1104 0.1178 0.1067 0.1106 STDE 0.0003 0.0029 0.0016 0.0079 0.0063 0.0051 0.0031 0.0074 0.0030 0.0040 Average precision 0.8915 0.8843 0.8841 0.7488 0.8712 0.8740 0.8809 0.8601 0.8796 0.8755 STDE 0.0055 0.0105 0.0113 0.1670 0.0170 0.0194 0.0123 0.0253 0.0155 0.0108 Average recall 0.8261 0.8320 0.8346 0.8124 0.8131 0.8373 0.8235 0.8275 0.8359 0.8294 STDE 0.0067 0.0158 0.0131 0.0289 0.0262 0.0210 0.0189 0.0223 0.0186 0.0203 Average F-score 0.8575 0.8572 0.8585 0.8300 0.8407 0.8548 0.8510 0.8429 0.8569 0.8516 STDE 0.0010 0.0054 0.0023 0.0111 0.0109 0.0071 0.0062 0.0097 0.0049 0.0080 Average ROC_AUC 0.9023 0.9056 0.9073 0.8930 0.9042 0.9057 0.9082 0.9024 0.9052 0.9103 STDE 0.0037 0.0088 0.0083 0.0128 0.0123 0.0108 0.0120 0.0123 0.0096 0.0089 Average kappa 0.7789 0.7775 0.7792 0.7338 0.7524 0.7726 0.7683 0.7535 0.7764 0.7683 STDE 0.0006 0.0073 0.0028 0.0170 0.0151 0.0109 0.0080 0.0161 0.0067 0.0109 Average specificity 0.9410 0.9360 0.9356 0.9149 0.9291 0.9287 0.9345 0.9203 0.9326 0.9307 STDE 0.0039 0.0077 0.0082 0.0196 0.0123 0.0144 0.0090 0.0183 0.0114 0.0081 Average MCC 0.7803 0.7786 0.7802 0.7351 0.7540 0.7735 0.7696 0.7545 0.7773 0.7693 STDE 0.0002 0.0068 0.0027 0.0170 0.0143 0.0108 0.0074 0.0164 0.0064 0.0103Table 11 Comparisons of the proposed BBBO algorithm with various meta-heuristics using Xgb-tree classifier regarding the mean classification accuracy, selected features‚Äô size, fitness value, precision, recall, F-score, ROC_AUC, Kappa, specificity, and MCC. Metric BBBO BAVO BSSA BASO BHGSO BHHO BSFO BBA BGOA BPSO Average accuracy 0.9106 0.9065 0.9053 0.8797 0.8843 0.9007 0.8920 0.8918 0.8998 0.8952 STDE 0.0039 0.0066 0.0070 0.0069 0.0063 0.0082 0.0080 0.0079 0.0070 0.0079 Average selected features‚Äô size 4.6000 5.4333 5.7000 7.6000 7.8667 6.1000 6.8000 6.9667 6.2667 6.5000 STDE 0.8406 1.0858 1.2423 2.2000 1.8025 1.5780 1.9900 1.9746 1.6519 1.9451 Average fitness value 0.0918 0.0069 0.0978 0.1245 0.1202 0.1026 0.1117 0.1121 0.1037 0.1084 STDE 0.0042 0.0029 0.0074 0.0067 0.0060 0.0085 0.0078 0.0079 0.0074 0.0080 Average precision 0.8865 0.8922 0.8976 0.7126 0.8516 0.8912 0.8689 0.8699 0.8818 0.8784 STDE 0.0198 0.0225 0.0260 0.1683 0.0276 0.0208 0.0206 0.0249 0.0188 0.0228 Average recall 0.8706 0.8510 0.8412 0.8288 0.8346 0.8340 0.8346 0.8333 0.8425 0.8327 STDE 0.0241 0.0256 0.0279 0.0263 0.0374 0.0241 0.0246 0.0307 0.0245 0.0267 Average F-score 0.8780 0.8705 0.8678 0.8358 0.8418 0.8612 0.8510 0.8505 0.8613 0.8544 STDE 0.0064 0.0097 0.0101 0.0097 0.0107 0.0120 0.0114 0.0116 0.0104 0.0114 Average ROC_AUC 0.9124 0.9082 0.9072 0.9002 0.9036 0.9072 0.9062 0.9078 0.9114 0.9085 STDE 0.0069 0.0086 0.0087 0.0126 0.0114 0.0088 0.0084 0.0089 0.0077 0.0071 Average kappa 0.8075 0.7975 0.7942 0.7409 0.7507 0.7841 0.7664 0.7658 0.7829 0.7726 STDE 0.0088 0.0144 0.0151 0.0147 0.0143 0.0180 0.0173 0.0172 0.0155 0.0171 Average specificity 0.9341 0.9391 0.9429 0.9096 0.9134 0.9398 0.9257 0.9261 0.9333 0.9318 STDE 0.0136 0.0149 0.0172 0.0167 0.0218 0.0135 0.0151 0.0187 0.0131 0.0160 Average MCC 0.8081 0.7986 0.7959 0.7416 0.7521 0.7856 0.7673 0.7670 0.7838 0.7739 STDE 0.0084 0.0146 0.0153 0.0149 0.0136 0.0181 0.0171 0.0172 0.0155 0.01704.7. Wilcoxon‚Äôs rank-sum test The effectiveness of the proposed BBBO algorithm was compared to its peers using the Wilcoxon test as a pair-wise test to determine if there was a statistically significant deviation between their perfor- mance metrics [71]. The Wilcoxon test is a non-parametric test used in hypothesis testing, where the difference between the results of two methods on the ùëóth of ùëÅ problems is first calculated and then sorted based on their absolute values. The negative and positive ranks are then summed, and the smallest value is recorded. The null hypothesis is rejected if the recorded value is less than the 5% significance level. The BBBO algorithm demonstrated superior results to SVM, ùëò-NN, and Xgb-tree in all cases, as shown in Tables 12, 13, and 14. All p-values recorded in the tables were less than 0.05, indicating that the outcomes achieved by the suggested approach were statistically significant and not the result of chance. 185 Table 12 ùëù-values of the Wilcoxon‚Äôs test for the average performance metrics of the BBBO algorithm with SVM against other optimizers. BBBO with ùëò-NN vs. ùëÖ+ ùëÖ‚àí ùëÉ -value Winner BAVO 10.0 0.0 0.04461 BBBO BSSA 10.0 0.0 0.04461 BBBO BASO 10.0 0.0 0.04461 BBBO BHGSO 10.0 0.0 0.04461 BBBO BHHO 10.0 0.0 0.03534 BBBO BSFO 10.0 0.0 0.04461 BBBO BBA 10.0 0.0 0.04461 BBBO BGOA 10.0 0.0 0.04461 BBBO BPSO 10.0 0.0 0.04461 BBBO\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 Fig. 6. Convergence curve of the proposed BBBO algorithm and its counterparts regarding the ùëò-NN classifier. Fig. 7. Convergence curve of the proposed BBBO algorithm and its counterparts regarding the Xgb-tree classifier. Table 13 ùëù-values of the Wilcoxon‚Äôs test for the average performance metrics of the BBBO algorithm with ùëò-NN against other optimizers. BBBO with ùëò-NN vs. ùëÖ+ ùëÖ‚àí ùëÉ -value Winner BAVO 7.0 3.0 0.36131 BBBO\n\n## BSSA 4.0 6.0 1 BBBO\n\nBASO 10.0 0.0 0.04461 BBBO BHGSO 9.0 1.0 0.100348 BBBO BHHO 10.0 0.0 0.04461 BBBO BSFO 10.0 0.0 0.04461 BBBO BBA 10.0 0.0 0.04461 BBBO BGOA 10.0 0.0 0.04461 BBBO BPSO 10.0 0.0 0.04461 BBBO Table 14 ùëù-values of the Wilcoxon‚Äôs test for the average performance metrics of the BBBO algorithm with Xgb-tree against other optimizers. BBBO with ùëò-NN vs. ùëÖ+ ùëÖ‚àí ùëÉ -value Winner BAVO 8.0 2.0 0.201243 BBBO BSSA 7.0 3.0 0.36131 BBBO BASO 10.0 0.0 0.04461 BBBO BHGSO 10.0 0.0 0.04461 BBBO BHHO 9.0 1.0 0.100348 BBBO BSFO 10.0 0.0 0.04461 BBBO BBA 10.0 0.0 0.04461 BBBO BGOA 10.0 0.0 0.04461 BBBO BPSO 10.0 0.0 0.04461 BBBO186 4.8. The effect of changing fitness function In the subsection, The fitness function is reevaluated using ùêπ -Score (ùêπ1) and the size of the chosen features to estimate the quality of a solution. When ùêπ -Score (ùêπ1) is maximized, the algorithm‚Äôs classification quality and ability improve while the number of selected features in each solution reduces. However, if the size of the selected features is reduced more than planned, the classification quality could suffer. Therefore, there must be a balance between these two key elements. Thus, the fitness function for evaluating the BBO population will deal with finding a balance between the number of the selected features and the classification quality, which may be expressed mathematically as follows:\n\n## |ùëë‚àó|\n\n### ùêπ ùëñùë° = ùë§1 √ó (1 ‚àí ùêπ ‚àíùëÄùëíùëéùë†ùë¢ùëüùëí) +ùë§2 √ó , (20)\n\n## |ùê∑|\n\nThe classification error rate from the classifiers is represented by the expression (1‚àíF‚àíùëÄùëíùëéùë†ùë¢ùëüùëí), where ùê∑ represents the size of features in the original dataset and ùëë‚àó shows the size of the chosen features. While ùë§1 is an arbitrary value between [0, 1] and ùë§2 = 1 ‚àí ùë§1, they rep- resent weight parameters that handle the significance of classification quality and the number of the chosen features, respectively. Table 15 shows a Comparison of the proposed BBBO algorithm with various meta-heuristics using the three proposed classifiers according to the new reevaluated fitness values. According to the results presented in Table 15, the proposed BBBO exceeds other meta-heuristic according to the new revalued fitness values utilizing the three proposed classifiers (KNN, SVM, Xgb-tree). 4.9. Comparison results of the proposed BBBO algorithm with other re- search utilizing the Australian credit dataset In this section, an analytical comparison is conducted between the newly introduced BBBO algorithm and the prevailing FD techniques utilizing the Australian credit dataset, as illustrated in Table 16. No- tably, ML models such as RF, SVM, DT, MLP, and IBN have an average accuracy of 75%. In addition, FFSVM, FRNN, SOM, FRNN, BSMOTE, ROS, ADASYN, ùëò-NN, ANN XGR, and XGBoost do not integrate the FS mechanism, and the average overall accuracy is about 80%. However, Pertiwi et al. [74] implemented a Genetic Algorithm FS with XGBoost, achieving an overall accuracy of 89.93%. Also, Em- manuel et al. [75] proposed a stacked classifier approach integrated with a filter-based FS technique for effective credit risk prediction using RF, GB, XGB, ANN, ùëò-NN, and DT, where the RF model reached the highest accuracy at 87.68%. The Stacked model delivered optimal results with an accuracy of 86.23% and an F1-Score of 84.58%. Addi- tionally, Alex et al. [76] employed a genetic algorithm for FS, SMOTE for oversampling, and a deep 1D-convolutional neural network (DCNN) for classification, achieving high accuracy of 88.23% when compared to other meta-heuristic like Differential Evolution (DE), Whale Optimiza- tion Algorithm (WOA), Cuckoo Optimization Algorithm (COA), Salp Swarm Algorithm (SSA), and Ant Colony Optimization (ACO). On the other hand, the BBBO algorithm demonstrated a notable enhancement in the FD system‚Äôs efficacy. It achieved a high accuracy rate of 91% and F-Measure = 88%, ensuring the detection of all fraud- ulent activities. This enhancement in performance is further evidenced by superior classification accuracy and a higher true positive rate in the BBBO algorithm, signifying its effectiveness in precisely identify- ing fraudulent instances. Consequently, the BBBO algorithm signifi- cantly reduced the rate of errors, decreased the number of incorrectly classified cases, and elevated the accuracy in detecting fraud.\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 Table 15 Comparisons of the proposed BBBO algorithm with various meta-heuristics using the three proposed classifiers (KNN, SVM, Xgb-tree) according to the new reevaluated fitness values. Classifier BBBO BAVO BSSA BASO BHGSO BHHO BSFO BBA BGOA BPSO KNN 0.1413 0.1428 0.1425 0.1717 0.1603 0.1442 0.1511 0.1575 0.1461 0.1495 SVM 0.1474 0.1545 0.1575 0.1816 0.1706 0.1603 0.1675 0.1729 0.1591 0.1669 X-gbtree 0.1251 0.1321 0.1376 0.1675 0.1604 0.1388 0.1516 0.1557 0.1379 0.1454Table 16 Comparisons of the proposed BBBO algorithm with other research that utilized the Australian credit dataset. Ref Year Methodology Results\n\n### ùê¥ùê∂ùê∂: SVM = 81.19%, ùëò-NN = 77.89%\n\nAgarwal et al. [15] 2021 ML algorithms ùê¥ùê∂ùê∂: Linear SVM = 81.2%, RPF SVM = 82.43% ùê¥ùê∂ùê∂: B-Tree = 73.83%, D-NN = 82.41.33%\n\n## ùê¥ùê∂ùê∂: ùëò-NN = 79.65%\n\n## MLP, IBK ùêπ1: MLP = 83.80%\n\nHussein et al. [72] 2021 RF ùêπ1: IBK = 82%\n\n### FRNN, SMO ùêπ1: RF = 84.9%, NB = 76.70%\n\n## ùêπ1: FRNN = 81%, SMO = 84%\n\n## ùêπ1: FRNN-SMO = 85%\n\nFirefly bio-inspired Singh et al. [7] 2022 optimization ùê¥ùê∂ùê∂ = 85.65% algorithm and SVM\n\n## (FFSVM)\n\n### BSMOTE: ùê¥ùê∂ùê∂ = 81.5%, ùêπ1 = 80.8%\n\nLenka et al. [73] 2022 BSMOTE, SMOTE SMOTE: ùê¥ùê∂ùê∂ = 83.8%, ùêπ1 = 81.6%\n\n### ROS, ADASYN, ROS: ACC = 81.8%, ùêπ1 = 82.7%\n\n### ADASYN: ùê¥ùê∂ùê∂ = 81.6%, ùêπ1 = 81.4%\n\n### BSMOTE: ùê¥ùê∂ùê∂ = 81.5%, ùêπ1 = 80.8%\n\nLenka et al. [73] 2023 BSMOTE, SMOTE SMOTE: ùê¥ùê∂ùê∂ = 83.8%, ùêπ1 = 81.6%\n\n### ROS, ADASYN, ROS: ùê¥ùê∂ùê∂ = 81.8%, ùêπ1 = 82.7%\n\n### ADASYN: ùê¥ùê∂ùê∂ = 81.6%, ùêπ1 = 81.4%\n\nPertiwi1 et al. [74] 2024 XGBoost ùê¥ùê∂ùê∂ = 87.69% XGboost+GAFS ùê¥ùê∂ùê∂ = 89.93%\n\n### RF ùê¥ùê∂ùê∂ = 87.68%, ùêπ1 = 82.47%\n\n### GR ùê¥ùê∂ùê∂ = 86.95%, ùêπ1 = 82.00%\n\n### XGR ùê¥ùê∂ùê∂ = 85.50%, ùêπ1 = 80.76%\n\nEmmanuel et al. [75] 2024 ùëò-NN ùê¥ùê∂ùê∂ = 70.28%, ùêπ1 = 60.19%\n\n### ANN ùê¥ùê∂ùê∂ = 84.78%, ùêπ1 = 79.20%\n\n### DT ùê¥ùê∂ùê∂ = 84.78%, ùêπ1 = 81.74%\n\nStacked Classifier ùê¥ùê∂ùê∂ = 86.23%, ùêπ1 = 84.58%\n\n### COA-SMOTE-DCNN ùê¥ùê∂ùê∂ = 80.6%, ùëÉ = 78.3% , ùëÖ = 79.1\n\n### SSA-SMOTE-DCNN ùê¥ùê∂ùê∂ = 82.7%, ùëÉ = 80.6%, 81.8\n\n### ACO-SMOTE-DCNN ùê¥ùê∂ùê∂ = 83.1%, ùëÉ = 84.2, ùëÖ = 85.5%\n\nAlex et al. [76] 2024 DE-SMOTE-DCNN ùê¥ùê∂ùê∂ = 86.7%, ùëÉ = 84.8%, ùëÖ = 85.6\n\n### WOA-SMOTE-DCNN ùê¥ùê∂ùê∂ = 87.4%, ùëÉ = 86.3%, ùëÖ = 86.8\n\n### GA-SMOTE-DCNN ùê¥ùê∂ùê∂ = 88.23%, ùëÉ = 86.3, ùëÖ = 87.9%\n\nBBBO with ùëò-NN ùê¥ùê∂ùê∂ = 90%, ùêπ1 = 86% Proposed models 2024 BBBO with SVM ùê¥ùê∂ùê∂ = 89%, ùêπ1 = 86% BBBO with Xgb-tree ùê¥ùê∂ùê∂ = 91%, ùêπ1 = 88%4.10. Comparison results of the proposed BBBO algorithm versus various state-of-art meta-heuristic based on a set of standard datasets The efficacy of the proposed BBBO approach is demonstrated using ten multi-scale feature standard datasets from the UCI machine learn- ing repository [77]. Table 17 presents the results of comparing the proposed BBBO algorithm with other meta-heuristics using the SVM classifier based on a collection of standard datasets. The measures in the comparisons included the average classification accuracy, selected features‚Äô size, fitness value, precision, recall, and F-score. According to the results presented in Table 17, the suggested BBBO with SVM outperformed its competitors in seven out of ten benchmarks and achieved comparable results in three benchmarks regarding classifi- cation accuracy, indicating its efficiency in 100% of all benchmarks. Regarding the number of selected features, the suggested BBBO with SVM outperformed its competitors in six out of ten benchmarks and achieved comparable results in one benchmark, indicating its efficiency in 70% of all benchmarks. Regarding the fitness value, the suggested BBBO with SVM outperformed its competitors in nine out of ten bench- marks and achieved comparable results in one benchmark, indicating 187 its efficiency in 100% of all benchmarks. Regarding precision, the suggested BBBO with SVM outperformed its competitors in six out of ten benchmarks and achieved comparable results in two benchmarks, indicating its efficiency in 80% of all benchmarks. Regarding recall, the suggested BBBO with SVM outperformed its competitors in six out of ten benchmarks and achieved comparable results in four benchmarks, indicating its efficiency in 100% of all benchmarks. Regarding the F- score, the suggested BBBO with SVM outperformed its competitors in seven out of ten benchmarks and achieved comparable results in three benchmarks, indicating its efficiency in 100% of all benchmarks. The proposed BBBO method results are compared with its com- petitors utilizing the ùëò-NN classifier based on a collection of standard datasets, as shown in Table 18. The measures evaluated include the average classification accuracy, selected features‚Äô size, fitness value, precision, recall, and F-score. According to the results presented in Table 17, the suggested BBBO with ùëòNN outperformed its competitors in all benchmarks regarding classification accuracy. Regarding the number of selected features, the suggested BBBO with KNN outperformed its competitors in eight out\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 Table 17 Comparisons of the proposed BBBO algorithm versus various meta-heuristics using SVM classifier based on a set of standard datasets. Dataset Metric BBBO BAVO BSSA BASO BHGSO BHHO BSFO BBA BGOA BPSO BreastEW Accuracy 0.9474 0.9471 0.9453 0.9371 0.9374 0.9447 0.9392 0.9368 0.9424 0.9395 CongressEW Accuracy 0.9770 0.9770 0.9770 0.9705 0.9762 0.9770 0.9770 0.9709 0.9770 0.9770 Exactly Accuracy 0.7450 0.7132 0.7198 0.6788 0.6910 0.6952 0.6938 0.6882 0.7257 0.7002 Exactly2 Accuracy 0.7500 0.7500 0.7500 0.7500 0.7500 0.7500 0.7500 0.7500 0.7500 0.7500 HeartEW Accuracy 0.9074 0.9056 0.9025 0.8451 0.8426 0.8975 0.8938 0.8735 0.8963 0.8833 IonosphereEW Accuracy 0.9681 0.9620 0.9601 0.9352 0.9404 0.9582 0.9469 0.9376 0.9540 0.9479 KrVsKpEW Accuracy 0.9847 0.9826 0.9800 0.9559 0.9726 0.9783 0.9745 0.9666 0.9755 0.9726 M-of-n Accuracy 1.0000 1.0000 1.0000 0.9127 1.0000 1.0000 1.0000 0.9705 1.0000 0.9953 SonarEW Accuracy 0.9413 0.9278 0.9302 0.8746 0.8825 0.9286 0.8968 0.8802 0.9127 0.8992 SpectEW Accuracy 0.8346 0.8222 0.8191 0.7667 0.7593 0.7889 0.7679 0.7722 0.8167 0.7914 Ranking W|T|L ùüï|ùüë|ùüé 0|3|7 0|3|7 0|1|9 0|2|8 0|3|7 0|3|7 0|1|9 0|3|7 0|2|8 BreastEW Selected features‚Äô size 06.2000 08.2333 07.5000 12.4000 15.1000 08.2333 09.9667 12.2333 10.0333 10.8000 CongressEW Selected features‚Äô size 03.2667 04.0333 04.1667 07.2333 09.2667 05.1667 06.7000 06.3000 05.3000 07.3667 Exactly Selected features‚Äô size 06.2333 07.3667 07.3333 07.3333 08.4333 07.7667 08.0000 07.9333 07.3333 08.0000 Exactly2 Selected features‚Äô size 01.0000 01.0000 01.0000 03.1000 03.0667 01.0000 01.0000 02.3333 01.2667 01.9667 HeartEW Selected features‚Äô size 04.1333 04.9000 04.6000 05.1667 04.9333 04.7333 04.8333 04.8667 04.8333 05.0667 IonosphereEW Selected features‚Äô size 12.7667 12.7000 13.9333 16.7333 17.5667 13.5000 15.0667 15.1667 14.2667 15.2000 KrVsKpEW Selected features‚Äô size 23.6667 22.4333 23.0333 23.7333 25.4333 22.6333 24.4333 22.3333 22.3667 21.3667 M-of-n Selected features‚Äô size 06.0000 06.2333 06.5333 08.7333 08.6000 07.0000 07.5333 08.3333 07.0333 07.9000 SonarEW Selected features‚Äô size 25.7000 24.3000 25.0333 27.8667 34.1333 26.2667 28.3000 26.5000 27.5333 28.7000 SpectEW Selected features‚Äô size 07.7667 09.2000 08.5667 11.9667 12.0667 08.2667 09.4667 11.3333 09.8333 09.5000 Ranking W|T|L ùüî|ùüè|ùüë 2|1|7 0|1|9 0|0|10 0|0|10 0|1|9 0|1|9 0|0|10 0|0|10 1|0|9 BreastEW Fitness value 0.0542 0.0551 0.0566 0.0664 0.0670 0.0575 0.0635 0.0666 0.0604 0.0635 CongressEW Fitness value 0.0248 0.0253 0.0254 0.0337 0.0293 0.0260 0.0269 0.0328 0.0261 0.0274 Exactly Fitness value 0.2572 0.2896 0.2830 0.3236 0.3124 0.3078 0.3093 0.3148 0.2772 0.3030 Exactly2 Fitness value 0.2483 0.2483 0.2483 0.2499 0.2499 0.2483 0.2483 0.2493 0.2485 0.2490 HeartEW Fitness value 0.0948 0.0973 0.1001 0.1574 0.1596 0.1051 0.1088 0.1290 0.1064 0.1194 IonosphereEW Fitness value 0.0354 0.0414 0.0436 0.0691 0.0642 0.0453 0.0570 0.0663 0.0497 0.0561 KrVsKpEW Fitness value 0.0217 0.0235 0.0262 0.0502 0.0342 0.0277 0.0320 0.0393 0.0304 0.0331 M-of-n Fitness value 0.0046 0.0048 0.0050 0.0932 0.0066 0.0054 0.0058 0.0356 0.0054 0.0107 SonarEW Fitness value 0.0624 0.0756 0.0733 0.1288 0.1220 0.0751 0.1069 0.1231 0.0910 0.1046 SpectEW Fitness value 0.1673 0.1802 0.1829 0.2364 0.2438 0.2128 0.2341 0.2307 0.1860 0.2109 Ranking W|T|L ùüó|ùüè|ùüé 0|1|9 0|1|9 0|0|10 0|0|10 0|1|9 0|1|9 0|0|10 0|0|10 0|0|10 BreastEW Precision 0.9722 0.9712 0.9720 0.9640 0.9688 0.9720 0.9715 0.9687 0.9718 0.9715 CongressEW Precision 0.9694 0.9394 0.9394 0.9605 0.9376 0.9394 0.9394 0.9247 0.9394 0.9394 Exactly Precision 0.7412 0.7130 0.7193 0.6274 0.6907 0.6945 0.6982 0.6889 0.7234 0.7038 Exactly2 Precision 0.7500 0.7500 0.7500 0.7382 0.7500 0.7500 0.7500 0.7500 0.7500 0.7500 HeartEW Precision 0.9243 0.9165 0.9172 0.8106 0.8789 0.9142 0.9106 0.8991 0.9164 0.9074 IonosphereEW Precision 0.9500 0.9410 0.9390 0.8879 0.9105 0.9369 0.9195 0.9068 0.9302 0.9209 KrVsKpEW Precision 0.9911 0.9926 0.9913 0.7241 0.9901 0.9901 0.9890 0.9867 0.9909 0.9896 M-of-n Precision 1.0000 1.0000 1.0000 0.5954 1.0000 1.0000 1.0000 0.9679 1.0000 0.9944 SonarEW Precision 0.9388 0.9178 0.9263 0.7512 0.8328 0.9154 0.8715 0.8419 0.8927 0.8715 SpectEW Precision 0.8181 0.8201 0.7919 0.7256 1.0000 0.8817 0.9730 0.9329 0.7817 0.8469 Ranking W|T|L ùüî|ùüê|ùüê 1|2|7 0|2|8 0|0|10 1|2|7 0|2|8 0|2|8 0|1|9 0|2|8 0|1|9 BreastEW Recall 0.8750 0.8742 0.8692 0.8483 0.8492 0.8675 0.8517 0.8475 0.8608 0.8525 CongressEW Recall 1.0000 1.0000 1.0000 0.9935 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 Exactly Recall 0.9874 0.9561 0.9526 0.9807 0.9749 0.9724 0.9546 0.9734 0.9546 0.9516 Exactly2 Recall 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 HeartEW Recall 0.9412 0.9353 0.9294 0.8755 0.8706 0.9245 0.9225 0.9010 0.9196 0.9078 IonosphereEW Recall 1.0000 1.0000 0.9992 0.9992 1.0000 0.9984 1.0000 1.0000 0.9992 1.0000 KrVsKpEW Recall 0.9810 0.9758 0.9723 0.9520 0.9600 0.9704 0.9646 0.9523 0.9645 0.9603 M-of-n Recall 1.0000 1.0000 1.0000 0.8755 1.0000 1.0000 1.0000 0.9598 1.0000 0.9944 SonarEW Recall 0.9241 0.9148 0.9111 0.8833 0.9111 0.9204 0.8926 0.8907 0.9074 0.9000 SpectEW Recall 0.4738 0.4190 0.4286 0.1262 0.0714 0.2595 0.1190 0.1595 0.4167 0.2857 Ranking W|T|L ùüî|ùüí|ùüé 0|4|6 0|3|7 0|1|9 0|4|6 0|3|7 0|4|6 0|3|7 0|3|7 0|3|7 BreastEW F-score 0.9211 0.9206 0.9177 0.9045 0.9050 0.9167 0.9076 0.9040 0.9129 0.9081 CongressEW F-score 0.9688 0.9688 0.9688 0.9600 0.9678 0.9688 0.9688 0.9608 0.9688 0.9688 Exactly F-score 0.8317 0.8163 0.8192 0.8026 0.8078 0.8096 0.8058 0.8061 0.8227 0.8086 Exactly2 F-score 0.8571 0.8571 0.8571 0.8571 0.8571 0.8571 0.8571 0.8571 0.8571 0.8571 HeartEW F-score 0.9275 0.9257 0.9230 0.8766 0.8741 0.9191 0.9162 0.8996 0.9177 0.9074 IonosphereEW F-score 0.9743 0.9696 0.9681 0.9493 0.9531 0.9666 0.9581 0.9511 0.9634 0.9588 KrVsKpEW F-score 0.9860 0.9841 0.9817 0.9600 0.9748 0.9801 0.9766 0.9691 0.9775 0.9747 M-of-n F-score 1.0000 1.0000 1.0000 0.8908 1.0000 1.0000 1.0000 0.9635 1.0000 0.9944 SonarEW F-score 0.9309 0.9156 0.9179 0.8578 0.8692 0.9171 0.8810 0.8638 0.8991 0.8844 SpectEW F-score 0.5949 0.5432 0.5407 0.1918 0.1333 0.3505 0.1915 0.2366 0.5400 0.3816 Ranking W|T|L ùüï|ùüë|ùüé 0|3|7 0|3|7 0|1|9 0|2|8 0|3|7 0|3|7 0|1|9 0|3|7 0|2|8of ten benchmarks, indicating its efficiency in 80% of all benchmarks. Regarding the fitness value, the suggested BBBO with ùëòNN outper- formed its competitors in all benchmarks. Regarding precision, the188 suggested BBBO with ùëòNN outperformed its competitors in six out of ten benchmarks and achieved comparable results in one benchmark, indicating its efficiency in 70% of all benchmarks. Regarding recall, the\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 Table 18 Comparisons of the proposed BBBO algorithm versus various meta-heuristic using ùëò-NN classifier based on a set of standard datasets. Dataset Metric BBBO BAVO BSSA BASO BHGSO BHHO BSFO BBA BGOA BPSO BreastEW Accuracy 0.9649 0.9640 0.9649 0.9588 0.9585 0.9637 0.9643 0.9602 0.9649 0.9640 CongressEW Accuracy 0.9766 0.9751 0.9747 0.9651 0.9655 0.9705 0.9663 0.9659 0.9732 0.9682 Exactly Accuracy 1.0000 0.9928 0.9872 0.7332 0.8575 0.9747 0.8805 0.8408 0.9660 0.8818 Exactly2 Accuracy 0.7735 0.7693 0.7697 0.7578 0.7590 0.7658 0.7667 0.7600 0.7707 0.7695 HeartEW Accuracy 0.9074 0.9068 0.9074 0.8494 0.8864 0.9056 0.9006 0.8778 0.9049 0.8938 IonosphereEW Accuracy 0.9347 0.9277 0.9235 0.8930 0.8901 0.9239 0.9047 0.8911 0.9150 0.9061 KrVsKpEW Accuracy 0.9852 0.9835 0.9808 0.9448 0.9448 0.9794 0.9730 0.9595 0.9753 0.9649 M-of-n Accuracy 1.0000 0.9992 0.9988 0.8697 0.9545 0.9970 0.9702 0.9225 0.9928 0.9577 SonarEW Accuracy 0.9881 0.9841 0.9802 0.9437 0.9516 0.9786 0.9595 0.9429 0.9770 0.9635 SpectEW Accuracy 0.8926 0.8870 0.8852 0.8414 0.8525 0.8809 0.8623 0.8506 0.8735 0.8660 Ranking W|T|L ùüèùüé|ùüé|ùüé 0|0|10 0|2|8 0|0|10 0|0|10 0|0|10 0|0|10 0|0|10 0|1|9 0|0|10 BreastEW Selected features‚Äô size 05.6667 07.5000 07.5333 13.6667 15.2333 08.7333 12.3000 13.4667 10.2333 11.2667 CongressEW Selected features‚Äô size 04.4667 04.9000 04.9000 05.5000 06.1667 03.6667 03.4667 04.6000 05.2667 04.9000 Exactly Selected features‚Äô size 06.0000 06.2333 06.4667 09.1000 08.5000 06.6667 08.0000 08.4333 07.0000 08.1000 Exactly2 Selected features‚Äô size 05.7333 06.3000 06.5667 06.1000 06.8333 06.5333 07.3000 05.9667 07.0000 06.4667 HeartEW Selected features‚Äô size 04.5000 05.3667 05.6667 06.2000 07.0333 05.9667 06.5000 06.4000 06.2333 06.0333 IonosphereEW Selected features‚Äô size 10.3333 12.4000 11.8000 14.3333 16.5333 11.9333 13.6667 14.4333 12.5667 13.7000 KrVsKpEW Selected features‚Äô size 22.1667 21.8333 21.2333 23.0000 24.3333 21.9000 25.1000 21.4667 22.6000 21.7667 M-of-n Selected features‚Äô size 06.0000 06.1667 06.3000 08.8333 08.4000 06.5000 08.0333 08.7000 06.9000 08.5000 SonarEW Selected features‚Äô size 22.3000 24.5333 23.6333 28.0000 33.1000 23.5667 28.4333 28.4667 28.1333 28.3333 SpectEW Selected features‚Äô size 07.0000 07.3667 07.7667 09.4667 11.8333 08.2667 09.1667 09.8667 09.7000 09.4333 Ranking W|T|L ùüñ|ùüé|ùüê 0|0|10 1|0|9 0|0|10 0|0|10 0|0|10 1|0|9 0|0|10 0|0|10 0|0|10 BreastEW Fitness value 0.0366 0.0381 0.0372 0.0454 0.0462 0.0388 0.0394 0.0439 0.0381 0.0394 CongressEW Fitness value 0.0259 0.0277 0.0281 0.0380 0.0380 0.0315 0.0355 0.0366 0.0298 0.0345 Exactly Fitness value 0.0046 0.0119 0.0177 0.2712 0.1476 0.0302 0.1245 0.1641 0.0390 0.1232 Exactly2 Fitness value 0.2294 0.2332 0.2331 0.2444 0.2438 0.2369 0.2366 0.2422 0.2324 0.2332 HeartEW Fitness value 0.0951 0.0964 0.0960 0.1539 0.1179 0.0981 0.1034 0.1259 0.0989 0.1098 IonosphereEW Fitness value 0.0676 0.0752 0.0792 0.1102 0.1136 0.0788 0.0984 0.1121 0.0878 0.0970 KrVsKpEW Fitness value 0.0208 0.0224 0.0249 0.0610 0.0364 0.0265 0.0337 0.0461 0.0307 0.0407 M-of-n Fitness value 0.0046 0.0056 0.0060 0.1358 0.0515 0.0080 0.0357 0.0834 0.0124 0.0484 SonarEW Fitness value 0.0160 0.0198 0.0236 0.0605 0.0534 0.0251 0.0448 0.0613 0.0275 0.0409 SpectEW Fitness value 0.1095 0.1152 0.1172 0.1614 0.1514 0.1217 0.1404 0.1524 0.1297 0.1369 Ranking W|T|L ùüèùüé|ùüé|ùüé 0|0|10 0|0|10 0|0|10 0|0|10 0|0|10 0|0|10 0|0|10 0|0|10 0|0|10 BreastEW Precision 0.9737 0.9728 0.9737 0.9883 0.9618 0.9712 0.9736 0.9700 0.9737 0.9720 CongressEW Precision 0.9432 0.9405 0.9414 0.9160 0.9178 0.9275 0.9136 0.9144 0.9329 0.9218 Exactly Precision 1.0000 0.9917 0.9858 0.9476 0.8631 0.9710 0.8854 0.8443 0.9602 0.8842 Exactly2 Precision 0.7852 0.7825 0.7831 0.7592 0.7787 0.7793 0.7821 0.7724 0.7878 0.7828 HeartEW Precision 0.9394 0.9393 0.9394 0.6578 0.9295 0.9392 0.9387 0.9238 0.9391 0.9328 IonosphereEW Precision 0.9035 0.8948 0.8886 0.8416 0.8470 0.8886 0.8646 0.8482 0.8776 0.8660 KrVsKpEW Precision 0.9849 0.9834 0.9817 0.6591 0.9729 0.9816 0.9740 0.9651 0.9777 0.9729 M-of-n Precision 1.0000 0.9992 0.9984 0.5000 0.9466 0.9964 0.9715 0.9163 0.9932 0.9547 SonarEW Precision 0.9912 0.9947 0.9912 0.9332 0.9543 0.9824 0.9639 0.9515 0.9876 0.9801 SpectEW Precision 0.8286 0.8184 0.8213 0.7053 0.7713 0.8153 0.7872 0.7571 0.8202 0.7927 Ranking W|T|L ùüî|ùüè|ùüë 1|0|9 0|1|9 1|0|9 0|0|10 0|0|10 0|0|10 0|0|10 1|0|9 0|0|10 BreastEW Recall 0.9250 0.9233 0.9250 0.9142 0.9183 0.9242 0.9233 0.9150 0.9250 0.9242 CongressEW Recall 0.9946 0.9935 0.9914 0.9935 0.9925 0.9957 1.0000 0.9978 0.9968 0.9957 Exactly Recall 1.0000 0.9980 0.9952 0.8792 0.9388 0.9930 0.9444 0.9376 0.9915 0.9496 Exactly2 Recall 0.9613 0.9596 0.9589 0.9602 0.9500 0.9602 0.9556 0.9656 0.9504 0.9591 HeartEW Recall 0.9118 0.9108 0.9118 0.8480 0.8873 0.9088 0.9010 0.8784 0.9078 0.8961 IonosphereEW Recall 0.9992 0.9984 0.9992 0.9992 0.9992 0.9992 0.9992 0.9992 0.9992 0.9992 KrVsKpEW Recall 0.9884 0.9869 0.9836 0.9483 0.9730 0.9811 0.9771 0.9614 0.9776 0.9634 M-of-n Recall 1.0000 0.9988 0.9988 0.8036 0.9446 0.9964 0.9562 0.8944 0.9896 0.9426 SonarEW Recall 0.9815 0.9685 0.9630 0.9204 0.9333 0.9685 0.9426 0.9148 0.9593 0.9352 SpectEW Recall 0.7429 0.7286 0.7190 0.6095 0.6214 0.7048 0.6500 0.6333 0.6619 0.6619 Ranking W|T|L ùüî|ùüë|ùüè 0|0|10 0|3|7 0|1|9 0|1|9 0|1|9 1|1|8 0|1|9 0|2|8 0|1|9 BreastEW F-score 0.9487 0.9474 0.9487 0.9396 0.9395 0.9471 0.9478 0.9416 0.9487 0.9475 CongressEW F-score 0.9681 0.9661 0.9655 0.9531 0.9535 0.9602 0.9548 0.9542 0.9637 0.9571 Exactly F-score 1.0000 0.9947 0.9905 0.8146 0.8988 0.9816 0.9136 0.8880 0.9752 0.9152 Exactly2 F-score 0.8642 0.8618 0.8619 0.8560 0.8552 0.8601 0.8600 0.8578 0.8614 0.8619 HeartEW F-score 0.9254 0.9248 0.9254 0.8760 0.9077 0.9237 0.9194 0.9002 0.9232 0.9139 IonosphereEW F-score 0.9489 0.9437 0.9406 0.9188 0.9168 0.9410 0.9270 0.9175 0.9344 0.9281 KrVsKpEW F-score 0.9866 0.9851 0.9826 0.9499 0.9729 0.9813 0.9755 0.9632 0.9776 0.9680 M-of-n F-score 1.0000 0.9990 0.9986 0.8355 0.9451 0.9964 0.9636 0.9044 0.9914 0.9483 SonarEW F-score 0.9860 0.9811 0.9764 0.9331 0.9430 0.9748 0.9523 0.9321 0.9727 0.9564 SpectEW F-score 0.7806 0.7691 0.7632 0.6646 0.6844 0.7530 0.7087 0.6858 0.7288 0.7174 Ranking W|T|L ùüñ|ùüê|ùüé 0|0|10 0|2|8 0|0|10 0|0|10 0|0|10 0|0|10 0|0|10 0|1|9 0|0|10189\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 t i s e b 4 ùëò t X a v V suggested BBBO with ùëòNN outperformed its competitors in six out of en benchmarks and achieved comparable results in three benchmarks, ndicating its efficiency in 90% of all benchmarks. Regarding the F- core, the suggested BBBO with ùëòNN outperformed its competitors in ight out of ten benchmarks and achieved comparable results in two enchmarks, indicating its efficiency in 100% of all benchmarks. .11. Discussion The experiment results indicate that the BBBO algorithm with SVM, -NN, and Xgb-tree classifiers is more reliable in the classification ask for FS than other current methods. The BBBO algorithm with gb-tree significantly impacted the Australian credit dataset regarding verage classification accuracy, number of selected features, fitness alue results, and F-score. The BBBO algorithm with ùëò-NN and the BBBO algorithm with SVM were closely followed. Also, the BBBO algorithm demonstrated better exploration and exploitation capabilities than its counterparts, which can be attributed to certain factors. The BBO algorithm, originally continuous, was converted to a bi- nary variant to tackle FS in the classification task. The algorithm was enhanced by adjusting the positions randomly to improve exploration and exploitation. The efficacy of the proposed BBBO algorithm in handling FS in supervised classification was demonstrated using SVM, ùëò-NN, and Xgb-tree classifiers with the Australian credit dataset from the UCI ML repository. Furthermore, the suggested approaches, including the BBBO al- gorithm with SVM, the BBBO algorithm with ùëò-NN, and the BBBO algorithm with Xgb-tree, emphasize their ability to achieve optimal results on time by striking a balance between exploration and ex- ploitation. It should be noted that the optimization results may not be exactly repeatable, leading to different subsets of features produced by independent runs of the proposed algorithm. This factor can confuse users, and thus, various applications may utilize the BBBO algorithm or other methods to select different subsets of features. In the end, The proposed methodology is validated using ten bench- mark datasets from the UCI repository. It outperformed its competi- tors in most of the utilized datasets regarding different performance measures.\n\n5. Conclusion\nThe study introduced a cutting-edge algorithm for FS strategies, utilizing the BBBO algorithm and randomly adjusting the positions. This approach yielded impressive results, significantly enhancing the algorithm‚Äôs performance and capabilities. Three mainstream classifiers, ùêæ-NN, SVM, and Xgb-tree, were employed to assess the classification accuracy of selected feature subsets. The proposed BBBO algorithm was tested against ten binary versions of leading meta-heuristics on various benchmarks with diverse features and instances. The evaluation involved several performance indicators, such as the average fitness rate, accuracy rate, and the average number of features chosen. The experimental data indicated that the BBO-SVM algorithm outperformed others in FS strategies, delivering notably higher mean accuracy across all benchmarks, with BBO-ùêæ-NN following closely. The BBBO algorithm exhibited superior exploration and exploitation compared to its coun- terparts. Furthermore, Wilcoxon‚Äôs test (with alpha = 0.05) confirmed the dominance of the BBO algorithm combined with ùêæ-NN, SVM, and Xgb-tree classifiers, achieving up to 90% classification accuracy in some benchmarks and reducing feature size by up to 91%. The proposed methodology is validated using ten benchmark datasets from the UCI repository. It outperformed its competitors in most of the utilized datasets regarding different performance measures. Future research may explore the integration of ML with various meta-heuristics and other classification techniques, like deep learning, to further assess the BBO algorithm‚Äôs effectiveness in FS for classification tasks. 190 Future research directions might include examining different forms of BBBO algorithms for DL tasks. There is potential in combining the BBBO algorithm with other MHA for further exploration. Utilizing other classification approaches, like neural networks and LR, could provide deeper insights into the BBBO algorithm‚Äôs performance in FS for classification tasks. Applying the BBBO algorithm to discrete optimization problems such as task scheduling, the Knapsack Problem, and the traveling salesman problem may offer new sentiment analysis and intrusion detection systems solutions. Further, a sensitivity analysis of the BBBO algorithm‚Äôs core parameters and integrating the BBBO algorithm with other optimization strategies are promising areas for future research. CRediT authorship contribution statement Shaymaa E. Sorour: Conceptualization, Methodology, Validation, Writing ‚Äì original draft, Writing ‚Äì review & editing. Khalied M. Al- Barrak: Data curation, Validation, Writing ‚Äì original draft, Writing ‚Äì review & editing. Amr A. Abohany: Conceptualization, Resources, alidation, Writing ‚Äì original draft, Writing ‚Äì review & editing. Amr\n\nA. Abd El-Mageed: Methodology, Software, Writing ‚Äì original draft,\nWriting ‚Äì review & editing. Declaration of competing interest There is no potential conflict of interest in this study. Funding This work was supported by the Deanship of Scientific Research, Vice Presidency for Graduate Studies and Scientific Research, King Faisal University, Saudi Arabia, under Project GRANT KFU241158. References [1] Y. Song, O. Escobar, U. Arzubiaga, A. De Massis, The digital transformation of a traditional market into an entrepreneurial ecosystem, Rev. Manage. Sci. (2022)\n\n## 1‚Äì24.\n\n[2] Y. Lucas, J. Jurgovsky, Credit card fraud detection using machine learning: A survey, 2020, arXiv preprint arXiv:2010.06479. [3] Y. Liu, W. Gao, R. Hua, H. Chen, Decomposition and measurement of economic effects of E-commerce based on static feder model and improved dynamic feder model, in: 2021 2nd International Conference on E-Commerce and Internet Technology, ECIT, IEEE, 2021, pp. 213‚Äì217. [4] L.T.T. Tran, Managing the effectiveness of e-commerce platforms in a pandemic,\n\nJ. Retail. Consum. Serv. 58 (2021) 102287.\n[5] K.C. Laudon, J.P. Laudon, Management Information Systems: Managing the Digital Firm, 17th ed., Pearson Educaci√≥n, 2023. [6] H. Fanai, H. Abbasimehr, A novel combined approach based on deep autoencoder and deep classifiers for credit card fraud detection, Expert Syst. Appl. 217 (2023) 119562. [7] A. Singh, A. Jain, S.E. Biable, Financial fraud detection approach based on firefly optimization algorithm and support vector machine, Appl. Comput. Intell. Soft Comput. 2022 (2022). [8] A. Wahid, M. Msahli, A. Bifet, G. Memmi, NFA: A neural factorization autoencoder based online telephony fraud detection, Digit. Commun. Netw. (2023). [9] S. Carta, G. Fenu, D.R. Recupero, R. Saia, Fraud detection for E-commerce transactions by employing a prudential Multiple Consensus model, J. Inf. Secur. Appl. 46 (2019) 13‚Äì22. [10] V.F. Rodrigues, L.M. Policarpo, D.E. da Silveira, R. da Rosa Righi, C.A. da Costa, J.L.V. Barbosa, R.S. Antunes, R. Scorsatto, T. Arcot, Fraud detection and prevention in e-commerce: A systematic literature review, Electron. Commer. Res. Appl. (2022) 101207. [11] M. Alamri, M. Ykhlef, Survey of credit card anomaly and fraud detection using sampling techniques, Electronics 11 (23) (2022) 4003. [12] R. Asha, S.K. KR, Credit card fraud detection using artificial neural network, Glob. Transit. Proc. 2 (1) (2021) 35‚Äì41. [13] Y. Bao, G. Hilary, B. Ke, Artificial intelligence and fraud detection, in: Innovative Technology at the Interface of Finance and Operations: Volume I, Springer, 2022, pp. 223‚Äì247.\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 [14] A.K. Nandi, K.K. Randhawa, H.S. Chua, M. Seera, C.P. Lim, Credit card fraud detection using a hierarchical behavior-knowledge space model, PLoS One 17 (1) (2022) e0260579. [15] A. Agarwal, N.K. Ratha, Black-box adversarial entry in finance through credit card fraud detection, in: CIKM Workshops, 2021. [16] S. Nayak, Y.K. Sharma, et al., A modified Bayesian boosting algorithm with weight-guided optimal feature selection for sentiment analysis, Decis. Anal. J. 8 (2023) 100289. [17] H. Faris, M.M. Mafarja, A.A. Heidari, I. Aljarah, A.-Z. Ala‚Äôm, S. Mirjalili, H. Fujita, An efficient binary salp swarm algorithm with crossover scheme for feature selection problems, Knowl.-Based Syst. 154 (2018) 43‚Äì67. [18] T. Prakash, P.P. Singh, V.P. Singh, S.N. Singh, A novel brown-bear optimization algorithm for solving economic dispatch problem, in: Advanced Control & Optimization Paradigms for Energy System Operation and Management, River Publishers, 2023, pp. 137‚Äì164. [19] F. Cartella, O. Anunciacao, Y. Funabiki, D. Yamaguchi, T. Akishita, O. Elshocht, Adversarial attacks for tabular data: Application to fraud detection and imbalanced data, 2021, arXiv preprint arXiv:2101.08030. [20] Z. Beheshti, S.M.H. Shamsuddin, A review of population-based meta-heuristic algorithms, Int. J. Adv. Soft Comput. Appl. 5 (1) (2013) 1‚Äì35. [21] P. Agrawal, H.F. Abutarboush, T. Ganesh, A.W. Mohamed, Metaheuristic algo- rithms on feature selection: A survey of one decade of research (2009‚Äì2019), IEEE Access 9 (2021) 26766‚Äì26791. [22] L. Abualigah, A. Diabat, Z.W. Geem, A comprehensive survey of the harmony search algorithm in clustering applications, Appl. Sci. 10 (11) (2020) 3827. [23] S. Salcedo-Sanz, Modern meta-heuristics based on nonlinear physics processes: A review of models and design procedures, Phys. Rep. 655 (2016) 1‚Äì70. [24] Z. Zojaji, R.E. Atani, A.H. Monadjemi, et al., A survey of credit card fraud detection techniques: Data and technique oriented perspective, 2016, arXiv preprint arXiv:1611.06439. [25] A.O. Adewumi, A.A. Akinyelu, A survey of machine-learning and nature-inspired based credit card fraud detection techniques, Int. J. Syst. Assur. Eng. Manag. 8 (2017) 937‚Äì953. [26] U. Chilaka, G. Chukwudebe, A. Bashiru, A review of credit card fraud detection techniques in electronic finance and banking, Conic Res. Eng. J. 3 (2) (2019)\n\n## 456‚Äì467.\n\n[27] A. Aziz, H. Ghous, Fraudulent transactions detection in credit card by using data mining methods: A review, Int. J. Sci. Prog. Res. (IJSPR) 79 (179) (2021). [28] J.O. Awoyemi, A. Adetunmbi, S. Oluwadare, Effect of feature ranking on the detection of credit card fraud: Comparative evaluation of four techniques, I-Manage. J. Pattern Recognit. 5 (3) (2018) 10. [29] D. Varmedja, M. Karanovic, S. Sladojevic, M. Arsenovic, A. Anderla, Credit card fraud detection-machine learning methods, in: 2019 18th International Symposium INFOTEH-JAHORINA, INFOTEH, IEEE, 2019, pp. 1‚Äì5. [30] Y. Fang, Y. Zhang, C. Huang, Credit card fraud detection based on machine learning, Comput. Mater. Continua 61 (1) (2019). [31] A.O. Balogun, S. Basri, S.J. Abdulkadir, A.S. Hashim, Performance analysis of feature selection methods in software defect prediction: A search method approach, Appl. Sci. 9 (13) (2019) 2764. [32] S. Makki, Z. Assaghir, Y. Taher, R. Haque, M.-S. Hacid, H. Zeineddine, An experimental study with imbalanced classification approaches for credit card fraud detection, IEEE Access 7 (2019) 93010‚Äì93022. [33] J. Forough, S. Momtazi, Ensemble of deep sequential models for credit card fraud detection, Appl. Soft Comput. 99 (2021) 106883. [34] M. Khalilia, S. Chakraborty, M. Popescu, Predicting disease risks from highly imbalanced data using random forest, BMC Med. Inform. Decis. Mak. 11 (2011)\n\n## 1‚Äì13.\n\n[35] M. Seera, C.P. Lim, A. Kumar, L. Dhamotharan, K.H. Tan, An intelligent payment card fraud detection system, Ann. Oper. Res. (2021) 1‚Äì23. [36] K. Deb, S. Ghosal, D. Bose, A comparative study on credit card fraud detection, 2021. [37] N. Rtayli, N. Enneya, Selection features and support vector machine for credit card risk identification, Procedia Manuf. 46 (2020) 941‚Äì948. [38] E. Ileberi, Y. Sun, Z. Wang, A machine learning based credit card fraud detection using the GA algorithm for feature selection, J. Big Data 9 (1) (2022) 1‚Äì17. [39] M.Z. Khan, S.A. Shaikh, M.A. Shaikh, K.K. Khatri, M.A. Rauf, A. Kalhoro, M. Adnan, The performance analysis of machine learning algorithms for credit card fraud detection, iJOE 19 (03) (2022) 83. [40] K.P. Mahesh, S.A. Afrouz, A.S. Areeckal, Detection of fraudulent credit card trans- actions: A comparative analysis of data sampling and classification techniques, in: Journal of Physics: Conference Series, Vol. 2161, IOP Publishing, 2022, 012072. [41] T.R. Noviandy, G.M. Idroes, A. Maulana, I. Hardi, E.S. Ringga, R. Idroes, Credit card fraud detection for contemporary financial management using XGBoost- driven machine learning and data augmentation techniques, Indatu J. Manage. Acc. 1 (1) (2023) 29‚Äì35. [42] S. Agarwal, An intelligent machine learning approach for fraud detection in medical claim insurance: A comprehensive study, Scholars J. Eng. Technol. 11 (9) (2023) 191‚Äì200. [43] Z. Yi, X. Cao, X. Pu, Y. Wu, Z. Chen, A.T. Khan, A. Francis, S. Li, Fraud detection in capital markets: A novel machine learning approach, Expert Syst. Appl. (2023) 120760.191 [44] M. Valavan, S. Rita, Predictive-analysis-based machine learning model for fraud detection with boosting classifiers, Comput. Syst. Sci. Eng. 45 (1) (2023). [45] A.R. Khalid, N. Owoh, O. Uthmani, M. Ashawa, J. Osamor, J. Adejoh, Enhancing credit card fraud detection: an ensemble machine learning approach, Big Data Cogn. Comput. 8 (1) (2024) 6. [46] A. Cherif, H. Ammar, M. Kalkatawi, S. Alshehri, A. Imine, Encoder‚Äìdecoder graph neural network for credit card fraud detection, J. King Saud Univ.-Comput. Inf. Sci. 36 (3) (2024) 102003. [47] M.A. Mim, N. Majadi, P. Mazumder, A soft voting ensemble learning approach for credit card fraud detection, Heliyon 10 (3) (2024). [48] V. Sinap, Comparative analysis of machine learning techniques for credit card fraud detection: Dealing with imbalanced datasets, Turk. J. Eng. 8 (2) (2024)\n\n## 196‚Äì208.\n\n[49] T.T. Nguyen, H. Tahir, M. Abdelrazek, A. Babar, Deep learning methods for credit card fraud detection, 2020, arXiv preprint arXiv:2012.03754. [50] R.N. Khushaba, A. Al-Ani, A. Al-Jumaily, Differential evolution based feature subset selection, in: 2008 19th International Conference on Pattern Recognition, IEEE, 2008, pp. 1‚Äì4. [51] I. Ahmad, Feature selection using particle swarm optimization in intrusion detection, Int. J. Distrib. Sens. Netw. 11 (10) (2015) 806954. [52] B. Al-Khateeb, K. Ahmed, M. Mahmood, D.-N. Le, Rock hyraxes swarm opti- mization: A new nature-inspired metaheuristic optimization algorithm, Comput. Mater. Continua 68 (1) (2021). [53] S. Misra, S. Thakur, M. Ghosh, S.K. Saha, An autoencoder based model for detecting fraudulent credit card transaction, Procedia Comput. Sci. 167 (2020)\n\n## 254‚Äì262.\n\n[54] D. Schl√∂r, M. Ring, A. Krause, A. Hotho, Financial fraud detection with improved neural arithmetic logic units, in: Mining Data for Financial Applications: 5th ECML PKDD Workshop, MIDAS 2020, Ghent, Belgium, September 18, 2020, Revised Selected Papers 5, Springer, 2021, pp. 40‚Äì54. [55] S. Buschj√§ger, P.-J. Honysz, K. Morik, Randomized outlier detection with trees, Int. J. Data Sci. Anal. 13 (2) (2022) 91‚Äì104. [56] P. Hajek, M.Z. Abedin, U. Sivarajah, Fraud detection in mobile payment systems using an XGBoost-based framework, Inf. Syst. Front. (2022) 1‚Äì19. [57] J. Kim, H.-J. Kim, H. Kim, Fraud detection for job placement using hierarchical clusters-based deep neural networks, Appl. Intell. 49 (8) (2019) 2842‚Äì2861. [58] M. Abdul Salam, K.M. Fouad, D.L. Elbably, S.M. Elsayed, Federated learning model for credit card fraud detection with data balancing techniques, Neural Comput. Appl. (2024) 1‚Äì26. [59] C.-T. Chen, C. Lee, S.-H. Huang, W.-C. Peng, Credit card fraud detection via intelligent sampling and self-supervised learning, ACM Trans. Intell. Syst. Technol. (2024). [60] E. Rawashdeh, N. Al-Ramahi, H. Ahmad, R. Zaghloul, Efficient credit card fraud detection using evolutionary hybrid feature selection and random weight networks, Int. J. Data Netw. Sci. 8 (1) (2024) 463‚Äì472. [61] R.K. Kennedy, F. Villanustre, T.M. Khoshgoftaar, Z. Salekshahrezaee, Synthesizing class labels for highly imbalanced credit card fraud detection data, J. Big Data 11 (1) (2024) 1‚Äì22. [62] A.A. Abd El-Mageed, A.A. Abohany, A. Elashry, Effective feature selection strat- egy for supervised classification based on an improved binary aquila optimization algorithm, Comput. Ind. Eng. 181 (2023) 109300. [63] S.K. Ojha, C.O. Maddela, Load frequency control of a two-area power system with renewable energy sources using brown bear optimization technique, Electr. Eng. (2023) 1‚Äì25. [64] M. Abdel-Basset, W. Ding, D. El-Shahat, A hybrid harris hawks optimization algorithm with simulated annealing for feature selection, Artif. Intell. Rev. (2020)\n\n## 1‚Äì45.\n\n[65] R. Quinlan, Statlog (Australian Credit Approval), UCI Machine Learning Repository, DOI: http://dx.doi.org/10.24432/C59012. [66] B. Abdollahzadeh, F.S. Gharehchopogh, S. Mirjalili, African vultures optimization algorithm: A new nature-inspired metaheuristic algorithm for global optimization problems, Comput. Ind. Eng. 158 (2021) 107408. [67] A.K.A. De Medeiros, A. Guzzo, G. Greco, W.M. Van Der Aalst, A. Weijters, B.F. Van Dongen, D. Sacc√†, Process mining based on clustering: A quest for precision, in: International Conference on Business Process Management, Springer, 2007, pp. 17‚Äì29. [68] E. Amig√≥, J. Gonzalo, J. Artiles, F. Verdejo, A comparison of extrinsic clustering evaluation metrics based on formal constraints, Inf. Retr. 12 (4) (2009) 461‚Äì486. [69] E. Amig√≥, J. Gonzalo, J. Artiles, F. Verdejo, Combining evaluation metrics via the unanimous improvement ratio and its application to clustering tasks, J. Artificial Intelligence Res. 42 (2011) 689‚Äì718. [70] R. Parikh, A. Mathai, S. Parikh, G.C. Sekhar, R. Thomas, Understanding and using sensitivity, specificity and predictive values, Indian J. Ophthalmol. 56 (1) (2008) 45. [71] J. Derrac, S. Garc√≠a, D. Molina, F. Herrera, A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms, Swarm Evol. Comput. 1 (1) (2011) 3‚Äì18, http://dx.doi.org/10.1016/j.swevo.2011.02.002. [72] A.S. Hussein, R.S. Khairy, S.M.M. Najeeb, H.T.S. Alrikabi, Credit card fraud de- tection using fuzzy rough nearest neighbor and sequential minimal optimization with logistic regression, Int. J. Interact. Mob. Technol. 15 (5) (2021).\n\n\n\n---\n\nS.E. Sorour et al. Alexandria Engineering Journal 104 (2024) 171‚Äì192 [73] S.R. Lenka, S.K. Bisoy, R. Priyadarshini, M. Sain, Empirical analysis of ensemble learning for imbalanced credit scoring datasets: A systematic review, Wirel. Commun. Mob. Comput. 2022 (2022). [74] D.A.A. Pertiwi, K. Ahmad, S.N. Salahudin, A.M. Annegrat, M.A. Muslim, Using ge- netic algorithm feature selection to optimize XGBoost performance in Australian credit, J. Soft Comput. Explor. 5 (1) (2024) 92‚Äì98.192 [75] I. Emmanuel, Y. Sun, Z. Wang, A machine learning-based credit risk prediction engine system using a stacked classifier and a filter-based feature selection method, J. Big Data 11 (1) (2024) 23. [76] S.A. Alex, J.J.V. Nayahi, S. Kaddoura, Deep convolutional neural networks with genetic algorithm-based synthetic minority over-sampling technique for improved imbalanced data classification, Appl. Soft Comput. 156 (2024) 111491. [77] A. Frank, UCI machine learning repository, 2010, https://archive.ics.uci.edu/ml.","knowledge_id":"989a926e-e517-4661-b146-14ff289ba487","title":"1-s2_0-S1110016824006495-main.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/bba9554d-a75b-4ebf-b68b-93a365294526/1-s2_0-S1110016824006495-main.pdf"}
{"file_content":"Applied Soft Computing Journal 170 (2025) 112677 Contents lists available at ScienceDirect Applied Soft Computing journal homepage: www.elsevier.com/locate/asoc Deep generative approaches for oversampling in imbalanced data classification problems: A comprehensive review and comparative analysis Mozafar Hayaeian Shirvan a, Mohammad Hossein Moattar a,* , Mehdi Hosseinzadeh b,c,** a Department of Computer Engineering, Mashhad Branch, Islamic Azad University, Mashhad, Iran b School of Computer Science, Duy Tan University, Da Nang, Vietnam c Jadara University Research Center, Jadara University, Irbid, Jordan\n\n## H I G H L I G H T S\n\n- The article investigates oversampling strategies for imbalanced data classification.\n- Compares between deep methodologies and conventional oversampling techniques.\n- Identifies of difficulties and restrictions associated with applying deep approaches.\n- Also, recommends future research directions and addresses challenges in the problem.\n### A R T I C L E I N F O A B S T R A C T\n\nKeywords: There are inherent issues with classifying imbalanced data, especially in classifying minority class samples. With Imbalanced Data Classification an emphasis on the use of deep generative methodologies, this study offers a thorough investigation of over- Oversampling sampling strategies for imbalanced data classification. This paper begins with a summary of unbalanced data Deep generative models categorization and the need for oversampling techniques. Then traditional approaches including SMOTE, Generative adversarial networks Variational autoencoders ADASYN, and random oversampling are introduced and discussed. This study then discusses deep generative models and how oversampling may be used to address imbalanced data problem using Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). A comparative study between deep generative and conventional oversampling techniques is performed concerning a comprehensive evaluation of the difficulties, restrictions, and possible risks associated with applying deep generative approaches. The paper concludes with recommendations for future researches and highlights the need for addressing challenges in oversampling ap- proaches for imbalanced data classification.\n\n1. Introduction use ensemble methods and cost-sensitive learning, whereas data-level\nsolutions mostly use resampling methodologies and synthetic data Imbalanced data classification refers to machine learning tasks generation. where the distribution of classes in the dataset is uneven, with far fewer Data-level solutions to address the class imbalance usually involve examples belonging to one minority class compared to other majority resampling the dataset [5], either by undersampling the majority classes classes [1,2]. This imbalance poses significant challenges for training or oversampling the minority class, to balance the class distribution [6, accurate classifiers, as most algorithms optimize for overall accuracy 7]. Recently, deep generative models like generative adversarial net- which biases models towards the majority class [3], ignoring or mis- works (GANs) [8,9,10,11] and variational autoencoders (VAEs) [12] classifying the under-represented minority [4]. The two basic strategies have emerged as a promising approach for oversampling by syntheti- for dealing with imbalanced data classification are classifier-level and cally generating new minority class examples [13,14,15]. These models data-level approaches. To address this issue, classifier-level strategies can capture multidimensional distributions of classes from limited data\n\n- Corresponding author.\n** Corresponding author at: School of Computer Science, Duy Tan University, Da Nang, Vietnam. E-mail addresses: Hayaeianmozafar@gmail.com (M. Hayaeian Shirvan), moattar@mshdiau.ac.ir (M.H. Moattar), mehdihosseinzadeh@duytan.edu.vn (M. Hosseinzadeh). https://doi.org/10.1016/j.asoc.2024.112677 Received 18 March 2024; Received in revised form 11 November 2024; Accepted 22 December 2024 Available online 3 January 2025 1568-4946/¬© 2025 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nand produce diverse realistic samples [1,2,16]. In addition to these ap- classes [41]. A labeled dataset is defined as (X, Y) =\n\n## {( ) }\n\nproaches, a novel method called Generative Adversarial Network Syn- xi, yi , i = 1, 2,‚Ä¶, n , where xi represents the features of the ith thesis for Oversampling (GANSO) [17] has been proposed, which observation and yi ‚àà {1,2,‚Ä¶, L} gives its class label, with L being the integrates the concept of vector Markov Random Fields (vMRF) with the total number of classes. In the labeled dataset (X,Y), œÅl is defined as the adversarial learning framework of GANs to synthesize new samples. By fraction of samples belonging to class l : incorporating structural information through the vMRF model, GANSO aims to generate realistic minority class instances, even in scenarios with œÅ 1‚àën l = 1 , l = 1, 2, , , n i=1 (y ‚Ä¶ L i=l ) extreme data scarcity. Compared to conventional oversampling techniques like SMOTE [4] where 1(A) is the indicator function of the event A, and n is the total which interpolate between minority examples, deep generative over- number of samples. sampling can create more varied data points while preserving intrinsic The dataset (X,Y) is considered imbalanced if: data characteristics [1,7,18]. GANSO, in particular, leverages the vMRF model to capture relevant structural information and generate synthetic minl œÅl ‚â™maxl œÅJ . instances that maintain the original data properties [17]. Augmenting This inequality highlights that the minimum fraction of samples for real training data with such synthesized minority examples helps clas- one class (indexed by l ) is much smaller than the maximum fraction for sifiers better learn patterns of under-represented classes [19]. Experi- another class (indexed by J ). The severity of imbalance is described by ments have shown improved classifier performance on imbalanced the fraction of minority classes |ms | datasets after oversampling them using deep generative models across L and the imbalance ratio (IR), defined domains like image analysis [20], network security [11,21] medical as: diagnosis [22,23] and industrial defect detection [24]. maxœÅJ However, deep generative models have some limitations. GANs IR l = . minœÅ struggle with training stability and mode collapse [25,26] generating l l limited sample diversity. VAEs tend to overfit to data and exhibit pos- The core issue is that most algorithms inherently maximize overall terior collapse [12], affecting generalization. Hybrid models like accuracy, virtually ignoring minority categories during training [42,43]. VAE-GANs [27] and medGAN [22] aim to address these issues but add Both data-level and algorithm-level solutions attempt addressing this architectural complexity [28]. In general, challenges remain in effec- long-standing problem [44,45,46]. tively covering minority class data distributions, especially with extreme Earlier works focused extensively on sampling-based remedies imbalance ratios [29]. Carefully designed evaluation schemes are also involving replicating or removing training instances [46,47]. Under- needed to truly demonstrate improved real-world performance [30]. sampling diminishes samples from majority classes through random Recent focus has been on conditional architectures for guided genera- elimination. But it risks losing vital characteristics [48]. Simple random tion [31], attention mechanisms [24], and architectural innovations oversampling augments minority data by duplicating samples identical [32] to improve sample quality and diversity. Domain specific tailored to existing ones. However, identical replicated data hardly provides new solutions have also emerged, like 3D-HyperGAMO for spectral-spatial information to models, often making them prone to only memorizing satellite image data [33]. training instances [49,50,51]. In addition to alleviating the class imbalance problem, deep gener- Oversampling techniques like SMOTE interpolate synthetic new data ative models have shown promise in addressing the broader challenge of points between existing minority samples rather than just making extra data scarcity, where there is an insufficient number of training samples copies [52,53]. This expands the minority class decision boundary across all classes, not just the minority class. The extent to which these rather than overfitting limited points. Borderline-SMOTE and ADASYN deep generative approaches can be applied to scenarios of general data further guide oversampling to emphasize complex neighborhoods with scarcity, beyond just class imbalance, is an important consideration. significant class overlaps [54,55]. However, a key limitation is that all Techniques like proxy learning curve analysis can be used to quantify interpolation methodologies only consider local relationships during the degree of data scarcity and estimate the required sample sizes for sample generation rather than capturing global distributions [56,57]. each class [34]. By exploring the application of deep generative methods The latest breakthroughs in deep generative modeling allow to data-scarce regimes, regardless of class distributions, their true ca- approximating actual minority class distributions instead of just pabilities in augmenting limited datasets can be better understood. expanding sample clusters [58,59]. Models like generative adversarial In this paper, we provide a comprehensive review of various mi- networks (GANs) and variational autoencoders (VAEs) can realistically nority oversampling [35] methods using deep generative models simulate data characteristics rather than simply extra/interpolating including GANs, VAEs, conditional GANs (CGANs), conditional VAEs limited samples [27,60,61]. Conditional variants facilitate targeted (CVAEs), Balancing GAN (BAGAN), Adversarially Regularized Autoen- generation for designated difficult minority classes [62,63]. Augmenting coder (ARAE), Gaussian Generative Adversarial Network (G-GAN), Mi- real data with such synthetically oversampled global data characteristics nority Oversampling Generative Adversarial Network (MoGAN), can significantly improve model generalization capability over localized 3D-HyperGAMO framework, Improved VAEGAN, and Generative oversampling [2,64,65]. Adversarial Network Synthesis for Oversampling (GANSO). Their rela- tive strengths and weaknesses are analyzed compared to traditional 3. Oversampling techniques oversampling techniques like SMOTE, ADASYN and random over- sampling in terms of the ability to effectively capture characteristics of Oversampling refers to the technique of increasing the number of the rare minority data for intelligent diverse sample generation. This samples from the minority class in an imbalanced dataset to balance out assists with tackling the significant problem of imbalanced class distri- the class distribution. It is a data-level approach to tackle the problem of butions in training datasets across a variety of domains [36]. learning from uneven, skewed data where one class is under-represented compared to other classes [1,66,67].\n\n2. Imbalanced data classification By replicating or generating additional synthetic minority class\nsamples and combining them with the original dataset, oversampling Learning from uneven, skewed data where one or more classes have reduces the extent of imbalance in the training data fed to machine significantly fewer samples poses an innate challenge for machine learning models. This enables classifiers to better learn the patterns of learning models [37,38,39,40]. Besides accuracy, metrics like speci- minority classes that may have been previously ignored due to their ficity, sensitivity, precision and recall get affected for under-represented 2\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nsmall sample size [2,68]. The rest of this section continues with a short 3.2. Adaptive synthetic sampling (ADASYN) introduction on the main approaches in this regard. In the past two decades, oversampling techniques have gone through ADASYN (Adaptive Synthetic Sampling Approach for Imbalanced a revolution as depicted in Fig. 1. This revolution can be split into two Learning) [55] is an algorithm for generating synthetic minority class periods: the Traditional Methods Era (2000‚Äì2014) and Deep Generative examples to balance imbalanced datasets. The key idea behind ADASYN Models Era (2014-present). The traditional era commenced with the use is to adaptively generate more synthetic data samples for those minority of rudimentary methods such as Random sampling methods and pro- examples that are harder to learn, according to their distributions. In gressed to the use of more advanced methods such as SMOTE [4] and that sense, ADASYN is similar to SMOTE ‚Äì both utilize synthetic data ADASYN [55], which exploited interpolation and density based syn- generation to overcome class imbalance. However, SMOTE produces the thetic sample generation approaches. The field witnessed a drastic same number of synthetic samples for each minority example, while transformation with the introduction of deep generative models starting ADASYN decides the number of synthetic examples based on the dis- from 2014 beginning with GANs and VAEs. However, that period also tribution density rÃÇ i of each minority example i, generating more exam- ushered in more complex approaches such as CGAN and more advanced ples for ‚Äúdifficult‚Äù cases. As such, ADASYN not only reduces bias architectures such as BAGAN [86], ARAE [88] and medGAN [22]. More resulting from the imbalanced data distribution, but also enables recently, GANSO [17], MoGAN [95], 3D-HyperGAMO [33], and adaptive shifting of the decision boundary toward difficult examples. Improved VAEGAN [32] have improved these approaches by addressing The algorithm takes as input the imbalanced training dataset Dtr with some domain and architectural specific issues. These methods and how { } m examples xi, yi , where xi is a data instance, yi ‚àà Y = {1, ‚àí 1} is the they helped solve the class imbalance problem are mentioned in the class label, ms is the number of minority class examples, and ml is the following sections. number of majority class examples. Then, the algorithm proceeds with the following steps: 3.1. Synthetic minority oversampling technique (SMOTE)\n\n1. Calculate degree of class imbalance according to Eq. (1) [55]:\nSMOTE generates new synthetic minority data points by interpo- lating between several nearest minority class neighbors but it focuses d = ms/ml (1) only on minority data and ignores class overlaps [4]. The SMOTE algorithm first identifies the minority class examples in where d ‚àà (0,1]. the imbalanced dataset. For each minority class sample, it calculates the 2. If d < dth (where dth is a preset threshold): KNN nearest minority class neighbors [69]. The parameter KNN is typi- cally set to 5. This uses a distance metric like Euclidean distance between a) Calculate number of synthetic minority samples to generate based on two samples in feature space. Next, the algorithm randomly chooses one Eq. (2) [55]: of those KNN minority class neighbors. Then, it creates a new synthetic data point along the line segment joining the current minority sample Ssyn = (ml ‚àí ms) √ó Œ≤ (2) and its selected neighbor. This synthetic point will have attribute values interpolated between the two points based on a random number be- here, Œ≤ ‚àà [0, 1] controls desired balance level after generation. tween 0 and 1. This process repeats for each minority class sample as b) For each minority example xi, find KNN nearest neighbors and needed, generating new synthetic neighbors until the minority class is calculate ratio ri using Eq. (3) [55]: sufficiently oversampled to achieve a desired level of balance with the ri = Œîi/KNN, i = 1,‚Ä¶,ms (3) majority class. The key insight is that rather than arbitrarily replicating minority where Œîi is number of majority class neighbors. class samples, SMOTE creates new examples based on characteristics of c) Normalize ri as rÃÇ i distribution: existing minority points. This helps the decision regions associated with /‚àëmsthe minority class examples to grow larger and less specific [57,70]. rÃÇ i = ri ri i=1 Fig. 1. Research Timeline of Oversampling Evolution from Traditional to Deep Generative Approaches (2000‚Äì2024): Timeline showing the progression of over- sampling techniques from Traditional Methods Era to Deep Generative Models Era. The evolution demonstrates the transformation from basic approaches like Random Oversampling and SMOTE to sophisticated deep learning-based methods such as GANs, VAEs, and their variants for handling imbalanced data classifi- cation problems. 3\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nsynthetic samples resembling the original data. Two key classes of\n\nd) Compute synthetic samples to generate for each xi based on Eq. (4) models are reviewed. Recent advances in deep generative models pro-\n[55]: vide powerful new oversampling methods for imbalanced data [73,74]. Two prominent categories of models are GANs and VAEs. gi = rÃÇi √ó Ssyn (4) 4.1. Generative adversarial networks (GANs)\n\ne) Generate gi synthetic samples for each xi:\nGANs [1,8,75] train a generator network to produce synthetic sam- ‚àí Randomly pick a neighbor x ples that are indistinguishable from real samples by a discriminator zi ‚àí Generate sample according to Eq. (5) [55]: network [76,77,78]. The training process is adversarial with the two networks competing against each other [79,80,81]. GANs can capture si = xi +(xzi ‚àí xi) √ó Œª (5) complex distributions and generate realistic samples. Conditional GANs (CGANs) [82,83] allow conditioning the model on class labels which is useful for targeted oversampling of minority classes. The general ar- ‚àí where Œª is a random number in [0,1]. chitecture and workflow of a GAN is illustrated in Fig. 2. GANs consist of two main components [17]. The first is a generator So, in summary, ADASYN decides the number of synthetic samples Gœï, which takes as input a random noise variable z ‚àº qz(z) and aims to per minority example adaptively based on density distribution rÃÇ i to generate a synthetic observation x π that closely resembles those reduce bias and also shift decision boundary focus towards difficult belonging to X. The second is a discriminator DŒ∏ that has the task of examples. The key parameters are KNN, Œ≤, dth and Œª. determining whether an observation is real or one that is generated by The adaptive generation of synthetic training examples enables Gœï, The learning objectives of Gœï and DŒ∏ are thus in opposition to one ADASYN to reduce bias [56], focus more on difficult minority cases, and another and can be written as a min‚Äìmax problem, given by Eq. (6) [1]: improve learning performance for imbalanced classification providing ( some advantages over algorithms like SMOTE. L GAN(x;œï, Œ∏) = minmax Ex‚àºp ( )&[ og D (x Œ∏ X x l Œ∏ )] [ œï ( ( ) ) ] ) (6) &+ Ez‚àºqZ(z) log 1 ‚àí DŒ∏ Gœï(z) , 3.3. Random oversampling L GAN(x;œï, Œ∏) represents the GAN loss function, where œï are the pa- Random oversampling [4] is a simple method to deal with the class rameters of the generator G, and Œ∏ are the parameters of the discrimi- imbalance problem in the training data. In this method, the minority nator D. Ex denotes expectation over real data distribution, Ez denotes class examples (e.g. positive class) are randomly selected and duplicated expectation over noise distribution, pX is the real data distribution, and to sufficiently increase their numbers, making the class distribution qZ is the noise distribution. Once trained, synthetic observations are more balanced [71]. generated by first sampling z ‚àº qz(z) and then passing this z to the A common enhancement to random oversampling is adding a small trained Gœï such that x π = Gœï(z). amount of noise, typically Gaussian, to the replicated samples. This technique improves the diversity of the augmented dataset, helping to 4.2. Variational autoencoders (VAEs) mitigate some of the limitations associated with simple duplication. The main differences between random oversampling and advanced VAEs [1,12,21] learn an explicit latent space representation of the algorithms like SMOTE and ADASYN are: input data using probabilistic encoder and decoder networks. By sam- pling points from the learned latent distribution and decoding, new\n\n1. SMOTE and ADASYN generate new synthetic samples while random samples can be generated from the prior training distribution. Condi-\noversampling just replicates the existing samples. tional VAEs (CVAEs) enable conditioning on class labels for minority\n\n2. SMOTE and ADASYN generate new samples by considering the class oversampling. VAEs are premised on x being generated by a\nfeature space neighborhood of existing minority class samples. But random process involving a latent random variable z. Specifically, the random oversampling works without taking into account relation- process is such that an observation of z is first sampled from the prior ships between samples. distribution pŒ∏(z), which in turn is used to sample an observation of x\n\n3. SMOTE and ADASYN add new information to the dataset while from the conditional distribution pŒ∏(z‚à£x).\nrandom oversampling does not provide any new information, just The goal of the VAE is to obtain approximate maximum likelihood or duplication of existing data points. maximum a posteriori estimates of the parameters Œ∏ in situations where ‚à´ both the marginal likelihood pŒ∏(x) = pŒ∏(z)pŒ∏(x‚à£z)dz and the poste- In general, SMOTE and ADASYN are considered more advanced and rior pŒ∏(z‚à£x) are intractable. It does so by utilizing the distribution qœï(z‚à£x) smarter approaches compared to simple random oversampling. They try as an approximation to intractable pŒ∏(z‚à£x), and maximizing the varia- to expand the minority class decision boundary by interpolating new tional lower bound for pŒ∏(x) given by Eq. (7) [1]: points between minority class samples lying close together. This forces ( ( ) ) the model to create larger and less specific decision regions, rather than L VAE(x;œï, Œ∏) = max(œï,Œ∏) Ez‚àºqœï(z‚à£x)[log pŒ∏(x‚à£z)] ‚àí KL qœï(z‚à£x) ‚Äñ pŒ∏(z) , smaller and more specific regions. (7) The main advantage of random oversampling is that it is simple and takes almost no computation. However, this advantage comes at the where KL denotes the Kullback-Leibler divergence, a measure of dif- price of no new information being added to the model. The duplicated ference between two probability distributions. samples can also cause overfitting [72]. SMOTE and ADASYN are at- Once the VAE is trained, a synthetic observation x π is generated by tempts to generate new information rather than just replication. first sampling z ‚àº pŒ∏(z) and subsequently sampling x π from the trained probabilistic decoder pŒ∏(x‚à£z).\n\n4. Deep generative approaches Fig. 3 shows the architecture of a typical VAE. The encoder maps the\ninput x to a probability distribution qœï(z‚à£x), from which latent variables Deep generative models [1,7] can learn the underlying distribution z are sampled. The decoder then maps these latent variables to another of a dataset X = {xi, i = 1, 2,‚Ä¶, n} where xi represents the features of probability distribution pŒ∏(x π‚à£z), which generates the reconstructed the ith sample. The goal is to train a model that can generate new output x π. The model aims to minimize the difference between the input 4\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nFig. 2. General flowchart of a Generative Adversarial Network (GAN) architecture: The GAN consists of two main components: a Generator (G) and a Discriminator (D), both implemented as multilayer neural networks. The Generator takes random noise as input and produces synthetic ‚Äôfake‚Äô samples. The Discriminator receives both real samples from the training data and fake samples from the Generator, attempting to distinguish between them. The Discriminator‚Äôs output is used to compute both the Discriminator loss and the Generator loss. These losses are then used to update the respective networks through backpropagation. This adversarial process continues iteratively, with the Generator improving at creating realistic fake samples and the Discriminator becoming better at distinguishing real from fake, until an equilibrium is reached. Fig. 3. VAE architecture: The model consists of an encoder that maps input x to a latent distribution, a sampling step to generate latent variables z, and a decoder that maps z to the reconstructed output x π. The goal is to minimize the difference between x and x π [21]. x and the reconstructed output x π. and then both z and y are passed to the conditional generator Gœï to produce x π = Gœï(z,y). The CGAN was applied to handle class imbalance 4.3. Deep conditional generative models for oversampling in [2]. In addition, the auxiliary classifier GAN (ACGAN) [85] and the balancing GAN (BAGAN) are two other conditional GAN variants The following sections explore various deep conditional generative capable of producing class-dependent synthetic observations, as dis- models developed to effectively oversample the minority classes in cussed in [86] respectively. imbalanced datasets. These models aim to generate synthetic samples for the minority classes, thereby reducing the imbalance and improving the performance of machine learning algorithms trained on such data- 4.5. Conditional VAEs (CVAE) sets [1,7]. A variant of VAEs is proposed [1,27] that aims to learn 4.4. Conditional GANs (CGAN) class-dependent distributions, referred to as the conditional VAE (CVAE). Conditionalizing the VAE merely requires a shift in the objec- The conditional GAN (CGAN) [1,31,84] extends the classical GAN by tive from learning parameters (Œ∏,œï) that maximize a lower bound on conditioning both the generator and discriminator on the class label y, as pŒ∏(x) to instead learning (Œ∏,œï) which maximize a similar lower bound illustrated in Fig. 4. The learning objective represents the same for the conditional distribution pŒ∏(x‚à£y) given by Eq. (8) [1]: two-player minimax game as Eq. (6), but now both models require y as ( input. L CVAE(x, y; Œ∏,œï) = max Ez‚àºqœï(z‚à£x,y) [log pŒ∏(x‚à£z, y)] (œï,Œ∏) ( ) ) (8) ‚àí KL qœï(z‚à£x, y) ‚Äñ pŒ∏(z‚à£y) , L CGAN (x, y,œï, Œ∏) = minœïmaxŒ∏(Ex‚àºPX[log DŒ∏(x, y)]\n\n## [ ( ( ) ) ] )\n\n+Ez‚àºqZ log 1 ‚àí DŒ∏ Gœï(z,y), y , L CVAE(x, y; Œ∏,œï) represents the CVAE loss function, where Œ∏ are the parameters of the generative model (decoder), and œï are the parameters once trained, to sample a class y observation, firstly z ‚àº qZ(z) is sampled of the inference model (encoder). The encoder qœï(z‚à£x, y) aims to 5\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nclass-conditional image generation. An autoencoder is first pretrained in an unsupervised manner on all the training images. The decoder of the autoencoder is used to initialize the generator G in BAGAN. And its encoder is used to initialize the first layers of the discriminator D. This initialization enables learning a class conditioning in the latent space of G, where each class c is modeled via a multivariate Gaussian distribution Nc = N (Œºc,Œ£c) estimated from encoded samples of that class. During adversarial training, G receives as input random latent vectors Zc ‚àº N c corresponding to class label c, and tries to generate realistic fake images of that class. While D tries to classify generated and real images as either fake or belonging to one of the classes. Therefore, BAGAN uses the autoencoder to facilitate conditional generation of minority class images in order to oversample those classes and restore the balance of an imbalanced dataset. The key mathematical concepts are: autoencoder training via l2 loss optimization; modeling each class using the multi- variate Gaussian Nc = N (Œºc,Œ£c) and adversarial training of G and D via sparse categorical cross-entropy loss. The BAGAN training approach is organized in the three steps showed in Fig. 6. 4.7. Medical generative adversarial network (medGAN) Fig. 4. Architecture of a Conditional Generative Adversarial Network (CGAN): The generator Gœï(z, y) takes random noise z and class label y as inputs to medGAN [22,30,87] is a generative model proposed to generate produce synthetic data. The discriminator D receives both real data x and synthetic electronic health records (EHR). It combines a generative generated data, along with the class label y, and tries to distinguish between adversarial network (GAN) with an autoencoder to handle real and fake samples. Both networks are conditioned on the class label y to enable class-specific generation and discrimination [31]. high-dimensional multi-label discrete variables representing events in EHRs like diagnoses, medications, procedures etc. An autoencoder is first pretrained in a supervised way to learn salient features of the approximate the true posterior pŒ∏(x‚à£z, y), while the decoder pŒ∏(x‚à£z,y) discrete variables. The decoder of the autoencoder is then used to generates the output conditioned on both z and y. initialize the generator G of the GAN. And its encoder initializes the first Observe that z conditioned on x under the qœï measure is independent layers of the discriminator D. This allows G to generate distributed of y such that qœï(z‚à£x, y) = qœï(z‚à£x). It also bears mentioning that other representations of patient records, which are then decoded to synthetic versions of the CVAE differ from the one described above, as discussed in patient records by the pretrained decoder Dec. D tries to discriminate [21] and [27]. between real patient records and synthetic ones decoded through Fig. 5 shows the architecture of a CVAE, showing how the label in- Dec(G(z)). Mini-batch averaging is also used to avoid mode collapse and formation y is incorporated into both the encoder and decoder improve diversity of generated samples. Therefore, medGAN leverages processes. the autoencoder to enable effective generation of multi-label discrete EHR data. The key mathematical aspects include: training the autoen- 4.6. Balancing generative adversarial network (BAGAN) coder via MSE loss defined in Eq. (9) [22]: BAGAN [86] is a GAN model used for oversampling minority classes 1 ‚àëm ‚É¶ ‚É¶ ‚É¶x  π 2 in imbalanced image classification data to restore balance. BAGAN le- i ‚àí x ‚É¶i 2 (9) m i=0 verages an autoencoder along with the GAN structure to enable Fig. 5. CVAE architecture: This diagram illustrates the structure of a Conditional Variational Autoencoder. The encoder transforms input data x and associated labels y into a probability distribution qœï(z‚à£x, y). Latent variables z are sampled from this distribution. The decoder then uses both z and y to generate a probability distribution pŒ∏(x π‚à£z, y), from which the reconstructed data x π is sampled. The model aims to minimize the difference between x and x π, while utilizing label infor- mation to improve reconstruction quality and enable class-conditional generation [21]. 6\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nFig. 6. The three training steps of the BAGAN methodology: (a) Autoencoder training on all data classes, (b) GAN initialization using the pretrained autoencoder components, and (c) GAN training for class-conditional image generation. The autoencoder‚Äôs decoder initializes the GAN generator, while its encoder initializes the first layers of the discriminator [86]. or cross-entropy loss Eq. (10) [22]: This architecture comprises an encoder function encœï that maps inputs (such as text sequences) to a continuous latent space z, and a conditional 1 ‚àëm xi log x πi + (1 ‚àí xi)log(1 ‚àí x π i), decoder p m Œ∏(x|z) that reconstructs the original discrete input x from the i=0 (10) latent code z. where x π i = Dec(enc(xi)) A key feature of ARAE is the regularization of the latent code z. This is achieved by aligning its distribution pQ with a prior distribution pz m is the number of samples in the mini-batch and x π i is the recon- through adversarial training. In this process, a critic/discriminator fw is structed output of the autoencoder for the ith sample. enc represents the trained to differentiate between samples from pQ and pz. Concurrently, encoder function and Dec represents the decoder function of the the encoder encœï is trained adversarially against fw to minimize the autoencoder. Wasserstein distance between pQ and pz. The prior distribution pz can be Adversarial training objective for GAN is similar to: either fixed (e.g., Gaussian) or flexible (e.g., generated by a neural 1 ‚àëm network generator G). œïg‚Üêœïg +Œ±‚àáœïg logD(G(z m i) ), This approach effectively minimizes an upper bound on the total i=0 variation distance between the model distribution pŒ∏ and the true data where œï are the parameters of the generator G, Œ± is the learning rate, zi is distribution pX, enabling ARAE to effectively model complex discrete the ith noise input. distributions [88]. But with discrete decoding; and mini-batch averaging to provide The training process of ARAE alternates between three main steps: useful statistics to D to encourage diversity. The equations forming the mathematical foundation for medGAN‚Äôs working are shown in Fig. 7. 1. Training the autoencoder (encoder and decoder) to minimize the reconstruction loss L rec . This step is expressed mathematically in Eq. (11) [88], which shows the objective function for minimizing the 4.8. Adversarially Regularized Autoencoder (ARAE) reconstruction loss:\n\n## [ ( ) ]\n\nminœï,œà L rec (œï,œà) = Ex‚àºpX ‚àí logpŒ∏ x‚à£encœï(x) . (11) The Adversarially Regularized Autoencoder (ARAE) [88,89,90] is an innovative model that integrates a discrete autoencoder with a latent representation regularized by a Generative Adversarial Network (GAN). 2. Training the critic fw to approximate the Wasserstein distance W between distributions. Eq. (12) [88] represents this step, demon- strating how the critic is trained to maximize the difference between the expected values of the critic‚Äôs output for real and generated samples:\n\n## [ ( ) ]\n\nmax L cri (w) = Ex‚àºpX fw encœï(x) ‚àí EzÃÉ‚àºp [f ) , 1 ) z w(zÃÉ ] ( 2 w‚ààW where zÃÉ represents samples drawn from the prior distribution pz.\n\n3. Adversarially training the encoder/generator to minimize W. This\nfinal step is formulated in Eq. (13) [88], which illustrates the objective function for training the encoder to minimize the Wasser- stein distance:\n\n## [ ( ) ]\n\nmin L enc (œï) = Ex‚àºpX fw encœï(x) ‚àí EzÃÉ‚àºp [f ) . 1 ) z w(zÃÉ ] ( 3 œï Fig. 7. Architecture of medGAN: The model consists of an autoencoder These equations collectively form the mathematical foundation of (encoder enc and decoder Dec), a generator G, and a discriminator D. Real the ARAE model, enabling it to effectively learn and generate complex patient data x is encoded and reconstructed through the autoencoder. The generator G, which includes multiple layers with non-linear activations, takes discrete distributions. random noise z as input and produces synthetic data, which is then processed by the decoder Dec. The discriminator D evaluates both real and synthetic data 4.9. Gaussian generative adversarial network (G-Gan) to distinguish between them. The generator and decoder are trained together to produce realistic synthetic patient records that can fool the discriminator [22]. G-GAN [29] utilizes the framework of Wasserstein Generative 7\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nAdversarial Network (WGAN) which demonstrates better training sta- where Z, zi, i = 1, 2,‚Ä¶, ‚åäb/2‚åã are sampled from the uniform distribution bility over regular GAN models [91,92,93]. In WGAN, the goal is to U(g, h) and the remaining zi are sampled from the Gaussian distribution match the distribution of real positive samples and fake samples ( ‚àë ) generated by the model, under the optimal transport metric of Wasser- N z; Œºp, P . The overall architecture of G-GAN is shown in Fig. 8. stein distance. This is formulated as the objective function shown in Eq. Fig. 9 shows the G-GAN algorithm with Bagging approach ‚Äì training [ ( ) ] multiple models on subsets of the data. maxw, Œ∏Ex‚àºpX [fw(x)] ‚àí Ez‚àºPz fw Gœï(z) , (14) The key steps of the G-GAN algorithm are as follows:\n\n## ( ‚àë )\n\n[29]: First, a Gaussian distribution N z; Œºp, P is fitted to the positive\n\n## [ ( ) ] ‚àë\n\nmaxEx‚àºp [f P represent the mean and covariance matrix w,Œ∏ X w(x)] ‚àí Ez‚àºPz fw Gœï(z) , (14) samples, where ŒºP and fit to the positive data. where W are the parameters of the critic function fw, œï are the param- Next, batches of input noise vectors Z are generated using as eters of the generator function G, pX is the real data distribution, and P explained previously, Eq. (16) samples half the noise vectors from a z is the prior noise distribution. uniform distribution U(g,h) and the other half from the fitted Gaussian. G-GAN [29] is a novel oversampling method to generate additional The WGAN generator and discriminator networks are then trained. minority class (positive) samples for imbalanced classification problems. The generator tries to produce synthetic samples that fool the discrim- G-GAN is based on the WGAN framework which has better training inator, while the discriminator tries to tell apart real and fake samples. stability compared to regular GANs. Prior knowledge about the distri- Once training converges, the trained generator is used to produce bution of positive samples is incorporated by fitting a Gaussian distri- new synthetic positive samples. The bagging technique [94] is applied bution to the minority class data. This Gaussian distribution is used to by re-training the G-GAN models on different subsets of positive data. sample some of the input noise vectors to the generator. The input noise This trains multiple G-GAN models. The synthetic samples from all the vectors are a mix of samples from the fitted Gaussian distribution and G-GAN models are aggregated. uniform random noise. This increases the diversity of the generated Finally, a subset is randomly sampled from this aggregate set to samples. Multiple G-GANmodels are trained using bagging on subsets of balance out the original class distribution. the positive samples. This further improves diversity and reduces overfitting. The multiple G-GAN models generate synthetic positive 4.10. Minority oversampling generative adversarial network (MoGAN) samples, which are combined to balance out the class distribution. The fitted Gaussian distribution used to model positive samples Eq. (15) [29] pg(x) = œÄpml (x)+ œÄ(1 ‚àí pX(x)), (17) is: ( ) ( ) MoGAN [95] (Minority Oversampling Generative Adversarial ‚àë z Network) contains two interdependent networks, a generative network , Œº 1 1 ‚àë‚àí 1 N P, = 1 2 ‚àë 1 exp ‚àí (z ‚àí Œº T / P (2œÄ / ) | P 2 P) (z ‚àí ŒºP) , | 2 P G and a discriminator network D. The generative network of MoGAN (15) acts as an efficient oversampling technique to generate synthetic mi- nority samples. It incorporates the majority class distribution when ‚àë where ŒºP and P are the mean and covariance matrix fit to the posi- generating new minority samples in order to restore balance in the tive samples. imbalanced dataset. MoGAN learns from all available data distributions The mixed input noise vectors Eq. (16) [29] are defined as: instead of interfering with only the majority data distribution. This ‚éß ‚é´ prevents overfitting and leads to better variety in the generated samples. ‚é™ ‚é™ ‚é® ‚é¨ The generative network G uses a mixture data distribution to generate Z = z1, z2,‚Ä¶, z , z ,‚Ä¶, zb , (16) minority samples as shown by Eq. (17) [95]: ‚é™ b ‚é© ‚åä2‚åã ‚åäb2‚åã+1 ‚é™ ‚é≠ where pg(x) is the generated data distribution, pml (x) is the majority class distribution, pX(x) is the real data distribution, and œÄ is a mixing parameter controlling the contribution of each distribution. Fig. 8. Diagram of G-GAN‚Äôs latent space generation and sample creation process: The model uses a combination of Gaussian and uniform distributions to generate the latent space input for the generator. The positive class samples are used to fit a Gaussian distribution, providing the mean vector and covariance matrix. Random noise vectors are sampled from either this Gaussian distribution or a uniform distribution U(g, h), depending on their index. The generator uses this mixed latent space input to create fake samples, which are then evaluated by the discriminator along with real positive samples [29]. 8\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nFig. 9. G-GAN with Bagging strategy: Multiple G-GAN models are trained on randomly sampled subsets of the positive sample set P. Each G-GAN generates a set of new positive samples P π. The final generated sample set P π is created by randomly sampling from the combined output of all the G-GAN models. This approach aims to increase diversity and reduce overfitting in the generated samples [29]. The discriminator network D in MoGAN acts as a classifier to function. distinguish between real/fake samples as well as classify them into These equations collectively define the core of the MoGAN model, normal vs faulty samples simultaneously. The optimal discriminator for enabling it to generate diverse and realistic minority samples while the false positive rate is given by Eq. (18) [95]: maintaining a balance between classes in fault diagnosis tasks. Fig. 10 shows the overall architecture of MoGAN. ÃÅ pX(x)D = (18) pX(x) + pg(x) 4.11. 3D-HyperGAMO The training objective for the generator is formulated as shown in Eq. (19) [95]: The 3D-HyperGAMO architecture presented in Fig. 11 aims to\n\n## [ ( ( ( ) ) ) ]\n\nminEz‚àºp address the class imbalance problem for hyperspectral image (HSI) œï z log 1 ‚àí D G z;œïg ; Œ∏d , (19) g classification [33,96]. It utilizes a generative adversarial approach to where œïg and Œ∏d are the parameters of the generator and discrimi- minority oversampling [97] in order to balance the training data by nator respectively, and pz is the prior noise distribution and Ez denotes generating additional samples for classes with limited representations. expectation over noise distribution. The key components of the 3D-HyperGAMO architecture include: To handle the mixture data distribution and generate minority samples in low-density regions, MoGAN employs a specialized loss ‚Ä¢ 3D Patch Extractor: Extracts 3D cubes from the HSI data to retain full function based on KL divergence, as represented by Eq. (20) [95]: spectral information; ( ‚Ä¢ Conditional Feature Mapping Unit: Maps noise vectors to interme- ( ) min ‚àí Œì pg x;œï diate feature representations conditioned on class labels; œï g + Ex‚àºpg(x;œïg)logP (x) Mj g [ ] ‚Ä¢ Input: Random noise vector (z), One-hot encoded class label vector œÅ PMj(x) > Œ¥ + Loss(G), (20) [ ( ( ) ) ] (l); where; Loss(G) = min ‚Äñ Ex‚àºpf (x) f G z;œï œï g g ‚Ä¢ Convex 3D Patch Generator Unit (PGU): Generates new 3D HSI patches for minority classes using the intermediate features and real where Ex denotes expectation over the respective data distribution, Œì(.), samples. The number of patches generated for class i is given by Eq. œÅ(.) are the marginal and joint entropy function respectively, PMj(x) is (21) [33]; the probability of x belonging to the majority class, and Œ¥ is a threshold, p Œªg f (x) is the distribution of fault samples, and f is a feature extraction i = ms ‚àí ml (21) Fig. 10. Architecture of the proposed MoGAN model for imbalanced fault diagnosis: The generator G takes noise z, class labels, and real data as inputs to produce a mixture density output. The discriminator D acts as both a classifier and fault detector, distinguishing between real/fake samples and predicting fault labels. N, F1, F2, F3, etc. represent normal and different fault classes [95]. 9\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nFig. 11. Architecture of the proposed 3D-HyperGAMO model for addressing class imbalance in hyperspectral image classification: The model includes a 3D patch extractor, a conditional feature mapping unit, a convex 3D patch generator (G) for minority classes, a 3D conditional discriminator (D), and a 3D classifier network (M). This approach generates new samples for minority classes to improve classification performance on imbalanced datasets [33]. where ml is the number of samples in the majority class, and ms is the number of available samples in the i minority class.\n\n- 3D Conditional Discriminator: Distinguishes between real and\ngenerated 3D patches;\n\n- 3D Classification Network: Categorizes original and generated\npatches into classes. The conditional feature mapping unit and PGU enable controlled generation of new minority class training samples. The discriminator provides adversarial supervision for realistic patches. The PGU generates a class-specific feature matrix Fm for creating new patches using Eq. (22) [33]: Fm = Im ‚Ä¢ (F T\n\nm) (22)\nwhere Im is the random feature matrix and T (Fm) is the transpose of Fm. The overall objective function is a minimax game between the Fig. 12. Flowchart of VAEGAN: Real input data is fed into the encoder and generator (G), discriminator (D), and classifier (M) networks. By opti- decoder components to produce synthesized false samples. The discriminator mizing this function, 3D-HyperGAMO synthesizes additional training then attempts to evaluate the authenticity of the generated samples versus the real data [32]. samples to mitigate imbalance and enhance classification performance. the fused standard deviation, and Œº0 is the fused mean.4.12. Improved variational autoencoder generative adversarial network The mean Œº0 of the fused distribution is calculated as shown in Eq.(improved VAEGAN) (24) [32]: Improved Variational Autoencoder Generative Adversarial Network Œº Œº1œÉ22 + Œº2œÉ2 (improved VAEGAN [32]) is a new oversampling method for handling 0 = 1 œÉ2 (24) 1 + œÉ22 imbalanced data classification problems. It is an improvement over the original VAEGAN model by adding an extra encoder, aiming to enhance The variance œÉ20 of the fused distribution is determined by Eq. (25) the model‚Äôs representation ability to generate more realistic and diverse [32]: minority class data for oversampling. œÉ2 œÉ21œÉ22 Specifically, the improved VAEGAN has two encoders E1 and E2 that 0 = œÉ2 ) 1 + œÉ2 (25 encode the input real data x into mean (Œº1, Œº2) and variance (œÉ21, œÉ2 2\n\n## 2) ( )\n\ncodes respectively. By fusing the outputs of the two encoders, the latent The latent code z is then sampled from the distribution N Œº0, œÉ20 , code z is generated, which is then decoded by the decoder to generate where N represents a normal distribution. fake samples x π. Fig. 12 shows the flowchart of improved VAEGAN. The improved VAEGANmodel is used to generate newminority class samples by training the encoder(s)-decoder flow. The generated samples h π 1 (x‚àí Œº0)2 ‚àí 2 are combined with the original imbalanced dataset to form an (x) = ‚àöÃÖÃÖÃÖÃÖÃÖ e 2œÉ0 (23) 2œÄœÉ0 augmented balanced dataset for training classifiers. Experiments demonstrate improved classification performance compared to original The fusion of the two encoder outputs is achieved by multiplying VAEGAN and other oversampling methods. their probability density functions, as shown in Eq. (23) [32]: where h π(x) represents the fused probability density function, œÉ0 is 10\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\n4.13. Generative adversarial network synthesis for oversampling [ ] [\n\n## (GANSO)\n\n‚èü‚èûK‚èû‚èü ‚ãÖ‚èü‚èûw‚èû‚èü = ‚èü‚èûv K x x ] = k s (1)‚Ä¶k(N)k s ‚èû‚èü (1)‚Ä¶k T v &1 = N\n\n## (N) & ‚àí 1N\n\n## (2N√óC) (C√ó1) (2N√ó1)\n\nGANSO [17] is a novel oversampling method that utilizes Generative (28) Adversarial Networks (GANs) and vector Markov Random Fields (vMRFs) to synthesize realistic instances from a limited number of where K is a matrix containing the feature vectors kx(N) and k s (N),wopt original samples. It aims to address the challenge of training classifiers is the vector of optimal discriminant coefficients, and v is a vector of with extremely small datasets by incorporating structural information labels (+1 for original instances and ‚àí 1 for synthetic instances). The into the generative process. The GANSO architecture consists of two solution is given by Eq. (29) [17]: main components: a generative block and a discriminative block, as illustrated in Fig. 13. The generative block uses the vMRF model to wopt = KT ( KKT )‚àí 1v (29) synthesize surrogates by applying the Graph Fourier Transform (GFT) on an extended graph representation of the data. The discriminative block implements a linear discriminant on features measuring clique similar- 5. If the discriminator achieves an error probability close to 0.5, accept ities between the synthesized and original instances. The two blocks the current synthetic instance. Otherwise, proceed to the generator engage in an iterative competition until the discriminator can no longer step. distinguish between the synthetic and original samples. 6. Update the generator by computing optimal correcting factors fopt,i to The key steps of the GANSO algorithm are as follows: modify the synthetic instance and deceive the discriminator. The corrected feature vectors are computed as shown in Eq. (30) [17]:\n\n1. Construct an extended undirected graph G(Ve, Ee,Ae) from the orig- [ ( ) ( ) ]\nks C T inal graph G(V,E,A),where each sample is assigned to a vertex in Ve, (n,1) ‚â° ks(n,0)f0 = k s1 1 1 C (m,0),x(n) ‚ãÖf0‚Ä¶k s ),x C (m,0 (n) ‚ãÖf0 (30) and edges in Ee connect vertices corresponding to samples from the same or connected segments in the original graph. where f0 is a vector of correcting factors for each clique. The optimal\n\n2. Generate an initial synthetic instance s(m,0) by applying the GFT on a correcting factors are obtained by solving the system of equations in\nselected original instance x(m). The GFT is computed using the ei- Eq. (31) [17]: genvectors Ue of the extended graph Laplacian matrix Le and a di- [ ] [ ]\n\n## T [ ]\n\nagonal matrix Œ¶ with random sign changes, as shown in Eq. (26) ks(1,0)‚ãÖf0‚Ä¶ks(N,0)‚ãÖf0 ‚ãÖw s T opt ,0= 1N ‚áî k(1,0)‚ãÖwopt ,0‚Ä¶ks(N,0)‚ãÖwopt ,0 ‚ãÖ f\n\n## ‚èü‚èû‚èû0‚èü\n\n## [17]: [ ]\n\ns(m,0) = UeŒ¶UHe x(m) (26) = 1N\n\n## (31)\n\nwhere Ue contains the eigenvectors of Le,Œ¶ is a diagonal matrix with random ¬± 1 values, and UHe is the Hermitian transpose of Ue.\n\n3. Compute feature vectors kx(n) and k\ns (n) based on similarities between The solution is given by Eq. (32) [17]: maximal cliques of original and synthetic instances. The feature ( )‚àí 1 fopt,0 = Ks Ks T s s w,0 w,0 Kw,0 [1N] K vectors are defined in Eq. (27) [17]: w,0 [ [ ( ) ( = ks(1,0)‚ãÖwopt,0‚Ä¶ ks ]\n\n## (N,0)‚ãÖW\n\nT opt,0 (32) kx(n) = k x1 1 C ) ] (m),x(n) ‚Ä¶k x C (m),x T (n) [ ks\n\n## ( 1 1 ) (\n\n(n) = k s(m),x(n) ‚Ä¶k sC(m),x\n\n## C ) ]T\n\n## (27)\n\nwhere Ks (n) w,0 is a matrix containing the dot products between the feature vectors ks(N,0) and the discriminant coefficients wopt,0. where k(xc,yc) is a similarity measure between cliques, xc and sC ( )\n\n7. Generate corrected cliques sc = sign f c c\nrepresent the cliques of the original and synthetic instances, (m,i+1) opt,i s an rec nst (m d o ,i ruct ) respectively, and C is the total number of maximal cliques. the updated synthetic instance s(m).\n\n4. Train the discriminator by finding the optimal linear discriminant 8. Repeat steps 4‚Äì7 for a predefined number of iterations or until the\ncoefficients w discriminator cannot distinguish the synthetic instance. opt,i to distinguish between original and synthetic in- stances. The optimization problem is formulated in Eq. (28) [17]: Fig. 13. Schematic diagram of the GANSO architecture: The generative block synthesizes instances using vMRFs and Graph Fourier Transform, while the discriminative block evaluates their authenticity. White arrows indicate inputs; solid arrows show outputs. The iterative process continues until synthetic instances are indistinguishable from original ones, enabling effective oversampling for imbalanced datasets [17]. 11\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nBy incorporating structural information through the vMRF model data scarcity. and leveraging the adversarial learning paradigm, GANSO can effec- As can be observed, techniques like generative adversarial networks tively generate realistic synthetic instances from a limited number of (GANs), variational autoencoders (VAEs), conditional GANs (CGANs), original samples. Experimental results on simulated and real-world conditional VAEs (CVAEs), and Balancing GAN (BAGAN) have been datasets demonstrate GANSO‚Äôs ability to improve classifier perfor- tested on image data, while Adversarially Regularized Autoencoder mance in scenarios with extreme data scarcity, outperforming tradi- (ARAE), Gaussian Generative Adversarial Network (G-GAN), and Med- tional oversampling methods like SMOTE. ical GAN (medGAN) handle textual, numerical and electronic health In summary, GANSO is a promising deep generative approach for record data respectively. Performance is quantified using metrics like oversampling that addresses the challenges of training classifiers with overall precision, recall, F1-scores, group-mean accuracy, balanced ac- extremely small datasets by integrating GAN-based adversarial learning curacy and others depending on context. GANSO, being a relatively new with vMRF-based structural information. Its innovative architecture and approach, has been evaluated using metrics such as classification ac- mathematical formulation make it a valuable tool for enhancing the curacy and learning curves on real-world datasets, showcasing its po- performance of machine learning models in data-scarce domains. tential for enhancing minority class representation. The results showcase some relative strengths and weaknesses of the models. For instance, CGANs achieve stable training but worst test 4.14. Discussions performance among comparable GAN techniques as per the MNIST case study. In contrast, CVAEs produce better F1-scores but have higher Deep generative models can be harder to train due to additional computational overhead. GANSO, on the other hand, leverages the neural network components. The models may also require more data vMRF model to capture structural information and generate realistic compared to simpler oversampling techniques. Overall, though, deep samples, but may face challenges in scalability and training complexity generative minority oversampling is a very promising direction for as the data dimensionality increases. Comparisons on criteria such as handling highly imbalanced datasets across a variety of problem sample quality, diversity, computational complexity, scalability and domains. training stability highlight areas needing improvement across models. The bar chart in Fig. 14 shows the growing attention towards using The structured analysis of various minority oversampling techniques Generative Adversarial Networks (GANs) for oversampling in recent using deep generative models and their characteristics facilitated years. through Table 1 sets the context for further sections discussing current As observed, the frequency of occurrence mention of the keywords challenges, shortcomings, comparative analysis and future opportu- ‚ÄúGAN‚Äù and ‚Äúoversampling‚Äù in scientific papers has notably increased nities in this rapidly evolving subfield of machine learning research. since 2018. Our analysis indicates this rising trend reflects intensifying efforts to harness GANs capabilities to address data imbalance by syn- 5. Challenges of oversampling with deep generative approaches thetically oversampling minority classes. However, open challenges remain around ensuring training stability, Each of explained models has its own distinct advantages and limi- diversity among generated samples, and rigorous, transparent bench- tations w.r.t scalability, training complexity and potential biases which marking. Further research and development are vital to demonstrate should be considered based on data type and application requirements. performance improvements over current approaches on real-world problems. Evaluating advancements on class-specific metrics is critical. 5.1. Scalability Table 1 presents a comparative analysis of various deep generative models that have been employed for minority oversampling to handle GANs are very popular as generative content models for high quality class imbalance. Key criteria used for comparison include the type of and high-resolution images, but their main challenge still remains data the models have been applied on, the datasets utilized in the scalability to very large dataset sizes. Increasing image sizes signifi- referenced studies, the evaluation metrics measured, the reported per- cantly increases the number of parameters required to generate samples, formance results, disadvantages or limitations of the methods, and their leading to increased training time and computational costs. In addition advantages. In addition to the models discussed, the Generative to hardware limitations, the efficiency of very high dimensional neural Adversarial Network Synthesis for Oversampling (GANSO) approach has networks also decreases. VAEs also face scalability challenges with shown promising results in generating realistic minority class samples increasing data dimensions, as the number of parameters and the by incorporating vector Markov Random Field (vMRF) structural in- complexity of the latent space increases rapidly, reducing model effi- formation into the GAN framework. GANSO has been applied to various ciency. Although the extra conditional input in CVAE partially controls data types, including EEG, fMRI, and ECG signals, and has demonstrated this issue, it is still challenging for large datasets. the ability to improve classifier performance in scenarios with extreme BAGAN utilizes both VAE and GAN components, so it has even higher computational costs compared to independent GAN and VAE. Since it requires separate training of VAE and GAN components, it can lead to a significant drop in efficiency at large data dimensions. ARAE has the advantage that unlike GAN and VAE, it does not require model structure adjustments for different input sizes and uses a standard ar- chitecture for different datasets. But very long sequence inputs still cause scalability issues leading to reduced efficiency. G-GAN is designed specifically for small-sized low-dimensional nu- merical data, so when volumes become very large or input dimensions become too high, it faces serious scalability constraints. MoGAN benefits from convolutional architectures that scale well to large image data. It also uses group normalization that accelerates the training. On the other hand, medGAN employs convolutional networks suitable for large im- ages and uses techniques like importance sampling for better compu- tational efficiency. 3D-HyperGAMO is designed for hyperspectral Fig. 14. Growth of Attention to GAN for Oversampling as suggested by Scopus imagery data containing large volumes of spatio-spectral information, indexed articles. and utilizes convolutional networks capable of processing such large 12\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\n13 Table 1 Comparative Analysis of Deep Generative Models for Minority Oversampling. Model Data Type Dataset Evaluation Metric Result Performance at Disadvantages Advantages Different Imbalance Ratios (IR) SMOTE [1] Image MNIST Prec.(Overall) 0.920 IR = 80 (F1 =0.905) Sensitive to hyperparameters Best performance in 4 out of 12 experiments Recall(Overall) 0.906 IR = 320 (F1 =0.392) F1 Score (Overall) 0.905 RANDOM [1] Image MNIST Prec.(Overall) 0.919 IR = 80 (F1 =0.868) May overfit Best performance in 2 out of 12 experiments Recall(Overall) 0.904 IR = 320 (F1 =0.529) F1 Score (Overall) 0.902 CGAN [1] Image MNIST Prec.(Overall) 0.900 IR = 80 (F1 =0.816) Worst performance Stable training Recall(Overall) 0.873 IR = 320 (F1 =0.147) F1 Score (Overall) 0.869 CVAE [1] Image MNIST Prec.(Overall) 0.916 IR = 80 (F1 =0.908) High computational cost Best performance in 6 out of 12 experiments Recall(Overall) 0.909 IR = 320 (F1 =0.662) F1 Score (Overall) 0.908 ADASYN [30] Tabular Adult Dataset F1 Score (Mean) 0.811 IR = 0.28 (F1 =0.537) Applicable only for classification Simple and fast oversampling (discrete) (Std) 0.055 IR = 0.33 (F1 =0.727) tasks VAE [30] Tabular Adult Dataset F1 Score (Mean) 0.820 IR = 0.28 (F1 =0.535) More difficult Stable training process (discrete) (Std) 0.039 IR = 0.001 (F1 =0.820) To train compared to GANs GAN [30] Tabular Adult Dataset F1 Score (Mean) 0.820 IR = 0.33 (F1 =0.734) Risk of mode collapse Generate sharp samples (discrete) (Std) 0.069 IR = 0.001 (F1 =0.820) medGAN [30] Tabular Adult Dataset F1 Score (Mean) 0.822 IR = 0.28 (F1 =0.534) Insufficient training data Privacy (discrete) (Std) 0.042 IR = 0.001 (F1 =0.822) Preservation, faster model prototyping BAGAN [86] Image MNIST SSMI couples Accuracy 0.2‚Äì0.4 0.99 IR= 1.67 (Accuracy = Requires further validation Robustness against data imbalance, prevents CIFAR10 (min-max) 0.1‚Äì0.6 0.70 99 %) mode collapse Flowers 0.1‚Äì0.7 0.75 IR= 40 (Accuracy =\n\n## GTSRB 0.1 0.97 96.5 %)\n\nARAE [88] Binary Text MNIST Yelp Reverse PPL Transfer Accuracy 82.2 81.8 % Performance at Sensitive to hyperparameters, Smoother latent space, Manipulation ability, images SNLI Forward Accuracy 44.3 70.9 % different IR ratios not Challenging for complex Semi-supervised performance PPL reported. structures G-GAN [29] Numerical 11 standard datasets G-mean 0.9152 IR = 1.84 (G- Not suitable for highly Better performance compared to 11 other from KEEL database mean=0.6115) imbalanced numerical data common balancing algorithms\n\n## IR = 66.67\n\n(F1 =0.9997) MoGAN [95] Time series IMS WTFF G-mean Precision 0.9789 0.9427 IR = 1:3 (F1 = 0.95) High computational complexity Better performance compared to existing Dataset Dataset F-measure 0.9064 IR= 1:1000 (F1 = 0.71) methods Recall 0.8902 Balanced Accuracy 99.23 IPF 0.8946 Dataset 0.8730 0.8902\n\n## 97.28\n\n3D- Hyperspectral Indian Pines (IP) Overall Average Kappa 86.96 78.72 85.17 IR = 10 (OA=95.19 %) High computational complexity Significantly improves classification HyperGamo image Keneedy Space Accuracy Accuracy (x100) 95.31 92.26 94.78 IR = 123 due to the use of deep neural performance for minority classes by [33] Center (KSC) 93.9 93.29 91.86 (OA=85.95 %) networks oversampling them during training University of Pavia 97.43 97.4 97.22\n\n## (UP)\n\nBotswana (BW) Improved Numerical Credit card fraud AUC 0.98476 IR= 0.25 (F1 = 0.8775) More complex architecture Generates more diverse data for minority VAEGAN [32] detection dataset Precision 0.8630 IR = 100 (F1 = 0.8793) compared to original VAEGAN class significantly improves precision, f1 (Number Ng=33700 Recall 0.8302 score, combines strengths of VAE and GAN of generated F1 score 0.8463 examples is varied) GANSO [17] Time series, EEG (Barcelona Probability of error 0.5‚Äì0.2‚Äì0.4 IR ratios not reported. Requires structural assumptions Effective for very small datasets (3‚Äì5 Images Test), fMRI 0.47‚Äì0.28 (vMRF model) samples), outperforms SMOTE (OpenNeuro), ECG 0.5 - < 0.3 (UCD Sleep Apnea Database)\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\ndata. complexity, as it involves eigendecomposition of the extended graph GANSO, being a GAN-based approach, also faces scalability chal- Laplacian matrix. However, the structured nature of the vMRF model lenges as the dimensionality of the data increases. The computational can provide some regularization and guidance during the training pro- complexity of training the generative and discriminative blocks in cess, potentially mitigating issues like mode collapse to some extent. GANSO grows with the size of the extended graph representation, which Nevertheless, finding the right balance between the competing objec- can impact its efficiency when dealing with high-dimensional data such tives of the generative and discriminative blocks remains a challenge in as fMRI. However, for lower-dimensional signals like EEG and ECG, GANSO, similar to other GAN-based methods. GANSO may be less affected by scalability issues compared to models handling image data. The incorporation of the vMRF model in GANSO 5.3. Potential biases introduces additional parameters and complexity, further impacting its scalability as the number of nodes and edges grows with the data Since the primary goal of the generator in GAN is to fool the dimensionality. discriminator, it may be more prone to just imitating particular samples On the other hand, improved VAEGAN suffers from similar scal- of the training data that it finds easiest to fool the discriminator with, ability issues as VAE and GAN with increasing data dimensions. While failing to be representative of the actual data diversity especially for the dual encoders compensate for this constraint to some extent, the complex datasets with multiple modes. training complexity grows rapidly making large scale training infeasible. CGAN has a similar goal of fooling the discriminator, hence likely to overfit to such biased patterns from the data like GAN [98,99]. VAEs 5.2. Training complexity also face the risk of overfitting to the observed training samples because of the reconstruction performed using the latent space, potentially GAN training is challenging in itself. Phenomena such as mode leading to poor generalization ability. CVAE may also overfit to the data collapse, non-convergence or high variational training, and sensitivity to as it performs reconstruction of the dependent variable y, requiring hyperparameters are common issues. The simple GAN training mecha- proper hyperparameter tuning to prevent such bias. nism of the interaction between the two networks of generator and BAGAN combines GAN and VAE that compensate for each other‚Äôs discriminator also induces and exacerbates these training problems. limitations - VAE reconstructs data images and estimates the latent space Training CGAN is similar to GAN and involves the interaction between increasing output sample diversity, while GAN‚Äôs interactive adversarial the two networks, hence facing similar issues including mode collapse, training guarantees generated samples to appear realistic. So, this training instability and sensitivity to hyperparameter selection. The combination reduces chances of overfitting and bias. extra conditional input can provide some guided training. ARAE uses an autoencoder, so like other generative content models VAEs face more complexity during inference and loss computation based on autoencoders, it also faces considerable risks of overfitting to compared to GANs. So, the optimization requires techniques like KL observed training samples and losing generalization power. Appropriate annealing to prevent problems like instability and divergence. CVAE mechanisms like tuning the weighting of the equilibrium loss are needed also needs specialized techniques like KL annealing for stable optimi- to reduce bias. The usage of prior distribution information about mi- zation similar to regular VAE. BAGAN training provides more stability norities and bagging allows G-GAN‚Äôs generator to be trained for pro- owing to the autoencoder component used along with GAN. But it re- ducing samples more related to minorities. NOISE allows increasing quires a two-step training process for the separate VAE and GAN sections diversity of produced samples and stretching coverage. adding to computational overhead and training time. Moreover, medGAN relies on strategies like estimating importance, generating considerable exploration of the large parameter space is needed to attain data from a mixture of distributions instead of direct minorities distri- harmonious tuning between the two sections. bution to enhance diversity and reduce bias of produced patterns. 3D- ARAE training is more stable than regular GAN training since it uses HyperGAMO employs a separate classifier to classify the generated an autoencoder for density estimation but very sensitive tuning of the samples, input noise injection, and multi-scale prediction as techniques equilibrium weighting factor Œª is needed to prevent overfitting and to reduce pattern bias and improve their diversity. Using dual encoders properly capture useful features from the data, increasing model and and tuning their fusion by the VAE and GAN parts can potentially reduce hyperparameter tuning complexity. bias in improved VAEGAN But risks of overfitting still exist necessitating The usage of the bagging technique and prior distribution informa- proper tuning. tion allows G-GAN training to be more stable and guided minimizing In the case of GANSO, the generator‚Äôs objective of fooling the mode collapse. medGAN aims to mitigate common GAN training chal- discriminator may lead to a bias towards generating samples that are lenges using techniques like importance estimation, group normaliza- easily confused with the real data, rather than capturing the full di- tion and generating data from a mixture of distributions leading to more versity of the minority class. The vMRF model used in GANSO aims to stable and faster training, but issues like mode collapse still persist. incorporate structural information and dependencies among the data 3D-HyperGAMO employs techniques like input noise injection and points, which can help in generating more representative samples. multi-scale prediction objectives for more robust training. It also uses a However, if the assumed vMRF structure does not accurately reflect the separate classifier for the generated data putting less pressure on the true underlying relationships in the data, it may introduce its own bia- model itself. The usage of dual encoders increases computational ses. The iterative adversarial training process in GANSO can potentially complexity in improved VAEGAN and attaining harmonious tuning be- mitigate some of these biases by continuously updating the generator tween them necessitates exploration of a vast parameter space. But the based on the discriminator‚Äôs feedback. Nevertheless, careful tuning of enhanced representation power somewhat compensates for this hyperparameters and regularization techniques may be necessary to limitation. strike a balance between generating realistic samples and maintaining GANSO, being a GAN-based approach, shares similar training chal- diversity. The limited number of training samples in the minority class lenges such as mode collapse, non-convergence, and sensitivity to can also make GANSO vulnerable to overfitting, as the generator may hyperparameters. The iterative competition between the generative and focus on replicating specific patterns present in the observed data. discriminative blocks in GANSO can be computationally demanding, Techniques such as data augmentation, cross-validation, and early especially when dealing with large extended graph representations. The stopping can be employed to reduce overfitting and improve general- incorporation of the vMRF model adds an additional layer of ization. Additionally, monitoring the diversity of the generated samples complexity, requiring careful tuning of hyperparameters to ensure and using evaluation metrics that capture both the quality and vari- convergence and stability. The use of the Graph Fourier Transform ability of the synthetic data can help in identifying and mitigating po- (GFT) for generating synthetic samples also contributes to the training tential biases in GANSO. 14\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\n6. Shortcomings and potential risks 6.3. Evaluation metrics\n6.1. Generalization GANs need customized metrics testing class-specific performance, biases, feature space coverage, and real-world impact compared to GANs cannot cover full data diversity due to problems like mode benchmarks. Accuracy metrics masking poor minority class handling collapse that reduce variation. They likely miss infrequent but critical and purely sample similarity judgments overlooking problems are minority cases. CGANs share the same deficiencies as GANs. The con- inadequate. CGANs share concerns, needing minority-class focused ditional input provides some guided training but does not ensure analysis. Aggregate performance metrics are insufficient. generalization or minority coverage. VAEs face latent space overfitting, VAEs require careful class-level metrics analyzing minority handling hampering generalization to new test data. Posterior collapse reduces to prevent inflated perceived metrics from overfitting issues like poste- their output diversity. CVAEs risk overfitting on reconstruction, limiting rior collapse. Aggregate similarity metrics overlook generalization dif- generalization. High KL costs can make them disregard the latent code, ficulties and must be supplemented with classification performance affecting diversity. BAGAN combines GAN & VAE strengths to improve impact analysis. CVAEs conceal overfitting problems if reconstruction generalization but requires extensive tuning. Like autoencoders, ARAE similarity alone is used without class-specific impact studies. Imbal- can overfit to training data, requiring preventative tuning. G-GAN has anced classification metrics are mandatory. scalability issues that constrain its generalization. Strategies in medGAN BAGAN‚Äôs high compute requirements affect properly tuning and help generalization, but problems like mode collapse persist. Input in- cross-validating it for reliability. Customized hybrid evaluators jection for 3D-HyperGAMO does not guarantee accurate conformity to analyzing GAN and VAE components separately on metrics like sample actual distribution. MoGAN leverages techniques like group normali- diversity before integrated model assessments are imperative. Proper zation for stability and convolutional architectures for scalability. analysis of fusion tuning impact is critical. ARAE needs customized However, it shares GAN risks like mode collapse that limit full gener- frameworks ensuring analysis of training data memorization issues, alization, especially on minority classes. The dual encoder approach of equilibrium tuning impact, biases, and feature space coverage through improved VAEGAN aims for compensation but high complexity affects both automated metrics and qualitative human assessment. Over- reliable large-scale generalization. GANSO‚Äôs generalization ability de- reliance on aggregate similarity metrics must be avoided. pends on the effectiveness of the vMRF model in capturing the under- G-GAN has narrow specific applicability limiting evaluations on real lying structure of the minority class. If the assumed vMRF does not complex data. Tailored analysis for distribution characteristics of under- accurately represent the true data dependencies, GANSOmay struggle to represented minorities is key. medGAN‚Äôs group normalizers need generate diverse and representative samples that generalize well to balanced class-specific multi-scale evaluations. Mixture density training unseen data. The limited number of minority class instances can also necessitates metrics ensuring minority coverage. 3D-HyperGAMO‚Äôs hinder GANSO‚Äôs generalization capability, as it may overfit to specific separate classifier keeps models isolated necessitating customized patterns in the training data. evaluators revealing true enhancements in generative model capability. MoGAN‚Äôs deep 3D-CNNs require procedures testing model handling 6.2. Overfitting when real-world variability is introduced in new unseen imbalanced cases. Extensive dual encoder tuning means improved VAEGAN needs GAN tends to overfit to particular training samples that easily fool customized non-aggregated metrics analyzing fusion model impact, not the discriminator, losing diversity and generalization capability. CGAN just similarity metrics. shares the overfitting tendencies of regular GANs. VAE risks overfitting For GANSO, evaluation metrics should focus on assessing the quality in its latent space, hampering generalization. Posterior collapse exac- and diversity of the generated minority class samples and their impact erbates this. CVAE can overfit on reconstruction of the dependent var- on improving classification performance. Class-specific metrics, such as iable. High KL costs increase risks of posterior collapse and overfitting. precision, recall, and F1-score, should be used to evaluate the classifier‚Äôs BAGAN combines GAN and VAE strengths, compensating limitations to performance on the minority class after oversampling. Additionally, reduce overfitting. But extensive tuning for optimal fusion of the com- metrics that capture the diversity and representativeness of the gener- ponents is computationally demanding. Like autoencoders, ARAE can ated samples, such as MMD or FID, can provide insights into how well easily overfit to training data. Sensitive tuning of equilibrium loss GANSO captures the underlying distribution. Visual inspection and weighting is required. G-GAN employs bagging and prior distribution domain expert feedback can also help assess the realism and plausibility information for more guided training with less overfitting. medGAN uses of the generated samples in the specific application context. techniques like importance estimation from mixture densities to mini- mize overfitting risks. MoGAN exploits group normalization and con- 7. Comparative analysis volutional architectures that provide regularization but does not eliminate underlying GAN overfitting risks [100]. Using a separate Traditional oversampling methods such as SMOTE, ADASYN and classifier in 3D-HyperGAMO reduces reliance on the model alone, random oversampling take a simple approach of rebalancing class dis- mitigating overfitting pressures. Tuning dual encoders in improved tributions in imbalanced datasets by increasing minority class samples VAEGAN targets reducing overfitting but significant tuning is essential. through replication or generation from existing samples. While GANSO, like other GAN-based models, is susceptible to overfitting, computationally cheap and intuitive, they tend to overfit limited mi- especially when trained on a limited number of minority class samples. nority data and lack diversity. Their scope is also mostly restricted to The generator may memorize specific patterns in the training data low-dimensional data. In contrast, deep generative models like GANs, instead of learning the overall distribution, leading to poor generaliza- VAEs, CGANs, CVAEs, BAGAN, medGAN, ARAE, G-GAN, MoGAN, 3D- tion. The vMRF model in GANSO can provide some regularization, but it HyperGAMO, Improved VAEGAN, and GANSO leverage complex neu- may not completely eliminate the overfitting risk if the assumed struc- ral networks to implicitly model intricate data distributions and sample ture is too simplistic or fails to capture the true data dependencies. new, realistic and diverse points from it. By capturing the authentic In general, adequate regularization and tuning controls are vital in underlying statistics, they significantly reduce overfitting and provide all models to account for overfitting risks. Complex models like BAGAN better generalization over traditional techniques. These powerful black- and GANSO require extensive tuning focus. Techniques like ARAE‚Äôs box models can scale to high-dimensional visual data, but require equilibrium tuning, G-GAN‚Äôs guided training, GANSO‚Äôs vMRF regula- extensive expertise and computing resources for effective training and rization, and MoGAN‚Äôs normalizer regularization help but no model is tuning due to architectural complexity. inherently immune to overfitting risks. An important observation from Table 1 is the varying performance of 15\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\ndifferent methods across imbalance ratios (IR). Traditional methods like with intricate visual details and diversity through their moderate-sized SMOTE and Random oversampling show high sensitivity to imbalance generator-discriminator networks, but adversarial training often cau- ratios, with SMOTE‚Äôs F1-scores dropping significantly from 0.905 at ses mode collapse and diminished diversity. VAEs enable interpretable IR= 80‚Äì0.392 at IR= 320. Deep generative approaches demonstrate latent manipulation and reasoning with their compact encoder-decoder varying levels of resilience to high imbalance ratios. Some models show structure, but reconstruct complex images poorly. CGANs and CVAEs, remarkable stability, such as Improved VAEGAN maintaining consistent with their larger architectures through additional class embedding performance (F1 ‚âà0.87) from IR= 0.25 to IR= 100, and BAGAN layers, improve conditional sample specificity yet still inherit GAN showing minimal degradation (Accuracy dropping only 2.5 % from training instability issues like mode collapse. BAGAN effectively cap- IR=1.67 to IR=40). However, others like CGAN exhibit high sensitivity tures the heterogeneous and sparse spatial-spectral distributions of sat- with F1-score declining substantially from 0.816 at IR= 80‚Äì0.147 at ellite imagery with few minority samples, utilizing substantial IR= 320. Models like MoGAN and 3D-HyperGAMO show moderate parameters shared between autoencoder and GAN components, but re- sensitivity, with MoGAN‚Äôs F1-score decreasing from 0.95 to 0.71 at quires data preprocessing and feature extraction and cannot operate on extreme ratios (IR=1:1000) and 3D-HyperGAMO‚Äôs accuracy declining raw data. medGAN employs a medium-sized architecture and is robust from 95.19 % to 85.95 % as IR increases from 10 to 123. Notably, some to severe imbalance in medical records but has binary classification models like medGAN and G-GAN demonstrate low sensitivity, even constraints and requires structured data. ARAE can generate diverse showing improved performance at higher imbalance ratios. This varying texts conditioned on labels through its LSTM-based framework but de- response to imbalance ratios suggests that while some deep generative mands complex architectures. G-GAN, with its lightweight structure, models can effectively handle class imbalances, their effectiveness varies generates new minority samples from imbalanced numerical data, significantly across different architectures and domains. These findings though it struggles with extreme imbalance ratios. MoGAN enables align with the sensitivity ratings provided in Table 2, where deep simultaneous machine fault detection and classification through learning-based methods show a spectrum from low to high sensitivity to specialized convolutional architectures but is domain limited. 3D- imbalance ratios, with newer architectures generally showing better HyperGAMO improves spectral-spatial satellite image classification resilience to high imbalance scenarios. performance but needs more complex neural architectures and higher To facilitate a multi-dimensional comparison of these oversampling computational costs. Improved VAEGAN combines two generators for strategies, we present Table 2 which evaluates both traditional and deep better fraud sample generation at the cost of increased complexity over generative approaches across several key criteria. These criteria were vanilla VAEs and GANs. GANSO maintains a highly efficient structure selected to provide a comprehensive view of each method‚Äôs strengths through its vMRF-based approach while combining the strengths of and limitations in addressing the challenges of imbalanced data GANs and vector Markov Random Fields to synthesize realistic minority classification. class samples even with extremely small training sets. It demonstrates The criteria in Table 2 offer insights into various aspects of each improved classifier performance over traditional methods when dealing method‚Äôs performance and applicability. The number of parameters with very limited data through iterative adversarial training, though it indicates model complexity and computational demands, ranging from requires careful vMRF structure design. GANSO shows particular non-parametric approaches to architectures with large-scale parame- promise for biomedical applications involving EEG, fMRI, and ECG data, ters. This parameter count directly impacts both training time and where obtaining large labeled datasets is often difficult and expensive, memory requirements, with larger models typically requiring extended despite potential scalability challenges with high-dimensional data. training periods on specialized hardware, while smaller models can be Among traditional methods, SMOTE is a simple and easy to imple- trained more quickly on standard computing resources. Data scarcity ment method for increasing the number of minority samples but overfits robustness indicates performance with limited samples. The data type limited minority data and lacks diversity of generated samples as well as handled shows the versatility of each approach. Sensitivity to high ineffectiveness in higher dimensions and continuous data like images. imbalance ratio is crucial for severely skewed datasets. Generated data While ADASYN performs targeted sampling based on difficulty of diversity is essential for creating representative synthetic samples. learning and prevents overfitting, scarce diversity of generated samples Scalability assesses the method‚Äôs applicability to large-scale problems. is observed. Both amplify existing samples. Random oversampling es- Specifically, GANs can produce high quality, realistic image samples tablishes implementation simplicity and class balance but likely causes Table 2 Multi-dimensional Comparison of Oversampling Strategies. Model Number of Data Scarcity Data Type Handled Sensitivity to High Imbalance Generated Data Scalability Parameters Robustness Ratio Diversity SMOTE N/A (Non- Low Numerical, Low- High Low High parametric) dimensional ADASYN N/A (Non- Low Numerical, Low- Medium Low High parametric) dimensional RANDOM N/A (Non- Low Image, Text High Low High parametric) GAN ~1.1 M Medium Image, Text Medium High Medium CGAN ~13.18 M Medium Image, Text, Conditional High High Medium VAE ~0.85 M Medium Image, Text Medium Medium Medium CVAE ~13.34 M Medium Image, Text, Conditional Medium Medium Medium BAGAN ~13.15 M High Image Low High Low medGAN ~5 M High Discrete Medical Data Low High Medium ARAE ~1.65 M Medium Text, Discrete Data Medium High Medium G-GAN ~3.2 K High Numerical, Low- Low Medium Low dimensional MoGAN ~1.5 M High Time Series Medium High Medium 3D-HyperGAMO ~8.5 M High Hyperspectral Images Medium High Low Improved ~131 K High Numerical Low High Medium\n\n## VAEGAN\n\nGANSO ~494 High Time Series, Image Medium High Medium 16\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\noverfitting due to replication without adding new information to For MoGAN reducing dimensionality could improve fault diagnosis training data. under different conditions. Creating a new loss function to handle Comparing these methods presents several challenges due to the mixture data distributions would enrich generator performance and diversity of data types, imbalance ratios, and application domains. This improve the discriminator‚Äôs detection abilities. diversity makes it impractical to provide a single set of experimental With 3D-HyperGAMO testing how the model generalizes to new results that would be universally applicable or meaningful across all hyperspectral datasets and comparing against recent deep learning scenarios. Performance can vary significantly based on these factors, techniques like attention-based networks could further guide architec- making direct comparisons complex. For instance, while deep genera- ture optimization. Researchers could also explore semi-supervised ex- tive models excel in high-dimensional spaces, traditional methods may tensions and adapt the 3D oversampling idea to other modalities. be more effective for simpler, low-dimensional data. The choice of For the improved VAEGAN boosting recall and AUC via model ar- method often depends on the specific requirements of the task, available chitecture changes and advanced sampling strategies should be prior- computational resources, and the nature of the dataset. Given these ities. Testing on more real-world problems will demonstrate broad challenges, a more effective approach to comparing these methods in- usefulness across classification tasks involving imbalance. volves analyzing their theoretical foundations, computational re- For GANSO, future research could focus on optimizing the vMRF quirements, and reported performance across various case studies in the structure design for different data types and domains. Investigating literature, rather than attempting to provide a single set of experimental adaptive or learnable vMRF structures could enhance the method‚Äôs results. Consequently, we have focused on comparing the general flexibility and generalization ability. Extending GANSO to handle multi- characteristics and applicability of these methods across various class imbalance problems and exploring its applicability to other data scenarios. modalities beyond EEG, fMRI, and ECG would broaden its impact. Re- In summary, while traditional techniques provide an accessible searchers could also work on improving GANSO‚Äôs scalability to higher- starting point, they falter on complex high-dimensional data. Deep dimensional data and larger datasets. Comparative studies against state- generative models can accurately model intricate data distributions for of-the-art few-shot learning methods would help position GANSOwithin intelligent sampling but require greater expertise and computing re- the broader context of learning from limited data. Additionally, devel- sources to harness their power. These complementary paradigms can be oping theoretical guarantees for GANSO‚Äôs performance under different innovatively combined into robust and sample-efficient solutions that data distributions and imbalance ratios could provide valuable insights. augment minority classes effectively across the data complexity Exploring the integration of GANSO with other deep learning architec- spectrum. tures, such as attention mechanisms or graph neural networks, might lead to improved performance in specific domains.\n\n8. Future directions and recommendations\n9. Conclusion\nFor BAGAN future efforts could focus on applying the methodology to additional classification tasks and datasets with imbalanced training This survey has analyzed various oversampling techniques using data to further validate its usefulness. Researchers could also explore deep generative models for handling class imbalance in machine modifications or extensions to the BAGAN framework itself to generate learning. Compared to traditional methods like SMOTE and ADASYN higher quality synthetic images and improve classifier performance. that simply replicate or interpolate minority samples, advanced deep For medGAN priority areas are developing a sequential version that models such as GANs, VAEs, and their variants (CGANs, CVAEs, BAGAN, can model patient data over time and expanding the types of data that medGAN, ARAE, G-GAN, MoGAN, 3D-HyperGAMO, Improved VAE- medGAN can synthesize, including lab results, demographics, and free- GAN, and GANSO) have demonstrated significant potential in capturing text notes. This will enhance the realism and usefulness of the generated intricate minority data distributions for intelligent, diverse sample outputs. generation. Researchers could evaluate ARAE‚Äôs scalability on more complex These deep generative approaches have shown promising results long-form discrete sequences like documents to test robustness. across various domains, including image classification, medical di- Reducing gaps between practical training objectives and theoretical agnostics, and fraud detection. Their ability to generate high-quality, optimal transport objectives could enhance performance. Additional realistic samples that maintain the underlying statistical properties of latent space regularization and structuring may enable more controlled minority classes represents a substantial advancement in addressing manipulations with improved interpretability. Comparing ARAE against class imbalance. For instance, models like BAGAN and 3D-HyperGAMO a wider range of recent discrete latent variable models would better have shown particular promise in handling imbalanced image datasets, highlight ARAE‚Äôs strengths and limitations to guide progress. while medGAN and GANSO have demonstrated effectiveness in gener- In terms of recommendations, researchers could release ARAE code ating synthetic medical data and time series, respectively. implementations to accelerate research. Testing model resilience on However, it is important to acknowledge that these advanced real-world discrete datasets would also evaluate scalability. Clearly methods also present new challenges. Scalability issues due to archi- reporting differences between theoretical and practical training objec- tectural complexity can constrain real-world applicability, especially for tives would support reproducibility. very large datasets. The delicate training procedures of some models, Further suggestions include adapting ARAE for more discrete prone to instability and overfitting, necessitate extensive tuning and sequence modeling tasks beyond initial experiments to demonstrate expertise. Additionally, the use of aggregate evaluation metrics may versatility. Enhancing discretization schemes for modalities like video or sometimes mask model deficiencies, calling for more nuanced, class- audio could widen applications. Developing specialized ARAE model specific assessments. architectures and training procedures for different data types could also Despite these challenges, the field has made substantial progress. improve results. Comprehensive benchmarks gauging reconstruction Innovative solutions combining complementary paradigms, such as the quality, manipulation precision, and computational efficiency would integration of GANs with vector Markov Random Fields in GANSO, or identify areas needing work. the dual encoder approach in improved VAEGAN, showcase the poten- Researchers can stretch the idea of G-GAN by extracting more in- tial for oversampling techniques that can scale across data complexity formation to inject into the GAN, adapting it for multi-class imbalance levels. These advancements are particularly crucial as datasets grow problems, and improving stability for small numerical datasets. more imbalanced and complex, necessitating robust, efficient, and Comparing against newer GAN-based solutions will also help advance transparent oversampling methods for unbiased model development. this approach. Looking forward, addressing these critical shortcomings is 17\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nimperative before large-scale deployment. Innovative solutions [4] N.V. Chawla, K.W. Bowyer, L.O. Hall, and W.P. Kegelmeyer, ‚ÄúSMOTE: Synthetic combining complementary paradigms could lead to oversampling Minority Over-sampling Technique,‚Äù 2002. [5] T. Feizi, M.H. Moattar, H. Tabatabaee, M2GDL: Multi-manifold guided dictionary techniques that scale across data complexity. As datasets grow more learning based oversampling and data validation for highly imbalanced imbalanced, developing robust, efficient and transparent oversampling classification problems, Inf. Sci. 682 (2024) 121280, https://doi.org/10.1016/j. is vital for unbiased model development. Techniques accurately repre- ins.2024.121280. [6] T. Feizi, M.H. Moattar, H. Tabatabaee, A multi-manifold learning based instance senting minority groups in training data will be integral to ensuring fair weighting and under-sampling for imbalanced data classification problems, J. Big and ethical AI systems. Data 10 (1) (Dec. 2023) 1‚Äì36, https://doi.org/10.1186/S40537-023-00832-2/ In conclusion, while challenges remain, the progress in deep gener- TABLES/24. ative oversampling techniques represents a significant step forward in [7] M. Buda, A. Maki, M.A. Mazurowski, A systematic study of the class imbalance problem in convolutional neural networks, Neural Netw. 106 (Oct. 2018) addressing class imbalance. As these methods continue to evolve, they 249‚Äì259, https://doi.org/10.1016/J.NEUNET.2018.07.011. hold great promise for enabling more accurate and fair machine learning [8] I.J. Goodfellow et al., ‚ÄúGenerative Adversarial Networks,‚Äù Jun. 2014, [Online]. models across a wide range of applications. The path forward involves Available: „Äàhttp://arxiv.org/abs/1406.2661„Äâ. [9] A.K. Gangwar, V. Ravi, WiP: Generative Adversarial Network for Oversampling interdisciplinary efforts uniting deep generative modeling expertise Data in Credit Card Fraud Detection, in: Lecture Notes in Computer Science with domain knowledge and ethical considerations. By continuing to (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in innovate and refine these techniques, we can work towards AI systems Bioinformatics), 11952, LNCS, 2019, pp. 123‚Äì134, https://doi.org/10.1007/978- 3-030-36945-3_7. that are not only powerful but also equitable and representative of all [10] W. Jo, D. Kim, OBGAN: Minority oversampling near borderline with generative data classes. adversarial networks, Expert Syst. Appl. 197 (Jul. 2022), https://doi.org/ 10.1016/j.eswa.2022.116694. [11] O. Dayan, L. Wolf, F. Wang, Y. Harel, Optimizing AI for Mobile Malware Ethical and informed consent for data used Detection by Self-Built-Dataset GAN Oversampling and LGBM, Proc. 2023 IEEE Int. Conf. Cyber Secur. Resil., CSR 2023 (2023) 60‚Äì65, https://doi.org/10.1109/ No ethical concern exists. CSR57506.2023.10224927. [12] D.P. Kingma and M. Welling, ‚ÄúAuto-Encoding Variational Bayes,‚Äù Dec. 2013, [Online]. Available: „Äàhttp://arxiv.org/abs/1312.6114„Äâ. Funding [13] C. Liu, X. Wang, K. Wu, J. Tan, F. Li, W. Liu, Oversampling for imbalanced time series classification based on generative adversarial networks, 2018 IEEE 4th Int. Conf. Comput. Commun., ICCC 2018 (2018) 1104‚Äì1108, https://doi.org/ The authors declare that no funding was received for this research. 10.1109/CompComm.2018.8780808. [14] W. Bouzeraib, A. Ghenai, N. Zeghib, A Multi-Objective Genetic GAN CRediT authorship contribution statement Oversampling: Application to Intelligent Transport Anomaly Detection, Proc. - 2020 IEEE 22nd Int. Conf. High. Perform. Comput. Commun., IEEE 18th Int. Conf. Smart City IEEE 6th Int. Conf. Data Sci. Syst., HPCC-SmartCity-DSS 2020 (2020) Mozafar Hayaeian Shirvan:Writing ‚Äì original draft, Investigation, 1142‚Äì1149, https://doi.org/10.1109/HPCC-SmartCity-DSS50907.2020.00148. Formal analysis. Mohammad Moattar: Writing ‚Äì review & editing, [15] T. Miftahushudur, B. Grieve, H. Yin, Permuted KPCA and SMOTE to Guide GAN- based oversampling for imbalanced HSI Classification, IEEE J. Sel. Top. Appl. Methodology, Formal analysis.Mehdi Hosseinzadeh:Writing ‚Äì review Earth Obs. Remote Sens 17 (2024) 489‚Äì505, https://doi.org/10.1109/ & editing, Resources. JSTARS.2023.3326963. [16] H. Ba, ‚ÄúImproving Detection of Credit Card Fraudulent Transactions using Generative Adversarial Networks,‚Äù Jul. 2019, [Online]. Available: „Äàhttp://arxiv. Declaration of Competing Interest org/abs/1907.03355„Äâ. [17] A. Salazar, L. Vergara, G. Safont, Generative adversarial networks and markov random fields for oversampling very small training sets, Expert Syst. Appl. 163 The authors declare that they have no known competing financial (Jan. 2021), https://doi.org/10.1016/j.eswa.2020.113819. interests or personal relationships that could have appeared to influence [18] S. Barutcu, A.K. Katsaggelos, and D. G√ºrsoy, ‚ÄúA Deep Generative Approach to the work reported in this paper Oversampling in Ptychography,‚Äù Jul. 2022, [Online]. Available: „Äàhttp://arxiv. org/abs/2207.14392„Äâ. [19] M. Dierolf, et al., Ptychography & lensless X-ray imaging, Europhys. N. 39 (1) Acknowledgements (Jan. 2008) 22‚Äì24, https://doi.org/10.1051/EPN:2008003. [20] J. Miao, R.L. Sandberg, C. Song, Coherent x-ray diffraction imaging, IEEE J. Sel. Top. Quantum Electron. 18 (1) (2012) 399‚Äì410, https://doi.org/10.1109/ Not applicable. JSTQE.2011.2157306. [21] M. Lopez-Martin, B. Carro, A. Sanchez-Esguevillas, J. Lloret, Conditional variational autoencoder for prediction and feature recovery applied to intrusion Authors contribution statement detection in iot, Sens. (Switz. ) 17 (9) (Sep. 2017), https://doi.org/10.3390/ s17091967. All authors contribute to the preparation of the article. [22] E. Choi, S. Biswal, B. Malin, J. Duke, W.F. Stewart, and J. Sun, ‚ÄúGenerating Multi- label Discrete Patient Records using Generative Adversarial Networks,‚Äù Mar. 2017, [Online]. Available: „Äàhttp://arxiv.org/abs/1703.06490„Äâ. Use of experimental animals, and human participants [23] A. Salazar, L. Vergara, G. Safont New applications of an oversampling method based on generative adversarial networks Proc. - 2020 Int. Conf. Comput. Sci. Comput. Intell., CSCI 2020, in Proceedings - 2020 International Conference on No animals and human participants were involved in this study in Computational Science and Computational Intelligence, CSCI 2020, 2020, pp. any kind. 1699 ‚Äì 1701. doi: 10.1109/CSCI51800.2020.00314.2020. [24] W. Zhang, X. Li, X.D. Jia, H. Ma, Z. Luo, X. Li, Machinery fault diagnosis with imbalanced data using deep generative adversarial networks, Measurement 152 Data Availability (Feb. 2020) 107377, https://doi.org/10.1016/J.MEASUREMENT.2019.107377. [25] M. Arjovsky and L. Bottou, ‚ÄúTowards Principled Methods for Training Generative No data was used for the research described in the article. Adversarial Networks,‚Äù Jan. 2017, [Online]. Available: „Äàhttp://arxiv. org/abs/1701.04862„Äâ. [26] C. Liao, M. Dong, Acwgan: an auxiliary classifier wasserstein gan-based References oversampling approach for multi-class imbalanced learning, Int. J. Innov. Comput., Inf. Control 18 (3) (2022) 703‚Äì721, https://doi.org/10.24507/ [1] V.A. Fajardo, et al., On oversampling imbalanced data with deep conditional ijicic.18.03.703. generative models, Expert Syst. Appl. 169 (May 2021), https://doi.org/10.1016/ [27] K. Sohn, X. Yan, and H. Lee, ‚ÄúLearning Structured Output Representation using j.eswa.2020.114463. Deep Conditional Generative Models.‚Äù [2] G. Douzas, F. Bacao, Effective data generation for imbalanced learning using [28] Z. Han, C. Shen, Y. Zhang, H. Wang, L. Yu, Data-driven fault detection of rotating conditional generative adversarial networks, Expert Syst. Appl. 91 (Jan. 2018) machinery using synthetic oversampling and generative adversarial network, 464‚Äì471, https://doi.org/10.1016/j.eswa.2017.09.030. Proc. SPIE - Int. Soc. Opt. Eng. (2023), https://doi.org/10.1117/12.2674215. [3] R.A. Nugraha, H.F. Pardede, A. Subekti, Oversampling based on generative [29] Y. Zhang, Y. Liu, Y. Wang, J. Yang, An ensemble oversampling method for adversarial networks to overcome imbalance data in predicting fraud insurance imbalanced classification with prior knowledge via generative adversarial claim, Kuwait J. Sci. 49 (2022), https://doi.org/10.48129/kjs.splml.19119. 18\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\nnetwork, Chemom. Intell. Lab. Syst. 235 (Apr. 2023), https://doi.org/10.1016/j. [57] R. Blagus, L. Lusa, SMOTE for high-dimensional class-imbalanced data, BMC chemolab.2023.104775. Bioinforma. 14 (2013) 106, https://doi.org/10.1186/1471-2105-14-106. [30] R.D. Camino, R. State, and C.A. Hammerschmidt, ‚ÄúOversampling Tabular Data [58] B. Das, N.C. Krishnan, D.J. Cook, RACOG and wRACOG: two probabilistic with Deep Generative Models: Is it worth the effort?‚Äù [Online]. Available: „Äàhttp:// oversampling techniques, IEEE Trans. Knowl. Data Eng. 27 (1) (2015) 222‚Äì234, archive.ics.uci.edu/ml/datasets/adult„Äâ. https://doi.org/10.1109/TKDE.2014.2324567. [31] M. Mirza and S. Osindero, ‚ÄúConditional Generative Adversarial Nets,‚Äù Nov. 2014, [59] S. Bond-Taylor, A. Leach, Y. Long, C.G. Willcocks, Deep generative modelling: a [Online]. Available: „Äàhttp://arxiv.org/abs/1411.1784„Äâ. comparative review of VAEs, GANs, normalizing flows, energy-based and [32] Y. Ding, W. Kang, J. Feng, B. Peng, A. Yang, Credit card fraud detection based on autoregressive models, IEEE Trans. Pattern Anal. Mach. Intell. 44 (11) (2022) improved variational autoencoder generative adversarial network, IEEE Access 7327‚Äì7347, https://doi.org/10.1109/TPAMI.2021.3116668. 11 (2023) 83680‚Äì83691, https://doi.org/10.1109/ACCESS.2023.3302339. [60] X. Mao, Q. Li, H. Xie, R.Y.K. Lau, Z. Wang, S.P. Smolley, Least squares generative [33] S.K. Roy, J.M. Haut, M.E. Paoletti, S.R. Dubey, A. Plaza, Generative adversarial adversarial networks, Proc. IEEE Int. Conf. Comput. Vis. 2017-October (2017) minority oversampling for spectral-spatial hyperspectral image classification, 2813‚Äì2821, https://doi.org/10.1109/ICCV.2017.304. IEEE Trans. Geosci. Remote Sens. 60 (2022), https://doi.org/10.1109/ [61] A. Creswell, A.A. Bharath, Inverting the generator of a generative adversarial TGRS.2021.3052048. network, IEEE Trans. Neural Netw. Learn Syst. 30 (7) (2016) 1967‚Äì1974, https:// [34] A. Salazar, L. Vergara, E. Vidal, A proxy learning curve for the Bayes classifier, doi.org/10.1109/TNNLS.2018.2875194. Pattern Recognit. 136 (Apr. 2023), https://doi.org/10.1016/j. [62] Y. Sun, M.S. Kamel, A.K.C. Wong, Y. Wang, Cost-sensitive boosting for patcog.2022.109240. classification of imbalanced data, Pattern Recognit. 40 (12) (2007) 3358‚Äì3378, [35] A. Koivu, M. Sairanen, A. Airola, T. Pahikkala, Synthetic minority oversampling https://doi.org/10.1016/j.patcog.2007.04.009. of vital statistics data with generative adversarial networks, J. Am. Med. Inform. [63] Y.O. Lee, J. Jo, J. HwangApplication of deep neural network and generative Assoc. 27 (11) (Nov. 2020) 1667‚Äì1674, https://doi.org/10.1093/jamia/ocaa127. adversarial network to industrial maintenance: A case study of induction motor [36] Z. Wei, Y. Fu, W. Shi, D. ChenOversampling algorithm based on generative fault detection Proc. - 2017 IEEE Int. Conf. Big Data, Big Data 2017, Vol. 2018- adversarial network in Proceedings of SPIE - The International Society for Optical Jan., Proceedings - 2017 IEEE International Conference on Big Data, Big Data Engineering, 2023, 10.1117/12.2684591. 2017, vol. 2018-January, pp. 3248‚Äì3253, Jul. 2017, doi: 10.1109/ [37] A. FernaÃÅndez, S. Garc√≠a, F. Herrera, and N.V. Chawla, ‚ÄúSMOTE for Learning from BIGDATA.2017.8258307.Jul. 2017, 3248325310.1109/ Imbalanced Data: Progress and Challenges, Marking the 15-year Anniversary,‚Äù BIGDATA.2017.8258307.\n\n2018. [64] M. Frid-Adar, I. Diamant, E. Klang, M. Amitai, J. Goldberger, H. Greenspan, GAN-\n[38] G. Haixiang, L. Yijing, J. Shang, G. Mingyun, H. Yuanyue, G. Bing, Learning from based synthetic medical image augmentation for increased CNN performance in class-imbalanced data: review of methods and applications, Expert Syst. Appl. 73 liver lesion classification, Neurocomputing 321 (Dec. 2018) 321‚Äì331, https:// (May 2017) 220‚Äì239, https://doi.org/10.1016/J.ESWA.2016.12.035. doi.org/10.1016/J.NEUCOM.2018.09.013. [39] Z. Pu, D. Cabrera, R.-V. SaÃÅnchez, M. Cerrada, C. Li, J.V. de Oliveira, Exploiting [65] T. Zhou, W. Liu, C. Zhou, L. Chen GAN-based semi-supervised for imbalanced generative adversarial networks as an oversampling method for fault diagnosis of data classification 2018 4th Int. Conf. Inf. Manag., ICIM 2018; 2018 4th an industrial robotic manipulator, Appl. Sci. (Switz. ) 10 (21) (2020) 1‚Äì21, International Conference on Information Management, ICIM 2018, pp. 17‚Äì21, https://doi.org/10.3390/app10217712. Jun. 2018, doi: 10.1109/INFOMAN.2018.8392662Jun. 2018, 172110.1109/ [40] Y. Guo, G. Xiong, Z. Li, J. Shi, M. Cui, G. GouCombating imbalance in network INFOMAN.2018.8392662. traffic classification using gan based oversampling 2021 IFIP Netw. Conf., IFIP [66] J. Yang, Z. Jiang, T. Pan, Y. Chen, W. Pedrycz, Oversampling method based on Netw. 2021, in 2021 IFIP Networking Conference, IFIP Networking 2021, 2021. GAN for tabular binary classification problems, Intell. Data Anal. 27 (5) (2023) doi: 10.23919/IFIPNetworking52078.2021.94727772021. 1287‚Äì1308, https://doi.org/10.3233/IDA-220383. [41] A. FernaÃÅndez, V. LoÃÅpez, M. Galar, M.J. Del Jesus, F. Herrera, Analysing the [67] G. KovaÃÅcs, An empirical comparison and evaluation of minority oversampling classification of imbalanced data-sets with multiple classes: Binarization techniques on a large number of imbalanced datasets, Appl. Soft Comput. 83 (Oct. techniques and ad-hoc approaches, Knowl. Based Syst. 42 (2013) 97‚Äì110, 2019), https://doi.org/10.1016/J.ASOC.2019.105662. https://doi.org/10.1016/J.KNOSYS.2013.01.018. [68] Y. Yan, R. Liu, Z. Ding, X. Du, J. Chen, Y. Zhang, A parameter-free cleaning [42] H. He, E.A. Garcia, Learning from imbalanced data, IEEE Trans. Knowl. Data Eng. method for SMOTE in imbalanced classification, IEEE Access 7 (2019) 21 (9) (2009) 1263‚Äì1284, https://doi.org/10.1109/TKDE.2008.239. 23537‚Äì23548, https://doi.org/10.1109/ACCESS.2019.2899467. [43] W. Han, Z. Huang, S. Li, Y. Jia, Distribution-sensitive unbalanced data [69] N. Verbiest, E. Ramentol, C. Cornelis, F. Herrera, Preprocessing noisy imbalanced oversampling method for medical diagnosis, J. Med Syst. 43 (2) (2019) 1‚Äì10, datasets using SMOTE enhanced with fuzzy rough prototype selection, Appl. Soft https://doi.org/10.1007/S10916-018-1154-8. Comput. 22 (2014) 511‚Äì517, https://doi.org/10.1016/J.ASOC.2014.05.023. [44] J. Van Hulse, T.M. Khoshgoftaar, and A. Napolitano, ‚ÄúExperimental Perspectives [70] S. Maldonado, J. LoÃÅpez, C. Vairetti, An alternative SMOTE oversampling strategy on Learning from Imbalanced Data.‚Äù for high-dimensional datasets, Appl. Soft Comput. 76 (Mar. 2019) 380‚Äì389, [45] M. Galar, A. Fernandez, E. Barrenechea, H. Bustince, F. Herrera, A Rev. https://doi.org/10.1016/J.ASOC.2018.12.024. Ensembles Cl. imbalance Probl.: Bagging-, Boost. -, Hybrid. -Based Approaches [71] M. Perez-Ortiz, P.A. Gutierrez, P. Tino, C. Hervas-Martinez, Oversampling the (2012), https://doi.org/10.1109/TSMCC.2011.2161285. minority class in the feature space, IEEE Trans. Neural Netw. Learn Syst. 27 (9) [46] B. Krawczyk, Learning from imbalanced data: open challenges and future (Sep. 2016) 1947‚Äì1961, https://doi.org/10.1109/TNNLS.2015.2461436. directions, Prog. Artif. Intell. 5 (4) (2016) 221‚Äì232, https://doi.org/10.1007/ [72] F.A. Ghaleb, F. Saeed, M. Al-Sarem, S.N. Qasem, T. Al-Hadhrami, Ensemble S13748-016-0094-0/TABLES/1. synthesized minority oversampling-based generative adversarial networks and [47] G.E.A.P.A. Batista, R.C. Prati, M.C. Monard, A study of the behavior of several random forest algorithm for credit card fraud detection, IEEE Access 11 (2023) methods for balancing machine learning training data, SIGKDD Explor. Newsl. 6 89694‚Äì89710, https://doi.org/10.1109/ACCESS.2023.3306621. (1) (2004) 20‚Äì29. [73] H. Tan, Tabular GAN-based oversampling of imbalanced time-to-event data for [48] S. Ndichu, T. Ban, T. Takahashi, D. Inoue Security-Alert Screening with survival prediction 2023 8th Int. Conf. Cloud Comput. Big Data Anal., ICCCBDA Oversampling Based on Conditional Generative Adversarial Networks Proc. - 2023; in 2023 8th International Conference on Cloud Computing and Big Data 2022 17th Asia Jt. Conf. Inf. Secur., AsiaJCIS 2022, in Proceedings - 2022 17th Analytics, ICCCBDA 2023, 2023, pp. 376 ‚Äì 380. doi: 10.1109/ Asia Joint Conference on Information Security, AsiaJCIS 2022, 2022, pp. 1 ‚Äì 7. ICCCBDA56900.2023.101548832023. doi: 10.1109/AsiaJCIS57030.2022.000112022. [74] J. Kim, H. Park, Reduced CNN model for face image detection with gan [49] W. Juanjuan, X. Mantao, W. Hui, Z. Jiwu, Classification of imbalanced data by oversampling, Lect. Notes Netw. Syst. 279 (2022) 232‚Äì241, https://doi.org/ using the SMOTE algorithm and locally linear embedding, in: International 10.1007/978-3-030-79728-7_23. Conference on Signal Processing Proceedings, 3, ICSP, 2007, https://doi.org/ [75] N. Abedzadeh, M. JacobsGANMCMCRO: A generative adversarial network 10.1109/ICOSP.2006.345752. markov chain Monte Carlo random oversampling algorithm for imbalance [50] Q. Kang, X.S. Chen, S.S. Li, M.C. Zhou, A noise-filtered under-sampling scheme for datasets; in International Conference on Web Information Systems and imbalanced classification, IEEE Trans. Cyber 47 (12) (2017) 4263‚Äì4274, https:// Technologies, WEBIST - Proceedings, 2023, pp. 587 ‚Äì 594. doi: 10.5220/ doi.org/10.1109/TCYB.2016.2606104. 00122596000035842023. [51] W.W.Y. Ng, J. Hu, D.S. Yeung, S. Yin, F. Roli, Diversified sensitivity-based [76] J.-H. Oh, J.Y. Hong, J.-G. Baek, Oversampling method using outlier detectable undersampling for imbalance classification problems, IEEE Trans. Cyber 45 (11) generative adversarial network, Expert Syst. Appl. 133 (2019) 1‚Äì8, https://doi. (2015) 2402‚Äì2412, https://doi.org/10.1109/TCYB.2014.2372060. org/10.1016/j.eswa.2019.05.006. [52] H. Han, W.-Y. Wang, and B.-H. Mao, ‚ÄúBorderline-SMOTE: A New Over-Sampling [77] S. Yang, Y. Zhou, X. Chen, C. Deng, C. Li, Fault diagnosis of wind turbines with Method in Imbalanced Data Sets Learning,‚Äù 2005. generative adversarial network-based oversampling method, Meas. Sci. Technol. [53] D.A. Cieslak, T.R. Hoens, N.V. Chawla, W.P. Kegelmeyer, Hellinger distance 34 (4) (2023), https://doi.org/10.1088/1361-6501/acad20. decision trees are robust and skew-insensitive, Data Min. Knowl. Discov. 24 (1) [78] A. Majeed, S.O. Hwang, CTGAN-MOS: conditional generative adversarial network (2011) 136‚Äì158, https://doi.org/10.1007/S10618-011-0222-1. based minority-class-augmented oversampling scheme for imbalanced problems, [54] H.M. Nguyen, E.W. Cooper, K. Kamei, Borderline over-sampling for imbalanced IEEE Access 11 (2023) 85878‚Äì85899, https://doi.org/10.1109/ data classification, Int J. Knowl. Eng. Soft Data Parad. 3 (1) (2011) 4, https://doi. ACCESS.2023.3303509. org/10.1504/IJKESDP.2011.039875. [79] K. Wang, C. Gou, Y. Duan, Y. Lin, X. Zheng, F.Y. Wang, Generative adversarial [55] H. He, Y. Bai, E.A. Garcia, S. Li, ADASYN: Adaptive synthetic sampling approach networks: Introduction and outlook, IEEE/CAA J. Autom. Sin. 4 (4) (2017) for imbalanced learning, Proc. Int. Jt. Conf. Neural Netw. (2008) 1322‚Äì1328, 588‚Äì598, https://doi.org/10.1109/JAS.2017.7510583. https://doi.org/10.1109/IJCNN.2008.4633969. [80] A. Anand, K. Gorde, J.R. Antony Moniz, N. Park, T. Chakraborty, B.-T. Chu [56] G. KovaÃÅcs, Smote-variants: a python implementation of 85 minority oversampling Phishing URL detection with oversampling based on text generative adversarial techniques, Neurocomputing 366 (2019) 352‚Äì354, https://doi.org/10.1016/J. networks Proc. - 2018 IEEE Int. Conf. Big Data, Big Data 2018; in Proceedings - NEUCOM.2019.06.100. 19\n\n\n\n---\n\nM. Hayaeian Shirvan et al. Applied Soft Computing 170 (2025) 112677\n2018 IEEE International Conference on Big Data, Big Data 2018, 2018, pp. [91] M. Arjovsky, S. Chintala, L. Bottou, Wasser. Gener. Advers. Netw. (2017). 1168‚Äì1177. doi: 10.1109/BigData.2018.86225472018. [92] X. Liu, T. Li, R. Zhang, D. Wu, Y. Liu, Z. Yang, A GAN and feature selection-based [81] M.S. Munia, M. Nourani, S. Houari, Biosignal Oversampling Using Wasserstein oversampling technique for intrusion detection, Secur. Commun. Netw. 2021 Generative Adversarial Network. 2020 IEEE International Conference on (2021), https://doi.org/10.1155/2021/9947059. Healthcare Informatics, ICHI 2020, Institute of Electrical and Electronics [93] X. Yao, K. Li, L. Yu, Imbalanced corporate bond default modeling using generative Engineers Inc., Nov. 2020, https://doi.org/10.1109/ICHI48887.2020.9374315. adversarial networks oversampling techniques, Xitong Gongcheng Lilun yu [82] J. Engelmann, S. Lessmann, Conditional Wasserstein GAN-based oversampling of Shijian/Syst. Eng. Theory Pract. 42 (10) (2022) 2617‚Äì2634, https://doi.org/ tabular data for imbalanced learning, Expert Syst. Appl. 174 (2021), https://doi. 10.12011/SETP2021-2328. org/10.1016/j.eswa.2021.114582. [94] E. Farahbakhsh, J. Maughan, R.D. M√ºller, Prospectivity modelling of critical [83] E. Nazari, P. Branco On oversampling via generative adversarial networks under mineral deposits using a generative adversarial network with oversampling and different data difficulty factors; in Proceedings of Machine Learning Research, positive-unlabelled bagging, Ore Geol. Rev. 162 (2023), https://doi.org/ 2021, pp. 76 ‚Äì 89. [Online]. Available: https://www.scopus.com/inward/record. 10.1016/j.oregeorev.2023.105665. uri?eid= 2-s2.0- [95] M. Zareapoor, P. Shamsolmoali, J. Yang, Oversampling adversarial network for 85120163757&partnerID= 40&md5= 5f4e969e9bc436314021043f34fb39fc class-imbalanced fault diagnosis, Mech. Syst. Signal Process 149 (Feb. 2021), 2021, 7689([Online]. Available). https://doi.org/10.1016/j.ymssp.2020.107175. [84] Y. Dong, H. Xiao, Y. Dong, SA-CGAN: An oversampling method based on single [96] S.S. Mullick, S. Datta, S. Das, Generative Adversarial Minority Oversampling, in: attribute guided conditional GAN for multi-class imbalanced learning, International Conference on Computer Vision, 2019-October, IEEE, 2019, Neurocomputing 472 (2022) 326‚Äì337, https://doi.org/10.1016/j. pp. 1695‚Äì1704, https://doi.org/10.1109/ICCV.2019.00178. neucom.2021.04.135. [97] Y. Zhan, D. Hu, Y. Wang, X. Yu, Semisupervised Hyperspectral Image [85] J. Hao, C. Wang, H. Zhang, G. YangAnnealing Genetic GAN for Minority Classification Based on Generative Adversarial Networks, IEEE Geosci. Remote Oversampling 31st Br. Mach. Vis. Conf., BMVC 2020; in 31st British Machine Sens. Lett. 15 (2) (2018) 212‚Äì216, https://doi.org/10.1109/ Vision Conference, BMVC 20202020([Online]. Available:). LGRS.2017.2780890. [86] G. Mariani, F. Scheidegger, R. Istrate, C. Bekas, and C. Malossi, ‚ÄúBAGAN: Data [98] J. Kim, H. Park OA-GAN: Overfitting avoidance method of GAN oversampling Augmentation with Balancing GAN,‚Äù Mar. 2018, [Online]. Available: „Äàhttp based on xAI Int. Conf. Ubiquitous Future Netw., ICUFN; in International ://arxiv.org/abs/1803.09655„Äâ. Conference on Ubiquitous and Future Networks, ICUFN, 2021, pp. 394 ‚Äì 398. doi: [87] B. Abbey, et al., Keyhole coherent diffractive imaging, Nat. Phys. 4 (5) (2008) 10.1109/ICUFN49451.2021.95285942021. 394‚Äì398, https://doi.org/10.1038/nphys896. [99] J. Wang, L. Yao Unrolled GAN-based oversampling of credit card dataset for fraud [88] J. Zhao, Y. Kim, K. Zhang, A.M. Rush, Y. LeCun, Adversarially Regularized detection 2022 IEEE, in 2022 IEEE International Conference on Artificial Autoencoders, 35th Int. Conf. Mach. Learn., ICML 2018 13 (2017) 9405‚Äì9420. Intelligence and Computer Applications, ICAICA 2022, 2022, pp. 858 ‚Äì 861. doi: Accessed: Dec. 26, 2023. [Online]. Available: „Äàhttps://arxiv.org/abs/1706.04223 10.1109/ICAICA54878.2022.98444212022. v3„Äâ. [100] J. Tao, G. Wei, Y. Song, J. Dou, W. Mu, Oversampling algorithm based on gradient [89] I.O. Tolstikhin, O. Bousquet, S. Gelly, B. SchoÃàlkopf, ‚ÄúWasserstein Auto- penalty generative adversarial network, Shanghai Ligong Daxue Xuebao/J. Univ. Encoders,‚Äù, Int. Conf. Learn. Represent. (2017). Shanghai Sci. Technol. 45 (3) (2023) 235‚Äì243, https://doi.org/10.13255/J.CNKI. [90] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, B. Frey, Adversarial JUSST.20220307005. Autoencoders, Elem. Dimens. Reduct. Manifold Learn. (2015) 577‚Äì596, https:// doi.org/10.1007/978-3-031-10602-6_21. 20","knowledge_id":"844a4118-044d-4940-82aa-fb24b6e5156a","title":"1-s2_0-S1568494624014510-main.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/419716c3-a46c-4d06-83ca-09eed03ff5ab/1-s2_0-S1568494624014510-main.pdf"}
{"file_content":"RESEARCH ARTICLE |  JULY 02 2025 Credit card fraud detection using machine learning algorithms: A review Ó§á Manali KulkarniÓ§£ ; Grantej Otari; Sameer Patil; Tanaji Patil; Mahesh Salunkhe AIP Conf. Proc. 3257, 020030 (2025) https://doi.org/10.1063/5.0265245 Ó§© Ó§≠ View Export Online Citation Articles You May Be Interested In Credit card transaction data analysis and performance evaluation of machine learning algorithms for credit card fraud detection AIP Conf. Proc. (October 2022) Exploring machine learning techniques for enhancing fraud detection AIP Conf. Proc. (November 2024) A study on machine learning approaches to detect credit card fraud AIP Conf. Proc. (July 2021) 17 November 2025 10:16:22\n\n\n\n---\n\nCredit Card Fraud Detection Using Machine Learning\n\n### Algorithms: A Review\n\n\nManali Kulkarnia), Grantej Otarib), Sameer Patilc), Tanaji Patild), Mahesh Salunkhee) K.I.T.s College of Engineering, Kolhapur, India. a)corresponding author:manalikulkarni7440@gmail.com b)otari.grantej@kitcoek.in c)patil.sameer@kitcoek.in d)patil.tanaji@kitcoek.in\n\n### e)salunkhe.mahesh@kitcoek.in\n\n\nAbstract. Credit card fraud constitutes a serve issue that might result in huge losses for both people and businesses. It causes billions of dollars in damage each year. The increase in online transactions has led to a rise in credit card fraud cases. Thieves can obtain your credit card details and use them for unauthorized purchases or to open new accounts under your name. This issue, known as credit card fraud, can be quite serious. If you suspect any fraudulent activity on your card, you should contact your bank right away. Financial institutions are constantly developing new techniques to identify and prevent credit card fraud. Despite ongoing advancements in fraud detection technology, staying ahead of constantly evolving criminal tactics remains a significant challenge. Because fraud continually evolves, traditional detection methods, like rule-based systems, often prove ineffective. An effective solution is needed to reduce the harm caused by fraudulent transactions at the outset. These can be solved by using machine learning models, such as OLightGBM, to enable more precise and effective fraud detection. The combination of Light Gradient Boosting Machine (LightGBM) and Bayesian optimization has demonstrated significant potential in detecting credit card fraud. It outperforms other machine learning algorithms in terms of accuracy, Area Under the Curve (AUC), precision, and F1 score.\n\n## INTRODUCTION\n\n\nThe transition to digital payment can have both positive and negative effects on credit card fraud. For digital payment, users have to provide personal Information and card details, making them potential targets for identity theft and the account takeover. Fraudsters may employ various techniques, such as a phishing, hacking, or social engineering, to gain unauthorized access to individuals‚Äô accounts or steal their identities.The different ways criminals commit credit card fraud can be categorized into distinct types. We will take a look at these various categories [1,4].\n\n‚óè Simple theft (offline fraud): The most basic type of credit card fraud involves stealing a physical card (offline fraud). These thefts are often caught quickly. ‚óè Application Fraud: Involves criminals using fake identities to apply for new credit cards. ‚óè Bankruptcy Fraud: Cardholders who know they cannot afford to repay their debts might commit credit card fraud by exceeding their limits or making purchases with no intention to pay. Credit scoring can help mitigate this risk. ‚óè Internal Fraud: happens when employees with access to card information misuse it for unauthorized transactions. ‚óè Cardholder-not-present Fraud/ behavioural Fraud: Online and mobile transactions lack the security of a physical card present. Fraudsters can steal genuine card details through skimming or shoulder surfing. Advanced fraud detection methods are crucial to identify unusual spending patterns in these situations. It is essential for users to stay vigilant to protect from credit card fraud. Using strong passwords, regularly monitoring accounts, and promptly reporting suspicious activity significantly reduces the risk. Fraud detection can pose several barriers when it comes to implementing machine learning techniques [2] because of imbalanced data, lack of labeled data, feature engineering, real time decision making. Detecting credit card fraud with machine learning can involve using both classification and regression methods, which are widely acknowledged as effective methods for fraud detection. Machine learning algorithms used for fraud detection can be broadly categorized into two types: supervised and unsupervised. unsupervised learning algorithm employs peer group analysis and does not require labeled data. Instead focuses on detecting unusual patterns or anomalies in the data. On other International Conference on Cognitive Computing and Artificial Intelligence (ICCCAI - 2024) AIP Conf. Proc. 3257, 020030-1‚Äì020030-9; https://doi.org/10.1063/5.0265245 Published under an exclusive license by AIP Publishing. 978-0-7354-5212-1/$30.00 020030-1 17 November 2025 10:16:22\n\n\n\n---\n\nhand, Supervised learning algorithms leverage labeled transactions to train the model for identifying fraudulent activity. Numerous machine learning algorithms have been proposed [3,4] to identify and prevent credit card fraud.These includes Logistic Regression, Decision Trees, Random Forest, Na√Øve Bayes, Support Vector Machines (Both Linear and non-linear variants), and K Nearest Neighbors. The effectiveness of these algorithms in categorizing transactions as legitimate or\n\n### fraudulent is assessed in this study in table 1.\n\n\n### TABLE 1.Comparison of different algorithms.\n\n\nAlgorithm Strength Limitation DT Offers simplicity in Prone to understanding and overfitting and interpretation, requiring unable to predict minimal data pre- numeric labels. processing, while providing clear insights into relevant features hold greater relevance for the classification model. SVM Shows resistance to Can be noisy features, operates computationally effectively in spaces expensive to train with high-dimensions, and lacks and retains memory Interpretability. efficiency. RF Exhibit high accuracy Sensitive to and robustness in outliers.\n\n### handling large datasets.\n\n\n## LITERATURE SURVEY\n\n\nFor the purpose of identifying and evaluating theaccuracy of fraudulent transactions, the Authoremployed the Random Forest algorithm (RFA)[5].RFA uses decision trees to classify datasets and is asupervised learning technique. The classificationresults are utilized to create a confusion matrix,serving as a means to assess the performance ofRandom Forest Algorithm. After processing dataset,an accuracy rate of around 90% was achieved. Y.Sahin and E. Duman introduced a hybrid model combining Artificial Neural Networks (ANNs) and Logistic Regression (LR) for classification.[6]. By utilizing real- world dataset, a comparison between ANN and LR methods can be conducted. The classifier generates score of each transaction and classifies them as suspicious or genuine. The study aims to advance the development of effective fraud detection methods by evaluating the performance of these models. The authors presented two distinct algorithms for fraud detection: one based on decision trees and another on random forest [7]. In first strategy, a tree is built using user activities, enabling the identification of potential fraud cases. The seco nd strategy is creating a forest based on user behaviors in order to detect suspicious behavior. Through experimentation, the results reveal that these techniques achieve high levels of precision in detecting fraudulent instances in credit card transactions.\n\nY. sahin et al. [8] introduced an innovative method known as the cost-sensitive decision tree utilized in identification of\ncredit card fraud. By choosing the best splitting characteristic for each non-terminal node, this strategy aims to reduce the overall misclassification costs. Using a real-world credit card dataset, the technique To detect fraudulent activities related to credit cards, the author [9] suggests a brand-new method known as the Naive Bayes enhanced k closest neighbor method (NBKNN). The experimental findings show how effective this method is at correctly categorizing transactions as fraudulent or not. Kaur et al. [10] replaces the naive Bayes classification method with SVM and decision trees enhance the accuracy of detecting fraudulent transactions. The Na√Øve Bayes classification algorithm, a probabilistic model computes the probability of various target classes to predict the likely outcome of test data. In fraud detection, the Naive Bayes classification algorithm achieved significant improvements in accuracy, precision, recall, and F-measure compared to a voting-based approach. These metrics increased by 10-15%, indicating a more effective fraud detection model. 020030-2 17 November 2025 10:16:22\n\n\n\n---\n\nThe K-nearest neighbors (KNN) algorithm [11] has shown promise in fraud detection and outlier identification. Studies suggest it can effectively reduce false alarms while maintaining a high fraud detection rate. The authors [12] compare various ML and DL algorithms on a dataset (potentially addressing data imbalance with techniques like undersampling). The model leverages Convolutional Neural Networks (CNNs), Artificial Neural Networks (ANNs), and Support Vector Machines (SVMs) for fraud prediction, evaluated the model's performance using standard\n\n### metrics like accuracy, precision, F1 score, and recall.\n\n\nSupport Vector Machines (SVMs) [13] are a powerful machine learning technique well-suited for real-time fraud detection. This supervised approach is utilized to train and evaluate the system, ultimately delivering a robust and efficient solution for detecting fraud. The study concludes by highlighting SVM as the optimal classifier for this purpose, as it demonstrates superior performance in identifying and flagging fraudulent transactions. By analyzing past transaction details and leveraging behavioral patterns, this method can effectively identify and prevent fraudulent activities, helping banks and financial institutions mitigate the risk associated with credit card fraud.\n\nSahin et al‚Äôs work [14] focuses on machine learning approaches like Support Vector Machines (SVMs) and decision trees offer promising avenues for building robust classification models to combat credit card fraud. The study distinguishes itself as among the initial research endeavours to evaluate the effectiveness of Support Vector Machines (SVMs) and decision trees in detecting credit card fraud using real-world transaction data. The results offer valuable insights for selecting appropriate models enhancing the precision and efficiency of the fraud detection system.\n\nAuthor proposes the Intelligent Learning Scheme for Digital Fraud Detection (ILSDFD), a deep learning-based system using autoencoders for feature selection and classification[15]. It claims superior performance compared to traditional Artificial Neural Networks (ANN) in terms of accuracy, precision, and recall.\n\nN. S. S. Pranavi et al. [16] proposes a novel approach to enhance fraud detection. It suggests combining Random Forest\nwith SMOTE, an oversampling technique, to achieve the ambitious goal of 100% fraud detection while minimizing false negatives. This method adds to the continuous exploration of ways to proactively identify and prevent fraudulent\n\n### activity[24],[25],[26].\n\n\nVarious ML algorithms such as XGBoost, Support Vector Machine, Decision Tree, Random Forest, and Logistic Regression are explored as fundamental tools. Additionally, a Convolutional Neural Network (CNN) is employed to augment fraud detection efficiency, leveraging its layers for accurate detection.Empirical analysis involves experimentation with recent CNN models, adjusting parameters such as hidden layer counts and epochs. Performance metrics including F1 score, accuracy, precision, recall, and Area Under Curve (AUC) are utilized, with improvements noted in achieving high AUC values, enhancing detection accuracy. The author[17] offers a comprehensive approach to remotely detecting fraudulent activities related to credit card fraud, Combining deep learning and machine learning techniques offers a powerful approach to fight fraud in online transactions[27],[28],[29]\n\nThe Author‚Äôs[18]objective is to compare the efficacy of three prominent ML algorithms such as Logistic Regression, Decision tree classifier, and Random Forest in identifying credit card fraud. Additionally, a novel hybrid model that amalgamates elements of these algorithms to enhance fraud detection capabilities. The model represents a departure from previous studies by incorporating a hybrid approach that combines the strengths of multiple algorithms. Through empirical evaluation, which demonstrates that the model surpasses existing methodologies in accurately identifying fraudulent transactions? By leveraging the latest advancements in machine learning and hybrid modelling techniques, the study contributes to the ongoing efforts to combat credit card fraud effectively in the digital era[30].\n\nB. Gedela et al. [19] proposed AdaBoost algorithm demonstrates that AdaBoost achieves superior credit card fraud\ndetection performance compared to traditional machine learning methods like Naive Bayes classifier, logistic regression, ANNs, and decision tree classifier. These findings advance the fields of fraud detection and offer valuable insights for financial institutions seeking reliable solutions to mitigate credit card fraud risks. 020030-3 17 November 2025 10:16:22\n\n\n\n---\n\n## THE OLIGHTGBM ALGORITHM\n\n\nEvery year, enormous financial losses are incurreddue to illegal credit card transactions. Fraud canmanifest in various forms and may range in its extent.Therefore, it is essential to confront the challengesrelated to credit card fraud detection. Furthermore, asnew technologies continue to evolve, criminalsconsistently devise innovative methods to perpetratefraud. Thus, it is crucial to stay ahead of theseevolving threats by actively working on improvingfraud detection mechanisms. In addition to being used in detecting credit card fraud, the OLightGBM approach is also compared against alternative machine\n\n### learning techniques.\n\n\nOLightGBM ‚óè To develop a more effective approach thatcan accurately identify fraudulent activities, thereby minimizing thefinancial losses. ‚óè To show that careful parameter tuning andoptimization contribute to the improved accuracy and overall effectiveness of thealgorithm. ‚óè Highlighting the OLightGBM techniques superior performance over various alternative Machine Learning algorithms, such as Naive Bayes classifier, Random Forest, decision tree classifier, k-Nearest Neighbor, Logistic Regression, and Support Vector Machines (both radial and linear),. ‚óè To ensuring that the experiment faithfully mirrors the algorithm's performance in real-world situations. ‚óè To demonstrate the superiority of the LightGBM algorithm in the areas of Precision, AUC, Accuracy, and F1-Score.\n\nModules FIGURE 1. illustrates the framework of an intelligent approach for detection of credit card fraud. It has following modules: ‚óè Data Set and Data Preprocessing. ‚óè Feature Selection. ‚óè The Optimized Light Gradient Boost ‚óè Classifier. ‚óè Model Evaluation Using Performance Metrics.\n\n### ‚óè Experimental Results.\n\n\nTo efficiently decrease the dimensionality of training data, LightGBM uses the information gain (IG)approach as a feature selection methodology point the most crucial characteristics. The information gain method helps identify the most informative features for distinguishing legitimate transactions from fraudulent ones. It assigns higher weights to features that contribute most to this classification. The OLightGBM algorithm utilizes information gain as a feature selection method because its computationally efficient and performance well, particularly in terms of accuracy and precision. LightGBM algorithm[20]is equipped with various hyperparameters thatgreatly influence its performance. Traditionally,these hyperparameters are manually set and thenfine-tuned through and iterative trial and errorprocess. The algorithm for Bayesian-basedhyperparameter optimization is applied tointelligently adjust the hyperparameters ofLightGBM algorithm. The LightGBM algorithm bundle unique featurestogether. By creating feature bundles, the algorithmenables the feature-scanning algorithm to generatehistograms based on these bundled features [20].The computation complexity is evaluated usingtheoretical time complexity. In first strategy, a tree is built using user activities, enabling the identification of potential fraud cases. The second strategy is creating a forest based on user behaviors in order to detect suspicious behavior. Through experimentation, the results reveal that these techniques achieve high levels of precision in detecting fraudulent instances in credit card transactions. 020030-4 17 November 2025 10:16:22\n\n\n\n---\n\n\n\n\n\nFIGURE 1.Framework of OLightGBM Approach\n\n### Data Set and Data Preprocessing\n\n\nThe real world datasets were used to performdifferent experiments. The dataset used is acompilation of European cardholder transactions,encompassing a total of 284,807 transactions duringSeptember 2013. Within this dataset, Within this dataset, 492 transactions were identified as potentially fraudulent, constituting 0.172% of the overall transaction volume [20]. Comprising 31 features, Initial 28 features (V1 to V28) denote principal components derived from a principal component analysis (PCA) procedure. The implementation of PCA aims to ensure privacy of data. It is noteworthy that two features, namely ‚ÄúTime‚Äù and ‚ÄúAmount‚Äù have not undergone transformation through PCA. Due to the notable difference in the quantity offraudulent and legitimate transactions, the datasetsexhibit an imbalanced data distribution. This imbalance presents a recognized challenge, as theeffectiveness of various machine learning methodstends to diminish when dealing with imbalanceddatasets. To enhance result accuracy, a cross-validation procedure is employed for training andtesting the model on each subset of the datasets. Theaverage of all measures that have been recorded isthen calculated for the full dataset. Furthermore,alternative techniques like resampling have beenproposed to\n\n### address the challenge posed byimbalanced datasets.\n\n\n### Feature Selection\n\n\nTo efficiently decrease the dimensionality of training data, LightGBM uses the information gain (IG)approach as a feature selection methodology point the most crucial characteristics. The information gain method helps identify the most informative features for distinguishing legitimate transactions from fraudulent ones. It assigns higher weights to features that contribute most to this classification. The OLightGBM algorithm utilizes information gain as a feature selection method because its computationally efficient and performance well, particularly in terms of accuracy and precision [20].\n\n### The Optimized Light Gradient Boost Classifier\n\n\nALGORITHM 1. shows the LightGBM algorithm[20]is equipped with various hyperparameters thatgreatly influence its performance. Traditionally,these hyperparameters are manually set and thenfine-tuned through and iterative trial and errorprocess. The algorithm for Bayesian-basedhyperparameter optimization is applied tointelligently adjust the hyperparameters ofLightGBM algorithm. The LightGBM algorithm bundle unique featurestogether. By creating feature bundles, the algorithmenables the feature-scanning algorithm to generatehistograms based on these bundled features [20].The computation complexity is evaluated usingtheoretical time complexity. It may be expressed as: computation complexity = (  * ) where ‚Äúm‚Äù is number of bundles, which is usually smaller than the total number of features, and ‚Äún‚Äù denotes number of features in dataset. Tomaintain the accuracy of information gainestimation, LightGBM utilizes the Gradient BasedOne Side Sampling (GOSS) technique. GOSSretains examples with substantial gradients (e.g., 020030-5 17 November 2025 10:16:22\n\n\n\n---\n\n\nthose falling inside the top percentiles or reaching apredetermined threshold) while selecting sampling cases with modest\n\n### gradients. [20].\n\n\n### Model Evaluation Using Performance Matrix\n\n\nCross-validation testing is done to assess how well credit card fraud is detected. The k-fold cross validation (CV) approach was employed to assess the OLightGBM algorithm's efficiency. One popular statistical analysis method is K-fold CV. A five-fold cross-validation (CV) approach is used to assess the performance. The given dataset shows class imbalance, with more legitimate transactions than fraudulent ones. In order to get more precise estimations, cross-validation is utilizing for training and testing the model on individual subsets of the datasets. [21]. The performance metrics are then computed and averaged over the entire data set to provide an overall assessment of the OLightGBM algorithm. To ensure robust evaluation on imbalanced datasets, a 5-fold cross-validation (CV) technique is used. The data was randomly divided into five equally sized folds. During each CV iteration, one fold served as the validation set, while the remaining four folds were used for training. Five times this process was repeated, checking each fold was used for validation once.\n\nAn overall assessment of the effectiveness of the OLightGBM algorithm is provided by evaluating its performance through averaging the results from five test subsets. AUC, F1-Score, Precision, Recall, Accuracy (ACC), Confusion Matrix, and other performance indicators are taken into account. The Confusion Matrix (CM) employs four classification performance metrics: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)[22]. Derived from these matrix, Accuracy, Precision, Recall, and F1-Score are defined. Precision and recall are crucial metrics as they indicate the accuracy and relevance of the findings, particularly in context of imbalanced data [20,23]. The F1-Score merges Precision and Recall, offering a well-balanced metric. The Precision-Recall curve offers insights into the classifier accuracy, especially in imbalanced datasets. Additionally, The AUC value, which signifies the area beneath the Receiver Operating Characteristic (ROC) curve, provides a well-rounded assessment of the model's effectiveness in various scenarios.Regardless of cutoff value, a higher AUC score denotes superior overall performance and is therefore useful for evaluating classifiers. Experimental Results Machine Learning Techniques are compared with the OLightGBM algorithm and then the performance is calculated. 4.The AUC is used to measure how well the predictions are ranked. Also measure the accuracy of the detecting fraud. As score of AUC increases then we can observe the better prediction performance. So, the OLightGBM algorithm achieves\n\n### the highest shows in fig2, fig3 and fig4.\n\n\n\n\n\n\nFIGURE 2.Accuracy 020030-6 17 November 2025 10:16:22\n\n\n\n---\n\n\n\n### FIGURE 3.Recall\n\n\n\n\n### FIGURE 4.Precision\n\n\n\n## CONCLUSION\n\nNow a days,credit card fraud detection is really significant topic because due to increase in e-commerce,and digital payment system many organizations and financial firms facing the losses.Significant and ongoing financial losses. There are many models to find out credit card fraud but OLightGBM algorithm performed well to find outcreditcardfraud.A comprehensive comparison was conducted, pitting OLightGBM against other machine learning algorithms commonly employed for fraud detection, including naive bayes classifier, support vector machine (both radial and linear), k-nearest neighbour, and decision tree classifier among others. The conclusive performance assessment, showcases the superiority of the 020030-7 17 November 2025 10:16:22\n\n\n\n---\n\nOLightGBM algorithm, exceeding all alternative machine learning algorithms in relation to F-1 Score, Accuracy, AUC, and Precision. This signifies its exceptional capability inaccurately identifying fraudulent transactions. This study emphasizes the critical role of a parameter optimization is used to get best prediction performance of OLightGBM algorithm. By attaining these promising results, the OLightGBM algorithm demonstrates its effectiveness as the most\n\n### proficient technique for fraud detection.\n\n\n\n## REFERENCES\n\n\n1. S. Makki, Z. Assaghir, Y. Taher, R. Haque, M. -S. Hacid and H. Zeineddine, \"An Experimental Study With\nImbalanced Classification Approaches for Credit Card Fraud Detection,\" in IEEE Access, vol. 7, pp. 93010-93022, 2019\n\n2. P.A.Dal,G.Borachi,O.Caelen,C.AlippiandG.Bontempi,‚ÄúCreditcardfrauddetection:Arealistic modeling and a novel\nlearning strategy‚Äù,IEEE Trans. Neural Netw. Learn. Syst., vol. 29,no.8,pp.3784-3797,Sep.2017.\n\n3. Suresh K Shirgave and Chetan J. AwatiRashmiMore ,Sonam S. Patil. ‚ÄúA Review On CreditCard Fraud Detection\nUsing Machine Learning.‚ÄùInternational Journal of Scientific & TechnologyResearch volume8, issue10, October2019.\n\n4. A.Mniai, M. Tarik and K. Jebari, \"A Novel Framework for Credit Card Fraud Detection,\" in IEEE Access, vol.\n11, pp. 112776-112786, 2023.\n\n5. 5.M. S. Kumar, V. Soundarya, S. Kavitha, E. S. Keerthika and E.Aswini, \"Credit Card Fraud Detection Using\nRandom Forest Algorithm, \"2019 3rd International Conference on ComputingandCommunicationsTechnologies(ICCCT),Chennai,India,pp.149-153,2019.\n\n6. Y. Sahin and E. Duman, \"Detecting credit card fraud by ANN and logistic regression,\n\"2011InternationalSymposiumonInnovationsinIntelligentSystemsandApplications,Istanbul,Turkey, pp.315- 319,2011.\n\n7. M. R. Dileep, A. V. Navaneeth and M. Abhishek, \"A Novel Approach for Credit Card Fraud Detection using\nDecisionTree and Random Forest Algorithms,\" 2021 Third International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV), Tirunelveli, India,pp.1025-1028,2021.\n\n8. Y. Sahin, S. Bulkan, and E. Duman, ‚ÄúA cost-sensitive decision tree approach for fraud detection, ‚ÄùExpert\nSyst.Appl.,vol.40,no.15,pp.5916‚Äì5923,2013.\n\n9. Sai Kiran, Jypti Guru, Rishabh Kumar, Naveen Kumar, Deepak Katariya, ‚ÄùCredit card fraud detection using Na√Øve\nBayes model based and KNN classifier‚Äù, Int.Journal of Adv. Research Ideas and Innovations in Technology, vol.4, 2018.\n\n10. Kaur, Bhagwant& Kumar, Rakesh. A Hybrid Approach for Credit Card Fraud Detection Using Na√Øve Bayes and\nVotingClassifier,2020.\n\n11. N. Malini and M. Pushpa, \"Analysis on credit card fraud identification techniques based\nonKNNandoutlierdetection,\"2017ThirdInternationalConferenceonAdvancesinElectrical,Electronic s,Information,CommunicationandBio-Informatics(AEEICB), Chennai, India, pp. 255-258,2017.\n\n12. S. M. Gopavaram and P. Vinothiyalakshmi, \"Cloud Based Credit Card Fraud Detection System in Banking Using\nMachine Learning and Deep Learning algorithms,\" 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT), Delhi, India, 2023, pp. 1-4.\n\n13. Prof.BhartiKudale,SwapnilBirajdar,AbhishekHattekar, Sameer Kulkarni, Sunil Gaikwad. \"CREDIT CARD\nFRAUD DETECTION USING MACHINE LEARNING. \"International Research Journal of Modernization in Engineering Technologyand Science04,no.01 (January2022) :4.\n\n14. Sahin,Yusuf&Duman,Ekrem.‚ÄùDetecting Credit Card Fraud by Decision Treesand Support Vector Machines‚Äù.\nIMECS 2011 -InternationalMultiConferenceofEngineersand Computer Scientists2011.1.442-447,2011.\n\n15. A.P, S. Bharath, N. Rajendran, S. D. Devi and S. Saravanakumar, \"Experimental Evaluation of Smart Credit Card\nFraud Detection System using Intelligent Learning Scheme,\" 2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES), Chennai, India, 2023, pp. 1-6.\n\n16. N. S. S. Pranavi, T. K. S. S. Sruthi, B. J. Naga Sirisha, M. S. Nayak and V. S. GuptavThadikemalla, \"Credit Card\nFraud Detection Using Minority Oversampling and Random Forest Technique,\" 2022 3rd International Conference for Emerging Technology (INCET), Belgaum, India, 2022, pp. 1-6.\n\n17. P. Y. Prasad, A. S. Chowdary, C. Bavitha, E. Mounisha and C. Reethika, \"A Comparison Study of Fraud Detection\nin Usage of Credit Cards using Machine Learning,\" 2023 7th International Conference on Trends in Electronics and Informatics (ICOEI), Tirunelveli, India, 2023, pp. 1204-1209.\n\n18. Q. S. Mirhashemi, N. Nasiri and M. R. Keyvanpour, \"Evaluation of Supervised Machine Learning Algorithms for\n020030-8 17 November 2025 10:16:22\n\n\n\n---\n\nCredit Card Fraud Detection: A Comparison,\" 2023 9th International Conference on Web Research (ICWR), Tehran, Iran, Islamic Republic of, 2023,pp. 247-252.\n\n19. B. Gedela and P. R. Karthikeyan, \"Credit Card Fraud Detection using AdaBoost Algorithm in Comparison with\nVarious Machine Learning Algorithms to Measure Accuracy, Sensitivity, Specificity, Precision and F-score,\" 2022 International Conference on Business Analytics for Technology and Security (ICBATS), Dubai, United Arab Emirates, 2022,pp. 1-6.\n\n20. A.A.Taha and S. J. Malebary, \"An IntelligentApproach to Credit Card Fraud Detection Usingan Optimized Light\nGradient Boosting Machine,\"in IEEE Access, vol. 8, pp. 25579-25587, 2020\n\n21. CreditCardFraudDataset,Sep.2019,[online]Available:https://www.kaggle.com/datasets/mlg-ulb/cre  ditcardfraud.\n22. G.Ke,Q.Meng,T.Finley,T.Wang,W.Chen,W. Ma, et al., \"LightGBM: A highly efficient gradient boosting\ndecision tree\", Proc.Adv.NeuralInf.Process.Syst.,pp.3146-3154,2017.Minu, R. I., S. Vishnuvardhan, Ankit Pasayat, and G. Nagarajan. \"Image colorization using CNNs.\" In Biologically Inspired Techniques in Many Criteria Decision Making: Proceedings of BITMDM 2021, pp. 603-612. Singapore: Springer Nature Singapore, 2022.\n\n23. A.C.Bahnsen, D.Aouada,A.Stojanovic and B.Ottersten, \"Feature engineering strategies for credit card fraud\ndetection\", ExpertSyst.Appl.,vol.51,pp.134-142, Jun.2016.\n\n24. Minu, R. I., and G. Nagarajan. \"A Statistical Non-Parametric data analysis for COVID-19 incidence data.\" ISA\ntransactions 130 (2022): 675-683.\n\n25. Srinivasulu, Singaraju, and G. Nagarajan. \"A Brief Survey on Efficient Models for Qubits Encoding Using\nQuantum Machine Learning With Multi Level Time Series Data Classification.\" 2023 Innovations in Power and Advanced Computing Technologies (i-PACT) (2023): 1-8.\n\n26. A Sivasangari, Kishore Sonti, Grace Prince Kanmani, Sindhu, D. Deepa, EEG-based computer-aided diagnosis of\nautism spectrum disorder, Cognitive Systems and Signal Processing in Image Processing, Academic Press, 2022, Pages 277-292, ISBN 9780128244104.\n\n27. A Sivasangari, P Ajitha, Immanuel Rajkumar, S Poonguzhali, Emotion recognition system for autism disordered\npeople, Journal of Ambient Intelligence and Humanized Computing,2019\n\n28. P. Ajitha, A. Sivasangari, R. Immanuel Rajkumar and S. Poonguzhali,‚Äù Design of text sentiment analysis tool\nusing feature extraction based on fusing machine learning algorithms‚Äù, Journal of Intelligent & Fuzzy Systems 40 (2021) 6375‚Äì6383.\n\n29. Ajitha, P., Gunasekaran, G.,‚ÄùSemantic based fuzzy inference system (SBFIS) prediction of patient emotion and\nprescription using support vector machine‚Äù, Journal of Medical Imaging and Health Informatics 6(3), pp. 769- 773,2016. 020030-9 17 November 2025 10:16:22","knowledge_id":"777dc687-daec-4ed6-8dd6-8e7e575eca0a","title":"PDF_Document_1763647907159.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/48a6ce6a-86e8-48fb-9d9f-cdc76872c189/020030_1_5_0265245.pdf"}
{"file_content":"Applied Artificial Intelligence An International Journal ISSN: 0883-9514 (Print) 1087-6545 (Online) Journal homepage: www.tandfonline.com/journals/uaai20 Credit Card Fraud Detection with Automated Machine Learning Systems Vasilios Plakandaras, Periklis Gogas, Theophilos Papadimitriou & Ioannis Tsamardinos To cite this article: Vasilios Plakandaras, Periklis Gogas, Theophilos Papadimitriou & Ioannis Tsamardinos (2022) Credit Card Fraud Detection with Automated Machine Learning Systems, Applied Artificial Intelligence, 36:1, 2086354, DOI: 10.1080/08839514.2022.2086354 To link to this article:  https://doi.org/10.1080/08839514.2022.2086354 ¬© 2022 The Author(s). Published with license by Taylor & Francis Group, LLC. Published online: 13 Jun 2022. Submit your article to this journal Article views: 8243 View related articles View Crossmark data Citing articles: 18 View citing articles Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=uaai20\n\n\n\n---\n\n### APPLIED ARTIFICIAL INTELLIGENCE\n\n2022, VOL. 36, NO. 1, e2086354 (1599 pages) https://doi.org/10.1080/08839514.2022.2086354 Credit Card Fraud Detection with Automated Machine Learning Systems Vasilios Plakandaras a, Periklis Gogas a, Theophilos Papadimitriou a, and Ioannis Tsamardinos b aDepartment of Economics, Democritus University of Thrace, Komotini, Greece; bDepartment of Computer Science, University of Crete and Gnosis Data Analysis, Greece\n\n## ABSTRACT ARTICLE HISTORY\n\nThe steady increase at the turnover of online trading during the Received 20 January 2021 last decade and the increasing use of credit cards has subse- Accepted 6 August 2021 quently made credit card frauds more prevalent. Machine Learning (ML) models are among the most prominent techni- ques in detecting illicit transactions. In this paper, we apply the Just-Add-Data (JAD), a system that automates the selection of Machine Learning algorithms, the tuning of their hyper- parameter values, and the estimation of performance in detect- ing fraudulent transactions using a highly unbalanced dataset, swiftly providing prediction model for credit card fraud detec- tion. The training of the model does not require the user setting up any of the methods‚Äô (hyper)parameters. In addition, it is trivial to retrain the model with the arrival of new data, to visualize, interpret, and share the results at all management levels within a credit card organization, as well as to apply the model. The model selected by JAD identifies 32 out of a total of 39 fraudulent transactions of the test sample, with all missed fraudulent transactions being small transactions below 50‚Ç¨. The comparison with other methods on the same dataset reveals that all the above come with a high forecasting performance that matches the existing literature. Introduction The creation of international agreements that promote transactions with credit cards such as the Single Euro Payments Area have significantly eased the use of card payments by consumers and businesses. The total value of card transactions using cards issued in the SEPA area amounted to ‚Ç¨4.38 trillion in 2016 (ECB, 2018) and is expected to double by 2025. Nevertheless, along with credit card transactions, we have seen a significant rise in credit card fraud. In 2016, credit card fraud in the SEPA area amounted to ‚Ç¨1.8 billion (European Central Bank 2018), while the worldwide incidents rose from $7.6 billion in 2010 to $21.81 billion in 2015 and are expected to reach $31.67 billion in 2020 (Robertson 2016). Despite the increasing effort to alleviate such fraudulent CONTACT Vasilios Plakandaras vplakand@econ.duth.gr Department of Economics, Democritus University of Thrace, Greece ¬© 2022 The Author(s). Published with license by Taylor & Francis Group, LLC. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/ licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\n\n\n\n---\n\nAPPLIED ARTIFICIAL INTELLIGENCE e2086354-1585 transactions and the substantial resources allocated by credit card issuers toward this end, the rising cost of fraudulent transactions suggests that there is much room for improvement in this research area. The detection of a fraudulent transaction is an ambitious task. First, frau- dulent cases are rare (in our dataset only 1 every 5000 records), rendering the outcome distribution severely skewed. The distribution of fraud cases seems to have seasonality effects and structural breaks as the attack strategies evolve over time (Dorronsoro et al. 1997). Another important issue is the accurate definition of the cost function, given that the cost of a false positive differs from the cost of a false negative (Dal Pozzolo et al. 2014). When the system characterizes a genuine transaction as fraudulent and freezes erroneously that transaction (false positive), the financial institution has an administrative cost to pay, as well as a decrease in customer satisfaction. In the case of frequent false-positive alarms, the financial institution faces the risk of losing customers and gathering adverse publicity. Conversely, when the system fails to detect a fraudulent transaction (false negative), the amount of that transaction is a loss for the financial institution or the merchant. Thus, it is very hard to define the asymmetric loss of each occurrence. Another significant difficulty in fraud detection is that electronic defrauders perform mainly legitimate transactions and occasionally fraudulent ones, rendering the profiling of them into universal standard patterns difficult. Each transaction must be examined separately, rendering the reaction overdue, especially on the non-working hours of electronic transactions. For an actual system to be useful, response to a fraudulent transaction should be almost contemporaneous, which is difficult given that most systems end up forward- ing automatic flagged transactions to (human) fraud examiners for manual inspection. Finally, security and privacy laws limit the public availability of data and/or censor the performed analyses, making them difficult to assess. Credit card fraud can be broadly separated into two categories: identity fraud with the physical presence of the card, and electronic fraud without the physical presence of the card. In the first case the fraud demands the acquisi- tion of the credit card and the identity of the actual owner. In order to perform a transaction, the imposter must be physically present. The second category does not require physical presence of the card or its owner/imposter and is targeted to online transactions, where only identity and safety details are required. The latter category accounts for more than 70% of the worldwide credit card fraud (Robertson 2016), given that no face-to-face contact between seller and buyer is required. Despite the use of several technological improve- ments such as the Address Verification System (AVS), Chip and Pin verifica- tion and the Card Verification Code (CVV), new credit card fraud strategies are continuously being developed. This makes the automated timely detection of fraudulent transactions a very significant defense mechanism in combating fraud and reducing the associated losses to financial institutions.\n\n\n\n---\n\ne2086354-1586 V. PLAKANDARAS ET AL. Modern data-driven statistical and machine-learning (ML) methods can provide statistical-like predictive models that output the probability of a transaction to be fraudulent and address the above challenge. Indeed, ML applications have shown to be promising in fraud detection (see Related Work). However, each such application requires coding its own script, experi- mentation with several algorithms, and significant experience with statistical and machine learning methods as some ML algorithms do not converge in big sample data, some return sub-optimal predictive models, some are inappropri- ate for imbalanced outcomes, others require fine-tuning of their hyper- parameter values, others are hard to explain or interpret, or challenging to combine with feature selection (see Related Work Section). Manual scripting is also time-consuming and prone to methodological errors. Thus, the chal- lenges that arise are ‚Äúcan credit card fraud predictive modeling be automated? Do the resulting models compete with the ones developed by human experts? Does automation obfuscate the interpretation of the model, or can it actually also help in obtaining intuition into the data patterns and task?‚Äù To respond to such challenges, systems and services that automate a large part of the machine learning pipeline have recently appeared under the name of Automated Machine Learning (AutoML) system. Such systems automate the selection of ML algorithms, the tuning of their hyper-parameter values, the estimation of performance, and the visualization and interpretation of results. In this paper, we demonstrate how AutoML tools could potentially increase the productivity of detecting fraudulent credit card transactions without a reduction in the prediction performance compared to a manual analysis. Specifically, we describe and use the Just Add Data Bio1 (hereafter JAD) AutoML tool on the fraud detection problem described above and achieve results on par with state-of-the-art previous analyses that are manually coded. Secondly, in addition to modeling, JAD performs automated feature selection to identify the most significant variables to fraud detection, providing valuable intuition to fraud inspectors. We‚Äôd like to note that JAD‚Äôs feature selection considers features jointly (multivariate) and not simply one by one. Features that are informative by themselves may become redundant given other fea- tures; similarly, features that are uninformative by themselves may be neces- sary for optimal prediction and become informative given other features. Hence, optimal feature selection is a combinatorial problem that returns the minimal-size feature subset that in combination leads to the optimally pre- dictive model. After examining numerous combinations of algorithms for feature selection and modeling, as well as their tuning hyper-parameter values, JAD selects the best one to create a final model for prediction. It estimates its predictive performance along several common metrics (e.g., AUC, accuracy, balanced accuracy, F1 score), the confidence intervals of performance, the Receiver Operating Characteristic (ROC) curve, and the contribution to per- formance for each selected feature.\n\n\n\n---\n\nAPPLIED ARTIFICIAL INTELLIGENCE e2086354-1587 Post-analysis, JAD provides an easy way to access the trained model and apply it on new data to get predictions, without the need for computer coding. This means that any employee of a financial institution or a credit card firm can get predictions and try different scenarios of credit card transactions to gauge how predictions change with the feature values. JAD also supports collaborative analyses by sharing projects, data, and analyses results; the later can also be shared with anybody via unique links to the specific results‚Äô page.2 The present work provides evidence that AutoML systems can indeed address to a large extent the challenges for automated credit card fraud detection modeling, at least within the limited scope of the present computational experi- ments performed. JAD does automatically output predictive models that can compete with prior work, selects the important features for prediction removing irrelevant and redundant features, and helps explain and interpret results. Several limitations of course, remain (see Discussion). Nevertheless, Auto ML can open a new path of research and provide supervision tools to the industry that overcome some of the limitations and obstacles of academic research. Based on this work, we argue that AutoML tools and services should be considered when analyzing credit card transaction data and potentially, other similar-type financial data. The simplicity, accuracy and speed of such systems make them an excellent fit in such financial transaction situations. The model can filter and flag a transaction as probably fraudulent in real time out of thousands of other transactions, keeping human intervention to a minimum. The remainder of the paper is organized as follows. In section 2 we describe in more detail the Related Work. In Section 3, we describe the data and the methodology, while the empirical findings are presented in section 4. Section 5 discusses the limitations of the study, and Section 6 concludes the paper. Related Work The obvious financial benefits in detecting fraudulent transactions has sparked a voluminous literature in the field. The first attempts to create automate detection systems that examine an‚Äì‚Äìoften‚Äîlarge number of transactions and classify them as fraudulent or legitimate, are expert systems based on a set of classification rules (Hanagandi, Dhar, and Buescher 1996). Nevertheless, given that the distribution of credit card transaction datasets changes due to seasonality patterns, new market trends and the evolvement of new fraud strategies, the applied rules should be constantly updated, making rule-induction systems infeasible and ineffective. Following an econometric approach, Ng and Jordan (2002) compare logistic regression with Na√Øve Bayes classification models, showing that logistic regression models have a lower asymptotic error than Bayes classifiers, but fail to converge in very large datasets, as the ones used in credit card transaction problems. The Bayes classifier converges quickly,\n\n\n\n---\n\ne2086354-1588 V. PLAKANDARAS ET AL. but its classification accuracy is lower than that of the logistic regression models. On a similar path, Maes et al. (2002) compare Bayesian and neural networks, concluding that the Bayesian network converges faster and exhibits a lower classification error than neural networks. In an extended benchmark simulation, Lessmann et al. (2015) compare 41 methodologies on various evaluation criteria and several credit scoring datasets. It is confirmed that the random forest method, i.e., the rando- mized version of bagged decision trees, outperforms logistic regression and has progressively become one of the standard models in the credit scoring industry (Grennepois, Alvirescu, and Bombail 2018). Over the last decades, the rapid advances in the field of ML, provided additional tools to the satisfaction of fraud investigators. In a thorough survey of the relevant literature Ngai et al. (2011) conclude that the most commonly used ML methods in fraud detection are decision trees, Artificial Neural Networks (ANN), Support Vector Machines (SVM) and genetic algorithms. These techniques can be used alone or in collaboration using ensemble or meta-learning techniques to build classifiers. Most of the applications are based on supervised training algorithms such as ANN (Dorronsoro et al. 1997; Prodromidis, Chan, and Stolfo 2000; Syeda, Zhang, and Pan 2002; Schindeler 2006; Juszczak et al. 2008; Quah and Sriganesh 2008) decision tree techniques like ID3, C4.5 and CART (Chen et al. 2005; Mena 2003; Wheeler and Aitken 2000) and SVM (Bhattacharyya, 2011). A synopsis of the relevant literature suggests that classification performance of ML methodologies is heavily dependent on the dataset under study, with Bayesian networks and logistic regression exhibiting higher classification performance in smaller samples and ANNs and C4.5 decision trees outper- forming all competing methodologies in larger samples. An obvious contrast of the previous works to the current proposed direction, is that a large part of the effort goes to the identification of the best algorithms for the given task and the optimization of the hyper-parameter. Moreover, as the number of the observations increases, the task of selecting the most informative features becomes a computationally impossible task. Thus, many researchers select a number of variables (often arbitrarily), conditioning the performance of their model to subjective feature selection processes. In contrast, the AutoML approach completely automates feature selection and model tuning. Data and Methodology The Data For our analysis we use a large and frequently used in the literature cross-sectional dataset on credit card fraud detection, available in Dal Pozzolo et al. (2014).3 The dataset includes online credit card\n\n\n\n---\n\nAPPLIED ARTIFICIAL INTELLIGENCE e2086354-1589 transactions made in September 2013 by European cardholders. It con- sists of 492 fraudulent out of a total of 284,807 transactions for a two- day period. Thus, the fraud rate is approximately 0.172% of all transac- tions or approximately 1 in every 579 transactions. The data contains 28 anonymized variables, plus two named variables ‚ÄúTime‚Äù and ‚ÄúAmount.‚Äù The anonymized variables are the result of a Principal Component Analysis (PCA) transformation of the original data for confidentiality issues. The time feature contains the seconds elapsed between each transaction and the first transaction in the dataset. The ‚ÄúAmount‚Äù feature is the transaction amount. Regarding the anonymized nature of the features, as stated in Carneiro, Figueira, and Costa (2017), the variables typically collected by financial institutions regarding credit card transactions are similar, since they are regulated by monetary authorities. Variable ‚ÄúAmount‚Äù ranges from ‚Ç¨0.1 to ‚Ç¨25,691.16, with an average of ÔøΩx ¬º 88:35 and a standard deviation of s ¬º 250:12. Table 1 provides an overview of the descriptive statistics of this variable. As we observe from Panel A, the data are severely skewed toward the left tail, while this finding is also highlighted in Panel B, since the majority of transactions are under ‚Ç¨200. According to the Augmented Dickey-Fuller and the Kwiatkowski‚ÄìPhillips‚ÄìSchmidt‚ÄìShin tests, the variable is stationary. Just-add-Data JAD is a Software-as-a-Service platform that runs on AWS, available at jadbio. com. JAD employs some simple feature transformations and imputation of missing values. For feature selection, it employs the Statistically-Equivalent- Signature (Lagani et al. 2017) algorithm (SES for short). A feature selection algorithm ideally returns a subset of the features that is minimal in size, and optimally predictive in a multivariate fashion, i.e., when all features are considered jointly. The predictors selected by SES are the neighbors of the outcome in any faithful Bayesian Network representing the data distribution, which a subset of the full Markov Blanket. The latter has been shown to be the optimal solution to the feature selection problem under certain broad condi- tions (Tsamardinos and Aliferis 2003). A feature of SES is that it heuristically and efficiently attempts to identify statistically, equivalent solutions, i.e., mini- mal-sized feature subsets with the same optimal predictive performance. Identifying all equivalent solutions is important when feature selection is employed for knowledge discovery and getting insight to the domain under study. Returning an arbitrarily chosen single solution S may mislead the domain expert into thinking that all other variables are either redundant or irrelevant, when they could just be substituting for a selected feature without loss of predictive power.\n\n\n\n---\n\ne2086354-1590 V. PLAKANDARAS ET AL. Table 1. Overview of the amount variable. Panel A: Descriptive statistics Mean Standard deviation Kurtosis Skewness Normality (Jarque-Bera test) Augmented Dickey-Fuller (ADF) test Kwiatkowski‚ÄìPhillips‚ÄìSchmidt‚ÄìShin (KPSS) test\n\n### ‚Ç¨88.35 ‚Ç¨250.12 848.07 16.98 8,488* ‚àí473.71* 6.68\n\nPanel B: Data distribution according to the value of each transaction.\n\n### >‚Ç¨2,000 (‚Ç¨1,000- ‚Ç¨2,000] (‚Ç¨500- ‚Ç¨1,000] (‚Ç¨300- ‚Ç¨500] (‚Ç¨200- ‚Ç¨300] (‚Ç¨0, ‚Ç¨200]\n\n### 0.237% 0.795% 2.178% 3.131% 3.784% 89.875%\n\nNote: * denotes rejection of the null hypothesis at the 5% level of significance. The null hypothesis of the Jarque ‚Äì Bera test is that the data originate from a normal distribution. The null of the ADF test is that the data are nonstationary, while the null of the KPSS test is that the data are stationary.\n\n\n\n---\n\nAPPLIED ARTIFICIAL INTELLIGENCE e2086354-1591 For classification, JAD considers Decision Trees (DT), Random Forests (RF), Support Vector Machines (SVM) with full polynomial and Gaussian kernels, and Ridge Logistic Regression. All the algorithms included above require the user to set the values of hyper-parameters. Hyper-parameters determine the behavior of an algorithm, typically regulating how sensitive the algorithm is in detecting patterns. The optimal values of the hyper- parameters must be found by trial-and-error. Results can vary greatly depend- ing on their appropriate tuning. Using an Artificial Intelligence (AI) system JAD automatically decides which algorithms to try and which hyper- parameter values, depending on the size of the data, the type of the data, and the user preferences. The AI system is based on a set of rules that guide the fine-tuning process. JAD then generates all combination of choices called configurations. A configuration is a pipeline of algorithms with specific hyper- parameters that take the data and lead to a forecasting model. To determine which configuration leads to the best model, JAD estimates the performance of the average model produced by each configuration using a (stratified) N-repeated, K-fold cross-validation protocol. The (standard) K-fold cross-validation (CV) protocol splits the data into K non-overlapping approximately equal-sized sets (called folds) of samples. The value K to use is determined by the AI system. The procedure progresses by keeping each fold out once, training models using all configurations on the remaining K-1 folds and estimating their performance on the held-out fold. The held-out test sets are used to simulate the application of the models on new, never- seen-before samples and to estimate the predictive performance obtained by training a specific configuration. In the end, the K performance estimates are computed on each fold, as well as the average, for each configuration. The configuration with the best average performance is selected as the winning configuration. For details on the repetition and stratification of CV see Tsamardinos, Greasidou, and Borboudakis (2018). To produce the final model, JAD applies the winning configuration on the full dataset. The reasoning behind this is that we expect that the model learnt on all the data to be best on average. Unfortunately, the cross-validated performance estimate of the winning configuration is optimistically biased and should not be reported as the final estimate. This is because numerous configurations have been tried. This is a statistical phenomenon conceptually equivalent to the adjustment of p-values in multiple hypothesis testing and related to the ‚Äúwinner‚Äôs curse‚Äù in biostatistics (Zollner and Pritchard 2007). In computer science it is called the Multiple Comparisons in Induction problem (Jensen and Cohen, 2000). JAD estimates the bias of the performance and the confidence intervals using a bootstrap-based method called Bootstrap Bias Corrected CV or BBC-CV and removes it to return the final performance estimate (adjust estimations for multiple tries of algorithms/configurations).The selection of the optimum\n\n\n\n---\n\ne2086354-1592 V. PLAKANDARAS ET AL. forecasting model is performed based on the Area Under the Receiver Operating Characteristic (AUC-ROC) curve, that explores the trade-offs between sensitivity and specificity of the model and selects the most cost- effective operational point. In addition to performance estimates, JAD provides several plots to help user understand and interpret results. The first is a Supervised 2D PCA plot, i.e., a 2D PCA plot based on the selected features (hence, the characteriza- tion ‚Äúsupervised‚Äù). The goal is to visually understand the data and detect anomalies (outliers) in the dataset. The Individual Conditional Expectation (ICE) plots displays how each instance‚Äôs prediction changes when a feature changes, in an effort to explain the role of each feature in the prediction output of the model. The Cumulative Variable Importance aims to explain the added value of each feature to the final forecast. Nevertheless, we do not provide extensive analysis on the feature selection abilities of JAD, given that the 28 variables of the credit-card transaction dataset come from a PCA compression of the original financial variables. Moreover, there is no information regarding the order of the variables; we do not possess infor- mation that the first variable is actually the first component of the PCA analysis, the second variable the second component etc. Thus, we do not present other post-analysis information, given that we cannot actually support evidence of the importance of an actual financial variable in forecasting. Empirical Findings In order to assess the ability of the JAD application to train and forecast credit card fraud in unknown data we split our sample into 2 parts using stratified sampling: 90% of the data are used to train the models and 10% is kept aside and it is only used to test the forecasting ability of the trained model to unknown data. Thus, we use 256,552 observations for training of which 446 correspond to credit card fraud and we left 28,255 observations for testing (46 are credit card fraud cases). Fraudulent transactions are labeled Class 1 and the rest are labeled Class 0. Overall, it took 8 hours and 40 minutes for JAD to train 415 models and test alternative configurations on different subsets of the training data. The best overall configuration in terms of maximizing the Area Under the Curve on the training dataset is: a) selecting features using the SES algorithm with hyper- parameters maxK-conditioning-set = 2 and significance level a = 0.1 and b) fit (learn) a ridge logistic regression model with penalty hyper-parameter lambda = 100. In step a) JAD selected 7 out of the total 30 explanatory variables (features) in our sample as the ones required for the optimal credit card fraud detection. The model with the highest predictive performance is:\n\n\n\n---\n\nAPPLIED ARTIFICIAL INTELLIGENCE e2086354-1593 ÔøΩ ÔøΩ P√∞y ¬º 1jX√û ln i ¬º 7:89√æ 0:66xi;14  0:51xi;4 √æ 0:22x 1 i;10 √æ 0:22xi;10 P√∞yi ¬º 1jX√û √æ 0:22xi;13 √æ 0:13xi;9 √æ 0:12xi;12 (1) where P√∞yi ¬º 1jX√û is the probability of observation yi of a transaction belong- ing to Class 1 (fraudulent) of seven regressors where xi,j is the ith observation of variable (feature) . The predictive performance is measured using several metrics reported in Table 2. The simplest of these metrics is classification accuracy, which equals the probability of the model making a correct classification on a new transac- tion. As we observe from Table 2, the overall classification performance of the best performing model is 99,9%. Nevertheless, this metric is not suitable to measure predictive performance in heavily unbalanced datasets. One can achieve a 99.93% accuracy by classifying all transactions as Class 0, since Class 1 (fraudulent cases) accounts only for the 0.172% of all observations. Thus, classification accuracy is a metric that is affected by the class dis- tribution. A better metric typically used for binary classification, is the area under the ROC curve (AUC). The AUC is a metric that is independent of the class distribution. It is also invariant to a change in the class distribution between the train and test sets, in other words, it will not be affected if the percentage of fraudulent transactions increases in the test data (provided this is the only change in the data distribution). Nonetheless, as we men- tioned above, we used stratified sampling so that our test and training distribution remain consistent. The AUC also has another, statistically intuitive interpretation: it is the probability that the model will correctly assign a higher probability of being fraudulent to a pair of transactions, Table 2. Accuracy metrics. Metrics Train Test Overall Accuracy 0.999 [0.999,1.000] 0.999 [0.999,1.000] Area Under the ROC Curve 0.973 0.981 Balanced Accuracy for class 0 0.891 [0.872,0.910] 0.924 [0.867, 0.972] Balanced Accuracy for class 1 0.891 [0.872,0.910] 0.924 [0.867,0.972] F-measure for class 0 0.999 [0.999,1.000] 0.999 [0.999, 1.000] F-measure for class 1 0.787 [0.757,0.815] 0.821 [0.727,0.898] Precision for class 0 0.999 [0.999,1.000] 0.999 [0.999, 1.000] Precision for class 1 0.791 [0.752,0.828] 0.796 [0.675,0.905] Recall for class 0 0.999 [0.999,1.000] 0.999 [0.999, 1.000] Recall for class 1 0.783 [0.744,0.815] 0.848 [0.735,0.944] Sensitivity for class 0 0.999 [0.999,1.000] 0.999 [0.999, 1.000] Sensitivity for class 1 0.783 [0.744,0.820] 0.848 [0.735, 0.944] Specificity for class 0 0.783 [0.744,0.820] 0.848 [0.735, 0.944] Specificity for class 1 0.999 [0.999,1.000] 0.999 [0.999, 0.999] Note: Class 0 denotes legitimate transactions, while Class 1 denotes fraudulent ones. 95% confidence intervals are reported in brackets.\n\n\n\n---\n\ne2086354-1594 V. PLAKANDARAS ET AL. given that one fraudulent and the other is legitimate. In our case, the AUC is 0.973, suggesting a high identification ability of the legitimate vs. frau- dulent transactions. The model estimates the probability that a new transaction is fraudulent i.e. P(y = 1|x), given the values x of the seven features of the transaction selected in the training step. To classify a new observation, one uses a threshold t, such that, if the probability is higher than t, the transaction is classified as fraudulent. Depending on t one can become more or less conservative in classifying any transaction as fraudulent. Depending on t one can achieve various values of sensitivity (percentage of fraudulent correctly classified), specificity (percentage of non-fraudulent correctly classified), true positive rate (which equals sensitivity), false-positive rate (which equals 1-specificity), precision, and recall. The ROC curve depicts all the potential tradeoffs between true positive rate and false positive rate (false alarms). Typically, to increase the true positive rate we must accept an increase in the false positive (false alarms) rate as well. The rate of this trade-off is described by the slope of the ROC. The ROC created by JAD for this problem is shown in Figure 1. The evaluation of a fraud detection model is more complex than simply identifying the model with the top predictive performance; the model should also aim at the best cost-effective classification, as it is defined a) by the cost of Figure 1. The ROC curve for Class 1 (fraudulent transactions), along with the respective 95% confidence intervals. The thick dashed (blue) line is the estimated ROC from the training data. Different points on the ROC curve provide a different trade-off between False Positive Rate (FPR) and the true positive rate (sensitivity) of the classifier. JAD can output models operating at different FPRs by selecting any of the circles.\n\n\n\n---\n\nAPPLIED ARTIFICIAL INTELLIGENCE e2086354-1595 Table 3. The confusion matrix. Train Test (Out-of-sample forecasting) Panel A: Cases Actual legitimate Actual fraudulent Actual legitimate Actual fraudulent Estimated legitimate 256,014 97 28,199 7 Estimated fraudulent 92 349 10 39 Total 256,106 446 28,209 46 Panel B: Percentages Actual legitimate Actual fraudulent Actual legitimate Actual fraudulent Estimated legitimate 99.96% 21.75% 99.96% 15.22% Estimated fraudulent 0.04% 78.25% 0.04% 84.78% Total 100% 100% 100% 100% misclassifying a fraudulent (true positive) transaction as legitimate (false negative), b) the cost of false positives, and c) the ratio of prevalence between positives and negatives. JAD can produce models that operate on any thresh- old and achieve several sensitivity-specificity trade-offs. The metrics shown in Table 2 are calculated with a threshold of 0.0481, selected from the ROC curve during the training phase, as the threshold that maximizes true positive rate and minimizes false-positive rate for Class 1. Balanced accuracy refers to the average of the proportion corrects of each class individually to account for the seriously imbalanced nature of the dataset. As we observe from Table 2, in terms of detecting fraudulent transactions (sensitivity of Class 1) our classifier achieves 78% on the training and 85% on the test sample, while the identification of legitimate transactions (specificity) reaches 100% in both cases. For the visualization of our results, in Table 3 we report the confusion matrix of the train and test sample. The best model identified by JAD correctly identified 39 out of the 46 fraudulent transactions (84.78%), missing only 7 transactions (15,22%) and producing 10 false positives. Thus, out of the 49 cases of credit card transactions that would be flagged for manual inspection, only 10 cases would be false alarms. Given that we are provided with the exact amount of each transaction we can study the behavior of the model on each of the observed instance of Table 3. The descriptive statistics are reported in Table 4. The economic valuation of the credit card fraud detection by JAD is very interesting. The ridge regression model correctly identified 39 frau- dulent transactions saving 7,535.24‚Ç¨ to the financial institution, while it has missed 7 transactions with a total cost of 477.64‚Ç¨. Most of the missed instances are small transactions below 50‚Ç¨ (39.90‚Ç¨, 11.39‚Ç¨, 3.39‚Ç¨ and the rest are below 1‚Ç¨), while only two transactions (311,91‚Ç¨ and 108,51‚Ç¨) exceed the amount of 100‚Ç¨. The false alarm transactions are all transac- tions below 1‚Ç¨ except one transaction of 89.90‚Ç¨. Thus, JAD exhibited the ability to efficiently detect all financially significant fraud transactions (above 500‚Ç¨) and to minimize the financial fraud cost and the adminis- trative cost of manual inspection.\n\n\n\n---\n\ne2086354-1596 V. PLAKANDARAS ET AL. Table 4. Descriptive statistics of out-of-sample forecasts. Fraudulent transactions Fraudulent transactions False identified missed Alarms Minimum 0.77‚Ç¨ 1.00‚Ç¨ 0.77‚Ç¨ Maximum 1809,68‚Ç¨ 311,91‚Ç¨ 89,99‚Ç¨ Mean 193,21‚Ç¨ 68,23‚Ç¨ 10,84‚Ç¨ Sum 7,535.24‚Ç¨ 477.64‚Ç¨ 97.53‚Ç¨ Observations 39 7 10 Number of transactions below 10‚Ç¨ 21 3 9 Number of transactions between 10‚Ç¨ and 1 2 1 50‚Ç¨ Number of transactions between 50‚Ç¨ and 4 0 0\n\n## 100‚Ç¨\n\nNumber of transactions between 100‚Ç¨ 8 2 0 and 500‚Ç¨ Number of transactions above 500‚Ç¨ 5 0 0 Comparing our findings with previous studies on the same dataset, we observe that our AutoML JAD setup exhibits similar or higher fraud detection abilities, while its AI interface simplifies the variable selection and fine-tuning procedures that are required compared to other applications. More specifi- cally, Dal Pozzolo et al. (2015) is the first use of the dataset in our study. The authors train Logit Boost, Random Forests and Support Vector Machines (SVM) classifiers in forecasting credit card fraud based on an under- sampling scheme. Awoyemi et al. (2017) train a Logistic Regression, a Na√Øve Bayes and a K-Nearest Neighbors classifier in forecasting credit card fraud using the same dataset, but without feature selection. Fiore et al. (2019) use the same dataset to produce artificial fraudulent transactions using a Deep Learning Artificial Neural Network (DLANN), in order to balance the dataset. Then, the artificial data are merged with the original dataset and a new DLANN is trained on the balanced dataset, keeping the last 30% observations for model evaluation (out-of-sample forecasting). Their application requires tuning 2 DLANN models that is a computationally intense and time- consuming procedure, while it requires expert knowledge and is prone to handling errors. The comparative results pertaining to fraudulent transactions (Class 1) in out-of-sample forecasting are reported in Table 5. Table 5. Comparison to earlier studies. Plakandaras et al. Dal Pozzolo et al. Johm, Adetunmbi, and Fiore et al. Metrics/Model (JAD) (2015) Oluwadare (2017) (2019) Overall Accuracy 0.999 0.969 0.999 Area Under the ROC 0.981 0.975‚Äì0.999 Curve Balanced Accuracy 0.924 0.928 0.851 F-measure 0.821 0.561 0.811 Precision 0.796 0.410 0.958 Recall 0.848 0.884 0.958 Sensitivity 0.848 0.884 0.702 Specificity 0.999 0.971 0.999\n\n\n\n---\n\nAPPLIED ARTIFICIAL INTELLIGENCE e2086354-1597 Overall, our AutoML approach simplifies training and testing even in such an imbalanced sample, produces a battery of useful forecasting performance metrics, while it achieves a similar or superior detection rate to the one reported in the literature. Limitations In terms of limitations, the current version of JAD does not automatically detect data distribution drift, perform automated data cleaning, raise alarms when the model seems to be invalidated in new samples, and in general, lacks functionalities for automatic model maintenance. In addi- tion, it requires formatting the data as a 2-dimensional matrix. In practice however, credit card data are originally stored in relational databases and require extensive data engineering for feature extraction and construction, a step that is not automated. A limitation of the specific study stems from the fact that the features have been linearly transformed using PCA on the original measured quantities. This precludes the economic and financial interpretation of the selected features. Further experimentation with more financial datasets is necessary to generalize further the conclusions of the study. Conclusion In this paper we use an AutoML SaaS platform, namely JAD, to credit cards fraud detection on a dataset of 284,807 online transactions. JAD automatically performs imputation, feature selection, modeling, fine tun- ing of the hyper-parameters of a significantly large number of models and estimates predictive performance and confidence intervals. The automatic nature of the application provides model training and model selection in a manner that shields against methodological errors and is accessible to all users, expert and non-experts alike. Moreover, the user- friendly interface makes the retraining of the model effortless and the model update straightforward. The gains in generality and applicability do not come at the expense of forecasting performance, given that our approach has matched or superseded existing applications on the same dataset. Notes\n\n1. JAD Bio has been developed specifically for low-sample, high-dimensional, molecular\nbiology data however, its algorithms are general enough to provide high-quality results in this application without any further customizations specifically for enterprise data.\n\n\n\n---\n\ne2086354-1598 V. PLAKANDARAS ET AL.\n\n2. The results for this analysis can be found at https://app.jadbio.com/share/4589e2ee-\n70aa-4594-aa5e-bae8d36c59ce\n\n3. The dataset can be accessed at https://www.kaggle.com/mlg-ulb/creditcardfraud\nDisclosure Statement No potential conflict of interest was reported by the author(s).\n\n## ORCID\n\nVasilios Plakandaras http://orcid.org/0000-0001-9351-9546 Periklis Gogas http://orcid.org/0000-0002-5134-3869 Theophilos Papadimitriou http://orcid.org/0000-0001-9035-183X Ioannis Tsamardinos http://orcid.org/0000-0002-2492-959X Ethical Approval This article does not contain any studies with human participants or animals performed by any of the authors. References Bhattacharyya, S., S. Jha, K. Tharakunnel, and J. C. Westland. 2011. Data mining for credit card fraud: A comparative study. Decision Support Systems 50 (3):602‚Äì13. doi:10.1016/j. dss.2010.08.008. Carneiro, N., G. Figueira, and M. Costa. 2017. data mining-based system for credit-card fraud detection in e-tail. Decision Support Systems 95:91‚Äì101. doi:10.1016/j.dss.2017.01.002. Chen, R.-C., S.-T. Luo, X. Liang, and V. C. S. Lee. 2005. Personalized approach based on SVM and ANN for detecting credit card fraud. Proceedings of the IEEE international conference on neural networks and brain. 810‚Äì15. Beijing, China. Dal Pozzolo, A. C., O. Caelen, Y.-A. Le Borgne, S. Waterschoot, and G. Bontempi. 2014. Learned lessons in credit card fraud detection from a practitioner perspective. Expert Systems with Applications 41 (10):4915‚Äì28. doi:10.1016/j.eswa.2014.02.026. Dal Pozzolo, A., O. Caelen, A. J. Reid, and G. Bontempi. 2015. Calibrating probability with undersampling for unbalanced classification. Symposium on Computational Intelligence and Data Mining (CIDM) Cape Town, South Africa, IEEE. Dorronsoro, J. R., F. Ginel, C. Sgnchez, and C. Cruz. 1997. Neural fraud detection in credit card operations. IEEE Trans. Neural Netw 8 (4):827‚Äì34. doi:10.1109/72.595879. European Central Bank. 2018. Fifth report on card fraud. https://www.ecb.europa.eu/pub/ cardfraud/html/ecb.cardfraudreport201809.en.html#toc11 . Fiore, U., A. De Santis, F. Perla, P. Zanetti, and F. Palmieri. 2019. Using generative adversarial networks for improving classification effectiveness in credit card fraud detection. Information Sciences 479:448‚Äì55. doi:10.1016/j.ins.2017.12.030. Grennepois, N., M. Alvirescu, and M. Bombail. 2018. Using random forest for credit risk models. London, UK: Deloitte Risk Advisory.\n\n\n\n---\n\nAPPLIED ARTIFICIAL INTELLIGENCE e2086354-1599 Hanagandi, V., A. Dhar, and K. Buescher. 1996. Density-based clustering and radial basis function modeling to generate credit card fraud scores. Proceedings of the IEEE/IAFE conference New York, NY, USA. 247‚Äì51. Jensen, D., and P. Cohen. 2000. Multiple Comparisons in Induction Algorithms. Machine Learning 38 (3):309‚Äì38. doi:10.1023/A:1007631014630. Johm, A., A. O. Adetunmbi, and S. A. Oluwadare. 2017. Credit card fraud detection using machine learning techniques: A comparative analysis, IEEE International Conference on Computing Networking and Informatics (ICCNI), Lagos, Nigeria. Juszczak, P., N. M. Adams, D. J. Hand, C. Whitrow, and D. J. Weston. 2008. Off-the peg and bespoke classifiers for fraud detection. Computational Statistics & Data Analysis 52 (9):4521‚Äì32. doi:10.1016/j.csda.2008.03.014. Lagani, V., G. Athineou, A. Farcomeni, M. Tsagris, and I. Tsamardinos. 2017. Feature selection with the R package MXM: discovering statistically-equivalent feature subsets. Journal of Statisitcal Software 80 (7): 1‚Äì25 . Lessmann, S., B. Baesens, H.-V. Seow, and L. C. Thomas. 2015. Benchmarking stateof-the-art classification algorithms for credit scoring: An update of research. European Journal of Operational Research 247:124‚Äì36. doi:10.1016/j.ejor.2015.05.030. Maes, S., K. Tuyls, B. Vanschoenwinkel, and B. Manderick. 2002. Credit card fraud detection using Bayesian and neural networks. Proceedings of the 1st international naiso congress on neuro fuzzy technologies Havana, Cuba. 261‚Äì70. Mena, J. (2003). Investigate data mining for security and criminal detection. Amsterdam: Butterworth-Heinemann. C. Phua, V. Lee, K. Smith, and R. Gayler. Ng, A. Y., and M. I. Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes. Advances in Neural Information Processing Systems 2:841‚Äì48. Ngai, E., Y. Hu, Y. Wong, Y. Chen, and X. Sun. 2011. The application of data mining techniques in financial fraud detection: A classification framework and an academic review of literature. Decision Support Systems 50 (3):559‚Äì69. doi:10.1016/j.dss.2010.08.006. Prodromidis, A. L., P. K. Chan, and S. J. Stolfo. 2000. Meta-learning in distributed data mining systems: Issues and approaches. In Chapter 3 Advances of distributed data mining, ed.\n\nH. Kargupta and P. Chan, AAAI Press pp. 81‚Äì114 .\nQuah, J. T., and M. Sriganesh. 2008. Real-time credit card fraud detection using computational intelligence. Expert Systems with Applications 35 (4):1721‚Äì32. doi:10.1016/j.eswa.2007.08.093. Robertson, D. The Nilson report. 2016. https://www.nilsonreport.com/upload/content_ promo/The_Nilson_Report_10-17-2016.pdf . Schindeler, S. 2006. Fighting card fraud in the USA. In Credit control, 50‚Äì56. House of Words Ltd. Syeda, M., Y. Zhang, and Y. Pan. 2002. Parallel granular neural networks for fast credit card fraud detection. Proceedings of the 2002 IEEE international conference on fuzzy systems Honolulu,\n\n## HI, USA. 572‚Äì77.\n\nTsamardinos, I., and C. Aliferis. 2003. Towards Principled Feature Selection: Relevancy, Filters and Wrappers, Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics, AISTATS, Key West, Florida, USA, January 3-6, 2003. Society for Artificial Intelligence and Statistics. Tsamardinos, I., E. Greasidou, and G. Borboudakis. 2018. Bootstrapping the out-of-sample predictions for efficient and accurate cross-validation. Machine Learning 107 (12):1895‚Äì922. doi:10.1007/s10994-018-5714-4. Wheeler, R., and S. Aitken. 2000. Multiple algorithms for fraud detection. Knowledge- Based Systems 13 (2/3):93‚Äì99. doi:10.1016/S0950-7051(00)00050-2. Zollner, S., and J. Pritchard. 2007. Overcoming the winner‚Äôs curse: Estimating Penetrance parameters from case-control data. American Journal of Human Genetics 80:605‚Äì15. doi:10.1086/512821.","knowledge_id":"0e3148f1-0fd3-4c47-a616-6047951e1e54","title":"Credit_Card_Fraud_Detection_with_Automated_Machine_Learning_Systems.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/088427aa-fefd-482d-aa0b-c0a1a73d905e/Credit_Card_Fraud_Detection_with_Automated_Machine_Learning_Systems.pdf"}
{"file_content":"Received March 20, 2022, accepted April 8, 2022, date of publication April 12, 2022, date of current version April 18, 2022. Digital Object Identifier 10.1109/ACCESS.2022.3166891 Credit Card Fraud Detection Using State-of-the-Art Machine Learning and Deep Learning Algorithms\n\n### FAWAZ KHALED ALARFAJ 1, IQRA MALIK2, HIKMAT ULLAH KHAN 3, NAIF ALMUSALLAM1,\n\n### MUHAMMAD RAMZAN 2, AND MUZAMIL AHMED 3\n\n1Department of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh 11564, Saudi Arabia 2Department of Computer Science and Information Technology, University of Sargodha, Sargodha 40100, Pakistan 3Department of Computer Science, COMSATS University Islamabad, Wah Campus, Wah Cantt 47040, Pakistan Corresponding author: Hikmat Ullah Khan (hikmat.ullah@ciitwah.edu.pk) This work was supported by the Deanship of Scientific Research at Imam Mohammad Ibn Saud Islamic University through the Research Group under Grant RG-21-51-01. ABSTRACT People can use credit cards for online transactions as it provides an efficient and easy-to-use facility.With the increase in usage of credit cards, the capacity of credit cardmisuse has also enhanced. Credit card frauds cause significant financial losses for both credit card holders and financial companies. In this research study, the main aim is to detect such frauds, including the accessibility of public data, high-class imbalance data, the changes in fraud nature, and high rates of false alarm. The relevant literature presents many machines learning based approaches for credit card detection, such as Extreme Learning Method, Decision Tree, Random Forest, Support Vector Machine, Logistic Regression and XG Boost. However, due to low accuracy, there is still a need to apply state of the art deep learning algorithms to reduce fraud losses. The main focus has been to apply the recent development of deep learning algorithms for this purpose. Comparative analysis of both machine learning and deep learning algorithms was performed to find efficient outcomes. The detailed empirical analysis is carried out using the European card benchmark dataset for fraud detection. Amachine learning algorithmwas first applied to the dataset, which improved the accuracy of detection of the frauds to some extent. Later, three architectures based on a convolutional neural network are applied to improve fraud detection performance. Further addition of layers further increased the accuracy of detection. A comprehensive empirical analysis has been carried out by applying variations in the number of hidden layers, epochs and applying the latest models. The evaluation of research work shows the improved results achieved, such as accuracy, f1-score, precision and AUC Curves having optimized values of 99.9%,85.71%,93%, and 98%, respectively. The proposed model outperforms the state-of-the-art machine learning and deep learning algorithms for credit card detection problems. In addition, we have performed experiments by balancing the data and applying deep learning algorithms to minimize the false negative rate. The proposed approaches can be implemented effectively for the real-world detection of credit card fraud. INDEX TERMS Fraud detection, deep learning, machine learning, online fraud, credit card frauds, transaction data analysis.\n\nI. INTRODUCTION fraud. Card-not-present fraud, or the use of your credit card\nCredit card fraud (CCF) is a type of identity theft in which number in e-commerce transactions has also become increas- someone other than the owner makes an unlawful transac- ingly common as a result of the increase in online shopping. tion using a credit card or account details. A credit card Increased fraud, such as CCF, has resulted from the expan- that has been stolen, lost, or counterfeited might result in sion of e-banking and several online payment environments, resulting in annual losses of billions of dollars. In this era of The associate editor coordinating the review of this manuscript and digital payments, CCF detection has become one of the most approving it for publication was Liangxiu Han . important goals. As a business owner, it cannot be disputed 39700 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 10, 2022\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nthat the future is heading towards a cashless culture. As a studies have examined the application of deep neural net- result, typical payment methods will no longer be used in the works in identifying CCF. [3]. It uses a number of deep future, and therefore they will not be helpful for expanding learning algorithms for detecting CCF. However, in this study, a business. Customers will not always visit the business with we choose the CNN model and its layers to determine if the cash in their pockets. They are now placing a premium on original fraud is the normal transaction of qualified datasets. debit and credit card payments. As a result, companies will Some transactions are common in datasets that have been need to update their environment to ensure that they can labelled fraudulent and demonstrate questionable transaction take all types of payments. In the next years, this situation behaviour. As a result, we focus on supervised and unsuper- is expected to become much more severe [1]. vised learning in this research paper. In 2020, there were 393,207 cases of CCF out of approx- The class imbalance is the problem in ML where the total imately 1.4 million total reports of identity theft [4]. CCF is number of a class of data (positive) is far less than the total now the second most prevalent sort of identity theft recorded number of another class of data (negative). The classification as of this year, only following government documents and challenge of the unbalanced dataset has been the subject of benefits fraud [5]. In 2020, there were 365,597 incidences of several studies. An extensive collection of studies can provide fraud perpetrated using new credit card accounts [10]. The several answers. Therefore, to the best of our knowledge, number of identity theft complaints has climbed by 113% the problem of class imbalance has not yet been solved. from 2019 to 2020, with credit card identity theft reports We propose to alter the DL algorithm of the CNN model increasing by 44.6% [14]. Payment card theft cost the global by adding the additional layers for features extraction and economy $24.26 billion last year. With 38.6% of reported the classification of credit card transactions as fraudulent or card fraud losses in 2018, the United States is the most otherwise. The top attributes from the prepared dataset are vulnerable country to credit theft. ranked using feature selection techniques. After that, CCF is As a result, financial institutions should prioritize equip- classified using several supervised machine-driven and deep ping themselves with an automated fraud detection system. learning models. The goal of supervised CCF detection is to create a machine In this study, the main aim is to detect fraudulent trans- learning (ML) model based on existing transactional credit actions using credit cards with the help of ML algorithms card payment data. The model should distinguish between and deep learning algorithms. This studymakes the following fraudulent and nonfraudulent transactions, and use this infor- contributions: mation to decide whether an incoming transaction is fraud- ‚Ä¢ Feature selection algorithms are used to rank the top ulent or not. The issue involves a variety of fundamental features from the CCF transaction dataset, which help problems, including the system‚Äôs quick reaction time, cost in class label predictions. sensitivity, and feature pre-processing. ML is a field of arti- ‚Ä¢ The deep learning model is proposed by adding a num- ficial intelligence that uses a computer to make predictions ber of additional layers that are then used to extract based on prior data trends [1] the features and classification from the credit card farad ML models have been used in many studies to solve detection dataset. numerous challenges. Deep learning (DL) algorithms applied ‚Ä¢ To analyse the performance CNNmodel, apply different applications in computer network, intrusion detection, bank- architecture of CNN layers. ing, insurance, mobile cellular networks, health care fraud ‚Ä¢ To perform a comparative analysis between ML with detection, medical andmalware detection, detection for video DL algorithms and proposed CNN with baseline model, surveillance, location tracking, Android malware detection, the results prove that the proposed approach outperforms home automation, and heart disease prediction. We explore existing approaches. the practical application of ML, particularly DL algorithms, ‚Ä¢ To assess the accuracy of the classifiers, performance to identify credit card thefts in the banking industry in this evaluation measures, accuracy, precision, and recall are paper. For data categorisation challenges, the support vector used. Experiments are performed on the latest credit machine (SVM) is a supervisedML technique. It is employed cards dataset. in a variety of domains, including image recognition [25], The rest of the paper is structured as follows: The second credit rating [5], and public safety [16]. SVM can tackle section examines the related works. The proposed model and linear and nonlinear binary classification problems, and it its methodology are described in depth in Section 3. The finds a hyperplane that separates the input data in the support dataset and evaluation measures are described in Section 4. vector, which is superior to other classifiers. Neural networks It also shows the outcomes of our tests on a real dataset, were the first method used to identify credit card theft in as well as the analysis. Finally, Section 5 concludes the paper. the past [4]. As a result, (DL), a branch of ML, is currently focused on DL approaches. II. RELATED WORK In recent years, deep learning approaches have received In the field of CCF detection, several research studies have significant attention due to substantial and promising out- been carried out. This section presents different research stud- comes in various applications, such as computer vision, nat- ies revolving around CCF detection. Moreover, we strongly ural language processing, and voice. However, only a few emphasise the research that reported fraud detection in the VOLUME 10, 2022 39701\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nTABLE 1. Algorithms of machine learning and their accuracy. FIGURE 1. Payment card authorisation process. problem of class imbalance. Many techniques are used to detect credit cards. Therefore, to study the most related work in this domain, themain approaches can be categories, such as DL, ML, CCF detection, ensemble and feature ranking, and user authentication approaches [1], [3]. Figure 1 shows the commonly used payment card autho- rization process for credit card authentication. There are two ways of authentication including passwords and authentica- tion through biometrics. Biometrics-based authentication can be further divided into three groups: physiological authenti- is that it was developed to relax and allow for dependencies cation and behavioural authentication, and combined authen- among variables. tication [4], [5]. Variable quantity is characterised as nodes, although dependencies of conditions between variables are shown as\n\nA. SUPERVISED MACHINE LEARNING APPROACHES arcs between nodes. The conditional probability table of each\nML has many branches, and each branch can deal with dif- node is linked, which makes the possibilities of the node‚Äôs ferent learning tasks. However, ML learning has different variable conditional on the parent‚Äôs node values [7], [8]. The framework types. The ML approach provides a solution for computational system of the bilateral-branch network (BBN) CCF, such as random forest (RF). The ensemble of the deci- is as follows: Finding a construction for the network is the sion tree is the random forest [3]. Most researchers use the first step: it was raised by human experts, which may be con- RF approach. To combine the model, we can use (RF) along ditional on the specific algorithms by using the data. When with network analysis. This method is called APATE [1]. this network topology originates, straightforwardly fitting the Researchers can use different ML techniques, such as super- network uses antique data in na√Øve Bayes so that the constant vised learning and unsupervised techniques. ML algorithms, variables are also discretised and supposedly distributed nor- such as LR, ANN, DT, SVM and NB, are commonly used mally. Correspondingly, in BBN, it is expected that each node for CCF detection. The researcher can combine these tech- is autonomous of its no offspring, assuming its maternities in niques with ensemble techniques to construct solid detection the graph [3], [9]. This is acknowledged as the condition of classifiers [3]. The linking of multiple neurons and nodes Markov. The linear classification model is a support vector is known as an artificial neural network. A feed-forward machine (SVM) and problems of regression. Rendering to perceptron multilayer is built up of numerous layers: an input the SVM algorithm, we can find the points closest to the layer, an output layer and one or more hidden layers. For line from both classes [10], [11]. These points are called sup- the representation of the exploratory variables, the first layer port vectors. This paper is concerned with the integration of contains the input nodes. With a precise weight, these input unsupervised techniques with supervised techniques for the layers are multiplied, and each of the hidden layer nodes is classification of CCF detection. Table 1 presents the summary transferred with a certain bias, and they are added together. of machine learning algorithms. An activation function is then applied to create the output of each neuron for this summation, which is then transferred B. DEEP LEARNING APPROACHES to the next layer. Finally, the algorithm‚Äôs reply is provided DL algorithms are useful, including the convolutional neural by the output layer. The first set randomly used weights network (CNN) algorithm, and more algorithms are deep and formerly used the training set to minimise the error. All belief networks (DBNs) and deep autoencoders; these are these weights were adjusted by detailed algorithms such as considered learning methods. They have numerous layers backpropagation [2], [6]. The graphic model for contingency of processing data, illustration learning and classification relationships between a set of variables is called the Bayesian of a pattern [7], [15]. The objective of deep-learning is belief network. The independence assumption in na√Øve Bayes to study artificial neural networks. The standard technique 39702 VOLUME 10, 2022\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nregards the size of neural networks, and it is considered takes two main modules. In training, all of the modules make the backpropagation model [8], [16]. The efficiency of the up a model of DL, which is a neural network. backpropagation algorithm decreases greatly, increasing the The main two methods used are a generator (G) and a depth of the neural networks, which can cause problems, discriminator (D). The network of the generator can generate such as insufficient local goals and a dilution of errors. Deep the data as simulated, and the difference between the simu- designs should be considered to be an achievement. They can lated data and the target data determines the discriminator, theoretically address the optimisation struggle in a profound yielding a determination that is true and false around the manner within the training parameters [17], [18]. virtual data. Finally, the model may generate higher-quality The training technique of the deep belief network is often simulation data to finish the data creation process [22], [23]. considered the effective primary case of deep architecture A VAE is a variational autoencoder with regularised training training. Traditional ML algorithms, such as SVM, DT and circulation to guarantee that its hidden space has adequate LR, have been extensively proposed for CCF detection [3]. assets, allowing us to create fresh data. A VAE is generated These traditional algorithms are not very well suited for large by introducing variation on the basis of the autoencoder. The datasets. A CNN is a DLmethod; it can deeply relate to three- VEG and theGANare extremely similar. Once again, the goal dimensional data, such as image processing. This method is is to change andmatch the data distribution to generate virtual similar to the ANN; the CNN has the same structure hidden data that is near the target [8], [22]. layer and a different number of channels in each layer in Usually, the number of samples is similar to that of a addition to special convolution layers. The idea of moving normal distribution. If all examples are found, thework can be filters through word convolution is linked to the data that very successful. Consequently, investigators frequently use can be used to capture the key information and automatically neural networks to approximate the mean and modification performs feature reduction. Thus, the CNN is widely used of normal distribution. Long short-term memory (LSTM) is in image processing. The CNN does not require heavy data an artificial recurrent neural network (RNN) architecture used pre-processing for training. in DL models [24], [25]. The LSTM network is compatible For image processing, the purpose of using a CNN is to with categorising, processing and building predictions based minimise processing without losing key features by reducing on time sequence data. The most common type of RNN is the image to make predictions [4], [6]. The main terms in the LSTM. An ordinary neural network (NN) cannot keep the CNN are feature maps, channels, pooling, stride, and track of the preceding information of a learning task every padding. For text, image and video processing, CNN models time they have to perform a task. In very simple words, with are conventionally used and take two-dimensional data as memory, the RNN is a neural network [26], [27]. RNNs tend input, which is called the 2DCNN. To learn the internal to have short-term memory because of the vanishing gradient representation, the feature mapping process is used from problem. The backbone of neural networks is backpropaga- the input data. The location of features is not relevant, and tion, as it reduces the loss by weights of network adjustment the same procedure can be used for one-dimensional data. by using gradients that it originated. In RNNs, as the gradient Natural language processing is a very popular example of a moves the backbone in the network, it shrinks, and then there 1DCNN application where sequence classification becomes is a minor update in weight. These small updates are affected a problem. In a 1DCNN, the kernel filter moves top to bottom by the earlier layers in the network. They do not learn more, in a sequence of a data sample, rather thanmoving left to right and the RNN loses the ability to recall early examples in long and top to bottom in the 2DCNN [17], [18]. sequences, making it a short-term memory network [28]. Raghavan [16] defined an autoencoder as an actual neural The use of DL methods is still very limited, and methods, network. An autoencoder can also encrypt the data the same such as CNN and LSTM are encouraged for image classifica- way as it would decrypt the data. In this method, for no tion, natural language processing (NLP), and RBM because anomalous points, the autoencoders are trained. According to of their ability to handle massive datasets. The way these DL the reconstruction error, it would present the anomaly ideas methods perform CCF classification is the major focus of this classify it as ‚Äôfraud‚Äô or ‚Äòno fraud,‚Äô meaning that the system has study [29]. In addition, data pre-processing is an important not been trained, which is predicted to have a higher amount stage in the ML process. How the classification performance of anomalies [19], [20]. However, a slight value overhead the is affected in response to data pre-processing when detecting higher bound value or considers the threshold an anomaly. credit cards is another question that needs to be answered. This technique is also used in [8], an autoencoder-based Table 2 presents the summary of deep learning algorithms. network detection of an anomaly. AMLmodel is a generative adversarial network where two neural networks collaborate to III. RESEARCH METHODOLOGY improve their prediction accuracy. GANs are often unsuper- Research is said to be methodical, and research methodol- vised and learn using an obliging zero-sum game framework. ogy is predicated by the applied research method. Applied The fundamental category of the deep-learning model is a research is administered to unravel the issues. Before real- GAN [11], [21], and the perception of development for DL world experimentation, the research covers all fundamentals progress it can offer is the most promising direction. GAN by performing these steps: VOLUME 10, 2022 39703\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nTABLE 2. Accuracy based results of deep learning algorithms. TABLE 3. The list of features available in the CCF dataset.\n\n### A. LIST OF FEATURES OF CREDIT CARD TRANSACTION\n\n## DATA\n\nTable 3 lists the important features and shows the mainframe TABLE 4. Characteristics of the dataset. transaction table of credit cards. Even though the whole construction of the transaction information table might be slightly dissimilar amongst card issuers, the vital character- istics recorded would be controlled in the database and are accessible for fraud detection modelling.\n\n## 1) EXPERIMENTAL STEP-UP\n\nWe discuss the dataset to be cast-off and the achievement evaluation measurements to be applied. a: DESCRIPTION OF DATASET The credit card dataset is accessible for research purposes. The dataset [11] holds transactions made by a cardholder over a two-day period, i.e., September 2018. There were 284,807 transactions in total, of which 492, or 0.172 percent, b: APPLIED MACHINE LEARNING & ENSEMBLE LEARNING were fraudulent. Because disclosing a consumer‚Äôs transaction TECHNIQUES details is considered a problem of confidentiality, the main We use and apply the following machine and ensemble learn- component analysis is applied to the majority of the dataset‚Äôs ing algorithm. features using principal component analysis (PCA). PCA is a standard and widely used technique in the relevant literature i) EXTREME LEARNING METHOD for reducing the dimensionality of such datasets, increasing The extreme learning method (ELM) is a neural network interpretability but at the same time minimizing information for classification, clustering, regression and feature learning. loss [2], [4], [19]. It does so by creating new uncorrelated vari- It can be used with one or a multilayer of unseen notes. ables that successively maximize variance. Table 4 presents Parameters of unseen nodes are tuned. The weights of the the detail of the dataset containing 31 columns, including output are hidden nodes learned in a single step. This is the time, V1, V2, V3. . . . . .V28 as PCA applied features, amount, essential amount that is needed to properly learn a linear and class labels. model. Given a single hidden layer of ELM, we assume that 39704 VOLUME 10, 2022\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nthe output function of the j-unseen node is h(z) = G (p, ‚àën Œ± q, z) wherever the parameters of the jth node are. The output iyi = 0; 0 ‚â§ Œ± ‚â§ C (5) j=1 function is as follows: ‚àën f vi) LOGISTIC REGRESSION L (z) = Œ≥ h (z) (1) j=1 i i Logistic regression is an easy algorithm that estimates the association between one dependent binary variable and inde- Œ≥i Is the weight of the output the ith hidden node? pendent variables, computing the probability of the occur- h (z) rence of an event. The regulation parameter C controls the = |Ghi (z) , . . . . . . , hL (z)| (2) trade-off between increasing complexity (overfitting) and ii) DECISION TREE keeping the model simple (underfitting). For large values of As a result, the decision tree classifier is used to create the C, the power of regulation is reduced, and themodel increases model, starting with the decision tree. We set the ‚Äòmax depth‚Äô its complexity, thus overfitting the data. The parameter ‚ÄòC‚Äô to ‚Äô4‚Äô in the algorithm, which indicates that the tree can split is tuned using Randomised Search CV () for the different four times, and the ‚Äòcriterion‚Äô to ‚Äòentropy,‚Äô which is similar datasets: the original, the standardised and the dataset with the to ‚Äòmax depth‚Äô but decides when to stop splitting the tree. most important features. Once the parameter ‚ÄòC‚Äô is defined We have thus finished installing and storing everything. for each dataset, the logistic regression model is initiated and then fitted to the training data, as described in the methodol- ogy. The logistic regression hypothesis function can be seen iii) K-NEAREST NEIGHBOURS (KNN) below, where the function g(z) i(s also)shown as follows: Supervised Learning is the learning that the amount or the result that we want or expect inside the training data (labelled data), and the amount in the data that we need to learn is hŒ∏ (x) = g Œ∏T x (6) known as the Target or the Dependent Variable. Next, for the K-Nearest Neighbours (KNN), we build the model using The logistic Regression for the hypothesis can be seen as the ‚ÄòK-Neighbours Classifier‚Äô model and take the value of k, follows: which represents the nearest neighbour, as ‚Äò5‚Äô. The value of 1 the ‚Äòn-neighbours‚Äô is arbitrarily selected, but it can be selected h (x :) = (7) positively through iterating a range of values, surveyed by 1+ e‚àí Œ∏Tx fitting and storing the predicted values into the ‚Äôknn-yhat‚Äô Here Œ∏ (theta) is a vector of restrictions that our model variable. calculates to appropriate to our classifier. iv) RANDOM FOREST (RF) vii) XG BOOST RF is an ensemble technique and is considered group learning The decision-tree-based ensemble ML algorithm is XG for classifying elements and regression. Deep trees are used to Boost,and it uses a framework for gradient boosting. There- learn irregular patterns. If deep trees learn the same part of the fore, when using unstructured data with prediction problems training sample, RF takes an average of its value‚Äôs variation, (text, etc.), artificial neural networks tend to outperform all which can be reduced by this method. The training data other algorithms or frameworks. The XG boost model for (p = p1. . . . . . .pn) with responses (Q = q1, . . . , qn) and bag- classification is called the XGB Classifier. It can be fit into ging (X times) choose a random sample and replace it with our training dataset. Models are fit using the sci-kit-learn API the training set that fits the trees for these samples as follows: and the model‚Äôs fit () function. Parameters for training the For x = 1. . . , X : 1 ‚àë model can be passed to the model in the constructor. Now, x we use serviceable defaults. fx(RÃá) (3) X x=1 c: APPLIED DEEP LEANING TECHNIQUES\n\nv) SUPPORT VECTOR MACHINE (SVM) We use and apply the following deep learning algorithm.\nThe SVM algorithm texts effectively. The SVM separates positive and negative instances with high margins. The SVM i) BASELINE MODEL provides better results than the na√Øve bayes in earlier studies Essentially, a baseline is a model that has a reasonable chance regarding fraud detection. A decision surface is used to split of providing acceptable results and is simple to set up, usually training points into two categories based on support vectors. rapidly experimenting with them, and implementations are Optimisation is calculaÔ£±ted as follows widely available in popular packages with low costs. : Ô£≤ ‚àën ‚àëp ‚àëp ( )Ô£ºÔ£Ω Classification on Imbalanced Data: This model deter- mines how to classify an extremely imbalanced dataset where Œ±E = argminÔ£≥‚àí Œ±j Œ±iŒ±yiy Ezj, Ezk Ô£æ (4) the number of examples in one class greatly outnumbers the j=1 k=1 k=1 examples in another. VOLUME 10, 2022 39705\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nFIGURE 3. CNN output layer. FIGURE 2. Pooling layer. ii) CONVOLUTIONAL NEURAL NETWORK (CNN) CNNs, also acknowledged as Conv-Nets, contain multiple layers and are mostly used for processing images. Object detection is widely used for image processing and classifi- cation, estimating time series and detecting differences. Layers in the CNN Model: Here are six distinct layers in the CNN model:\n\n1) Input layer\n2) Convo layer (Convo + ReLU)\n3) Pooling layer\n4) Fully connected layer (FC)\n5) SoftMax/logistic layer\n6) Output layer FIGURE 4. Application of dropout over neural network.\nInput Layer: The input layer in the CNN model incor- porates CSV data. Text data is characterised by three- SoftMax/Logistic Layer: The SoftMax or Logistic layer is dimensional matrices, which should be reshaped into one the final layer of the CNN. It is placed after the FC layer and is column. used for binary classification. Logistic is used, and SoftMax Convo Layer:The convo layer is occasionally known as the is used for multiclassification. feature extraction layer since the text features are extracted Output Layer: The output layer holds the label, which within this layer. First, a part of the text is associated with the is in the procedure of one-hot encoding. Hence, we have a Convo layer to make a convolution operation and calculate better understanding of CNN.We implement a CNN inKeras. the dot product between the approachable field and filter. Figure 3 depicts the architecture of CNN from input to output The outcome of the process is a single number of output layer. capacities. The Convo layer also holds the ReLU activation function to build all negative values to zero. iii) IMPLEMENTATION WITH KERAS Pooling Layer: The pooling layer is used to decrease the Creation of theModel:The pipeline of CNNmodel over keras spatial capacity of the input text after convolution. The layer includes conv layer, max pooling layer, dropout layer, conv can use two layers of convolution. If we put a fully connected layer, max pooling layer, dropout layer along with two fully layer after the Convo layer without first including a pooling or connected layers sequentially. Figure 4 depicts input neural max pooling layer, then it will be computationally expensive, network and output of dropout layer. which we do not want. Therefore, max pooling must be used Compile the Model: Categorical Cross-Entropy:We build to reduce the spatial volume of the input text, as shown in binary cross-entropy at prior portions and in ML. At that Figure 2. time, we used definite cross-entropy. This means that we have Fully Connected Layer (FC): A fully connected layer multi-classes. Th‚àëe equaN (tion can be se(en as f)ollows:includes weights, biases, and neurons. It attaches the neurons in one layer to the neurons in an additional layer. This layer ) CCE = ‚àí1/N y . o ( )+ y i j l g y 1‚àí =0 j j .log(1‚àí yj) is used to classify data between dissimilar categories by training. (8) These categories are: Epochs and Batch Size: We used a dataset of 20 samples,\n\n- Flattening a batch size of 2 and determined that the algorithm needed\n- Dropout to run for three epochs. Consequently, in all epochs, we use\n39706 VOLUME 10, 2022\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nfive batches (20/2 = 10). All batches are run through the algorithm; then, we have five iterations per epoch. This method is often an improvement over the sequential model. The most modification comes from the Stalk group and a few slight changes within the module of the sequential model. d: PERFORMANCE-EVALUTION MEASURES Traditional methods of estimating ML classifiers can use confusion metrics relating to the difference between the rock bottom dataset truth and the model‚Äôs prediction where TP, TN, FP, and FN denote true positive, true negative, false- positive and false negative, respectively. FIGURE 5. Class distribution of fraudulent and nonfraud transactions.\n\ni) ACCURACY Another insight about the data is that there are no null\nAccuracy is used to measure the performance in the evidence values; hence, there is no need to fill in missing values. domain recovery and processing of the data. The fraction of the results that are successfully classified can be represented B. TOP 10 ALGORITHMS IN MACHINE LEARNING FOR by equation (9) as follows: FRAUD DETECTION TP+ TN In the study [3], the top ten ML algorithms are incorporated Accuracy = (9) TP+ FP+ TN + FN for the detection of credit card frauds. The list of these algorithms is given below: ii) PRECISION 1. Linear Regression Precision is a performance assessment that measures the 2. Logistic Regression ratio of correctly identified positives and the total number of 3. Decision Tree identified positives. This can be seen as follows: 4. SVM TP 5. Na√Øve Bayes Precision = (10)\n\n## TP+ FP 6. CNN\n\n7. K-Means\niii) F-MEASURE/F1-SCORE 8. Random Forest The f-measure considers both the precision and the recall. The 9. Dimensionality Reduction Algorithms f-measure may be assumed to be the average weight of all 10. Gradient Boosting Algorithms values, which can be seen as follows: These algorithms can also encompass association analysis, 2X precision √ó Recall clustering, classification, statistical learning, and linkmining.\n\n## F = (11)\n\nprecision+ Recall This is among all the critical topics covered by ML research and development. iv) RECALL The recall is also referred to as the sensitivity, which is the 1) THE CONFUSION METRICS FOR MODELS ratio of connected instances retrieved over the total number A classification model visualisation is a confusion metric that of retrieved instances and can be seen as follows: displays howfit themodel is projected to be to the results once TP associated with the earliest ones. Frequently, the anticipated Recall = (12) TP+ FN results are deposited in a variable that is then changed into an association table. Utilizing the association table in the form of IV. RESULTS AND DISCUSSIONS a heatmap, the confusion metrics can be plotted. Even though\n\nA. DATA VISUALISATION there are numerous built-in methods to envision confusion\nThe dataset covers credit cards transactions in October metrics, we can define and visualize them based on the score 2018 by European cardholders. The dataset includes trans- to allow for better correlation. Figure 6 depicts the confusion actions that happened in two days, and it includes 492 frauds metrics of machine learning algorithms. out of 284,807 transactions. It covers onlymathematical input variables, which are the outcome of a PCA transformation. 2) THE ACCURACY OF MACHINE LEARNING ALGORITHMS Due to the issue of concealment, we cannot offer the struc- In this phase, we structure six distinct kinds of classification tures of the original dataset and the data more background models. We could use numerous other models to resolve information. The feature ‚ÄòTime‚Äô covers the seconds elapsed classification problems; however, these are the most popular between the first transaction in the dataset and each transac- models in use. Using the algorithms, all these models can tion. Figure 5 shows the class distribution of the CCF dataset be built workably provided by the sci-kit-learn package. The into a fraudulent and nonfraud transactions. results of applied ML algorithms are presented in Table 5. VOLUME 10, 2022 39707\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nFIGURE 7. The case count statistics for fraud and non-fraud transactions. FIGURE 6. Confusion metrics of machine learning algorithms. TABLE 5. The accuracy and F1-socre of machine learning algorithms. FIGURE 8. Comparative analysis of machine learning algorithms.\n\n### 3) RESULT OF THE CASE AMOUNT STATISTICS OF THE\n\n## DATASET\n\nAs shown in Figure 7, the case count statistics, the values FIGURE 9. Metrics of deep learning with epoch sizes as 35 and 14. of the ‚ÄôAmount‚Äô variable vary substantially once associated with the respite of the variables. To decrease the wide range of the values, we can standardise it bymeans of the ‚ÄòStandard- C. TOP 10 ALGORITHMS IN DEEP LEARNING FOR FRAUD Scaler‚Äô method in Python. DETECTION In [8], ten DL algorithms are identified as top algorithms d.\n\n4) THE COMPARATIVE ANALYSIS OF MACHINE LEARNING The list of these algorithms is given below:\nALGORITHMS 1. Convolutional neural networks (CNN) Figure 8 show the comparative analysis of applied ML algo- 2. Long short-term memory (LSTM) rithms for CCF using accuracy and F1 measure metrics. 3. Residual neural network (RNN) 39708 VOLUME 10, 2022\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nTABLE 6. The result of CNN model using epoch size as 35 and 14. TABLE 7. The accuracy of deep learning models using different epochs. FIGURE 10. Area under the interpolated precision-recall curve.\n\n4. Baseline (BL)\n5. Generative adversarial networks (GAN)\n6. Radial basis function network (RBFN)\n7. Multilayer perception (MLP)\n8. Self-organise map (SOM)\n9. Deep belief network (DBN)\n10. Restricted Boltzmann machine (RBM)\n11. Autoencoders is applied by varying the layers from 11 to 20 and comparing\nthe result with baseline 5-layer architecture.\n\n## 1) THE EVALUATION METRICS\n\nWe can use confusion metrics to summarise the labels of 3) THE SUMMARY OF THE CNN MODEL actual vs. predicted, wherever the X-axis is the label of the Once a model is ‚Äò‚Äòbuilt‚Äô‚Äô, the summary () method can be predicted, and the Y-axis is the label of the actual: called to show its details as shown in Table 8. However, If the model had projected the whole thing accurately, this it can be beneficial when constructing a sequential model would be a diagonal metric whose values would be away from incrementally to show the summary of themodel thus far with the main diagonal and demonstrate an incorrect prediction the current output. value of zero. In this case, the metrics display that because The total number of parameters is 119,457 and the total of the comparatively rare false-positives, it is determined that number of trainable parameters is 119,265. Finally, the num- a few legitimate transactions were flagged incorrectly. This ber of nontrainable parameters is 192. trade-off might be desirable because false negatives would permit more fraudulent transactions to go through. 4) THE SUMMARY OF THE BASELINE MODEL By using the function, we now develop and train the pre-\n\n2) THE ACCURACY OF DEEP LEARNING ALGORITHMS viously defined model. Note that the model is best suited\nTable 7 shows the training and validation accuracy of pro- to using a batch size larger than 2048; this is important for posed CNN and baseline CNN algorithms. The CNN model confirming that each batch has a decent chance of comprising VOLUME 10, 2022 39709\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nTABLE 8. The summary of CNN sequential model. TABLE 9. The summary of baseline CNN sequential model. FIGURE 11. Positive distribution of the data. a rare positive fraud example. The summary of the baseline model is presented in Table 9. The total amount of parameters is 497 and the total number of trainable parameters is 497. Finally, the total amount of nontrainable parameters is 0.\n\n## 5) DISTRIBUTION OF THE DATA\n\nIdentifying fraudulent credit card transactions is a common type of imbalanced binary classification where the focus is on the positive class (is fraud) class and negative class (is not fraud) class. Then, we compare the classification of the positive and negative instances over a rare feature. The positive and negative distributions are shown in Figure 11 and Figure 12 respectively. FIGURE 12. Negative distribution of the data.\n\n## 6) VARIATION OF EPOCHS\n\nWe train the model for 20 and 30 epochs, with and with- precision recall accuracy (prc), precisions and recall over out careful initialisation, and compare the losses. The figure 35 epochs. clearly shows that careful initialisation gives a clear advan- Table 10 presents the training and validation results of tage in regard to validation loss. Figure 13 shows the valida- baseline deep learning model using 35 and 14 epochs. tion loss using zero bias and careful bias.\n\n### 8) THE DIAGNOSIS MODEL BEHAVIOUR\n\n7) RECORD OF THE TRAINING DATASET The behaviour of aML andDLmodel can be used to diagnose\nIn this section, we construct schemes of the model‚Äôs accuracy the shape and dynamics of a learning curve and to possibly and loss on the training and validation sets. We check for recommend the best configuration changes for improving overfitting; these measurements are valuable too, as they can performance and learning. There are four learning curves: help us learn more about the overfitting and underfitting of Underfit, Overfit, Good Fit, Epoch. The learning curve is used the model. Figure 14 depicts the training and validation loss, to plot the model for training and validation accuracy and 39710 VOLUME 10, 2022\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nFIGURE 13. Validation loss using zero bias and careful bias. TABLE 10. Results of deep learning model using different epochs. training and validation loss vs. epochs. We display overfitting over the epochs, which is where validation accuracy is less than training accuracy and epochs where validation loss is greater than the training loss.\n\n### 9) RESULTS OF DL ALGORITHMS ON BALANCED DATA\n\nThe imbalanced CCF dataset is transformed into a balanced dataset by removing non fraudulent transactions from the dataset. In a real-world transaction, fraudulent and non- fraudulent classes are not balanced due to the nature of the problem. For instance, if one million transactions are per- formed in a day, only a few can be fraudulent. The convo- lutional neural network model with 14 layers architecture is applied to the balanced dataset to validate the proposed model. The model is trained over 100 epochs. The CNN 14 layers architecture obtained 94.60 and 95.80 % training and validation accuracy respectively as shown in Table 7. Figure 15 depicts the accuracy and loss of CNN model using FIGURE 14. Training and validation history of loss, precision Recall the balanced CCF dataset. Accuracy (PRC), precisions and recall (Epoch size 35). VOLUME 10, 2022 39711\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nFIGURE 15. Training and validation history of accuracy and loss of CNN model using 100 epochs. FIGURE 16. Model accuracy when epoch sizes are 20 and 50.\n\n10) PLOT TRAINING & VALIDATION ACCURACY VALUE dense layer has a ReLU activation function of (100). The\nFigure 16 depicts the training and validation accuracy of second dense has a ReLU activation function of (50). The proposed model over 20 and 50 epochs. third dense layer has a ReLU activation function of (25). Finally, we add a dense layer for classification with a sigmoid\n\n11) RESULT OF THE CNN LAYERS IMPLEMENTATION activation function. At 100 epochs, the accuracy is 96.34%.\nOur proposed sequential model has a convolutional layer with 32 filters of size 3 and a ReLU activation function, which is b: ARCHITECTURE OF 17 LAYERS followed by a batch normalisation layer and a dropout layer Our proposed model has 17 layers: a convolutional layer with with a dropout rate of 0.25. Figure 17 depicts the accuracy of a kernel size of 32 √ó 2 and a ReLU activation function, CNN model using different layers architecture. The architec- followed by a batch normalisation layer and a dropout layer tures of our proposed model are as follows. with a dropout rate of 0.2. Then, we add another convo- lutional layer with a kernel size of 64 √ó 2 and a ReLU a: ARCHITECTURE OF 14 LAYERS activation function, followed by a batch normalisation layer Our proposed model has 14 layers: a convolutional layer with and a dropout layer with a dropout rate of 0.5. Then, we add a kernel size of 32 √ó 2 and a ReLU activation function, another convolutional layer with a kernel size of 64√ó 2 and a followed by a batch normalisation layer and a dropout layer ReLU activation function, followed by a batch normalisation with a dropout rate of 0.2. Then, we add another convolutional layer and a dropout layer with a dropout rate of 0.25. layer with a kernel size of 64√ó 2 and a ReLU activation func- Then, we add a flattened layer with a kernel size of 64 √ó 2 tion, followed by a batch normalisation layer and a dropout and a ReLU activation function, followed by a dense layer and layer with a dropout rate of 0.5. Then, we add a flattened a dropout layer with a dropout rate of 0.5, followed by 3 dense layer with a kernel size of 64 √ó 2 and a ReLU activation layers. The first dense layer has a ReLU activation function of function, followed by a dense layer and a dropout layer with (100). The second dense layer has a ReLU activation function a dropout rate of 0.5, followed by 3 dense layers. The first of (50). The third dense layer has a ReLU activation function 39712 VOLUME 10, 2022\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nTABLE 11. Comparative analysis of ML and DL algorithms. a dense layer and a dropout layer with a dropout rate of 0.5, followed by 3 dense layers. The first dense layer has a ReLU activation function of (100). The second dense layer has a ReLU activation function of (50). The third dense layer has a ReLU activation function of (25). Finally, we add a dense layer for classification with a sigmoid activation function. At 100 epochs, the accuracy is 94.92%.\n\n### 12) THE COMPARATIVE ANALYSIS OF THE MACHINE\n\n### LEARNING AND DEEP LEARNING ALGORITHMS\n\nThe most important distinction between DL and standard ML is how well deep learning performs when the amount of data changes, as DL techniques do not perform well when the amount of data is minimal. This is because DL algo- rithms require a large quantity of data to fully learn features. ML algorithms are less accurate than deep learning algo- FIGURE 17. Accuracy of the CNN model over number of layers. rithms. Therefore, the existing accuracy of ML algorithms and DL algorithms is low compared to the accuracy of the proposed model. Table 10 presents a comparative analysis of of (25). Finally, we add a dense layer for classification with a ML and DL algorithms. sigmoid activation function. After 100 epochs, the accuracy is 95.53%. V. CONCLUSION AND FUTURE WORK CCF is an increasing threat to financial institutions. Fraud- c: ARCHITECTURE OF 20 LAYERS sters tend to constantly come up with new fraud methods. Our proposed model has 20 layers: a convolutional layer with A robust classifier can handle the changing nature of fraud. a kernel size of 32 √ó 2 and a ReLU activation function, Accurately predicting fraud cases and reducing false-positive followed by a batch normalisation layer and a dropout layer cases is the foremost priority of a fraud detection system. with a dropout rate of 0.2. Then, we add another convo- The performance of ML methods varies for each individual lutional layer with a kernel size of 64 √ó 2 and a ReLU business case. The type of input data is a dominant factor that activation function, followed by a batch normalisation layer drives different ML methods. For detecting CCF, the number and a dropout layer with a dropout rate of 0.5. Then, we add of features, number of transactions, and correlation between another convolutional layer with a kernel size of 64√ó 2 and a the features are essential factors in determining the model‚Äôs ReLU activation function, followed by a batch normalisation performance. DLmethods, such as CNNs and their layers, are layer and a dropout layer with a dropout rate of 0.5. associated with the processing of text and the baseline model. Then, we add another convolutional layer with a kernel Using these methods for the detection of credit cards yields size of 64√ó 2 and a ReLU activation function, followed by a better performance than traditional algorithms. Comparing batch normalisation layer and a dropout layer with a dropout all the algorithm performances side to side, the CNN with rate of 0.25. Then, we add a flattened layer with a kernel 20 layers and the baseline model is the top method with an size of 64 √ó 2 and a ReLU activation function, followed by accuracy of 99.72%. Numerous sampling techniques are used VOLUME 10, 2022 39713\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\nto increase the performance of existing examples, but they [18] N. Kousika, G. Vishali, S. Sunandhana, and M. A. Vijay, significantly decrease on the unseen data. The performance ‚Äò‚ÄòMachine learning based fraud analysis and detection system,‚Äô‚Äô\n\nJ. Phys., Conf., vol. 1916, no. 1, May 2021, Art. no. 012115,\non unseen data increased as the class imbalance increased. doi: 10.1088/1742-6596/1916/1/012115. Future work associated may explore the use of more state of [19] R. F. Lima and A. Pereira, ‚Äò‚ÄòFeature selection approaches to fraud detection art deep learning methods to improve the performance of the in e-payment systems,‚Äô‚Äô in E-Commerce and Web Technologies, vol. 278,\n\nD. Bridge and H. Stuckenschmidt, Eds. Springer, 2017, pp. 111‚Äì126, doi:\nmodel proposed in this study. 10.1007/978-3-319-53676-7_9. [20] Y. Lucas and J. Jurgovsky, ‚Äò‚ÄòCredit card fraud detection using machine REFERENCES learning: A survey,‚Äô‚Äô 2020, arXiv:2010.06479. [21] H. Zhou, H.-F. Chai, and M.-L. Qiu, ‚Äò‚ÄòFraud detection within bankcard [1] Y. Abakarim, M. Lahby, and A. Attioui, ‚Äò‚ÄòAn efficient real time model enrollment on mobile device based payment using machine learning,‚Äô‚Äô for credit card fraud detection based on deep learning,‚Äô‚Äô in Proc. 12th Frontiers Inf. Technol. Electron. Eng., vol. 19, no. 12, pp. 1537‚Äì1545, Int. Conf. Intell. Systems: Theories Appl., Oct. 2018, pp. 1‚Äì7, doi: Dec. 2018, doi: 10.1631/FITEE.1800580. 10.1145/3289402.3289530. [22] S.Makki, Z. Assaghir, Y. Taher, R. Haque,M.-S. Hacid, andH. Zeineddine, [2] H. Abdi and L. J. Williams, ‚Äò‚ÄòPrincipal component analysis,‚Äô‚Äô Wiley Inter- ‚Äò‚ÄòAn experimental study with imbalanced classification approaches for discipl. Rev., Comput. Statist., vol. 2, no. 4, pp. 433‚Äì459, Jul. 2010, doi: credit card fraud detection,‚Äô‚Äô IEEE Access, vol. 7, pp. 93010‚Äì93022, 2019, 10.1002/wics.101. doi: 10.1109/ACCESS.2019.2927266. [3] V. Arora, R. S. Leekha, K. Lee, and A. Kataria, ‚Äò‚ÄòFacilitating user [23] I. Matloob, S. A. Khan, and H. U. Rahman, ‚Äò‚ÄòSequence mining and authorization from imbalanced data logs of credit cards using artificial prediction-based healthcare fraud detection methodology,‚Äô‚Äô IEEE Access, intelligence,‚Äô‚Äô Mobile Inf. Syst., vol. 2020, pp. 1‚Äì13, Oct. 2020, doi: vol. 8, pp. 143256‚Äì143273, 2020, doi: 10.1109/ACCESS.2020.3013962. 10.1155/2020/8885269. [24] I. Mekteroviƒá, M. Karan, D. Pintar, and L. Brkiƒá, ‚Äò‚ÄòCredit card fraud [4] A. O. Balogun, S. Basri, S. J. Abdulkadir, and A. S. Hashim, ‚Äò‚ÄòPerformance detection in card-not-present transactions: Where to invest?‚Äô‚Äô Appl. Sci., analysis of feature selection methods in software defect prediction: A vol. 11, no. 15, p. 6766, Jul. 2021, doi: 10.3390/app11156766. search method approach,‚Äô‚Äô Appl. Sci., vol. 9, no. 13, p. 2764, Jul. 2019, [25] D.Molina, A. LaTorre, and F. Herrera, ‚Äò‚ÄòSHADEwith iterative local search doi: 10.3390/app9132764. for large-scale global optimization,‚Äô‚Äô in Proc. IEEE Congr. Evol. Comput. [5] B. Bandaranayake, ‚Äò‚ÄòFraud and corruption control at education system (CEC), Jul. 2018, pp. 1‚Äì8, doi: 10.1109/CEC.2018.8477755. level: A case study of the Victorian department of education and early [26] M. Muhsin, M. Kardoyo, S. Arief, A. Nurkhin, and H. Pramus- childhood development in Australia,‚Äô‚Äô J. Cases Educ. Leadership, vol. 17, into, ‚Äò‚ÄòAn analyis of student‚Äôs academic fraud behavior,‚Äô‚Äô in Proc. no. 4, pp. 34‚Äì53, Dec. 2014, doi: 10.1177/1555458914549669. Int. Conf. Learn. Innov. (ICLI), Malang, Indonesia, 2018, pp. 34‚Äì38, [6] J. B≈Çaszczy≈Ñski, A. T. de Almeida Filho, A. Matuszyk, M. SzelgÃß, and doi: 10.2991/icli-17.2018.7.\n\nR. S≈Çowi≈Ñski, ‚Äò‚ÄòAuto loan fraud detection using dominance-based rough set [27] H. Najadat, O. Altiti, A. A. Aqouleh, and M. Younes, ‚Äò‚ÄòCredit card\napproach versus machine learning methods,‚Äô‚Äô Expert Syst. Appl., vol. 163, fraud detection based on machine and deep learning,‚Äô‚Äô in Proc. 11th Jan. 2021, Art. no. 113740, doi: 10.1016/j.eswa.2020.113740. Int. Conf. Inf. Commun. Syst. (ICICS), Apr. 2020, pp. 204‚Äì208, doi: [7] B. Branco, P. Abreu, A. S. Gomes, M. S. C. Almeida, J. T. Ascens√£o, 10.1109/ICICS49469.2020.239524. and P. Bizarro, ‚Äò‚ÄòInterleaved sequence RNNs for fraud detection,‚Äô‚Äô in Proc. [28] A. Pumsirirat and L. Yan, ‚Äò‚ÄòCredit card fraud detection using deep 26th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2020, learning based on auto-encoder and restricted Boltzmann machine,‚Äô‚Äô pp. 3101‚Äì3109, doi: 10.1145/3394486.3403361. Int. J. Adv. Comput. Sci. Appl., vol. 9, no. 1, pp. 18‚Äì25, 2018, doi: [8] F. Cartella, O. Anunciacao, Y. Funabiki, D. Yamaguchi, T. Akishita, and 10.14569/IJACSA.2018.090103.\n\nO. Elshocht, ‚Äò‚ÄòAdversarial attacks for tabular data: Application to fraud [29] P. Raghavan and N. E. Gayar, ‚Äò‚ÄòFraud detection using machine\ndetection and imbalanced data,‚Äô‚Äô 2021, arXiv:2101.08030. learning and deep learning,‚Äô‚Äô in Proc. Int. Conf. Comput. Intell. [9] S. S. Lad, I. Dept. of CSERajarambapu Institute of TechnologyRa- Knowl. Economy (ICCIKE), Dec. 2019, pp. 334‚Äì339, doi: jaramnagarSangliMaharashtra, and A. C. Adamuthe, ‚Äò‚ÄòMalware clas- 10.1109/ICCIKE47802.2019.9004231. sification with improved convolutional neural network model,‚Äô‚Äô Int. [30] M. Ramzan, A. Abid, H. U. Khan, S. M. Awan, A. Ismail, M. Ahmed,\n\nJ. Comput. Netw. Inf. Secur., vol. 12, no. 6, pp. 30‚Äì43, Dec. 2021, M. Ilyas, and A. Mahmood, ‚Äò‚ÄòA review on State-of-the-Art violence detec-\ndoi: 10.5815/ijcnis.2020.06.03. tion techniques,‚Äô‚Äô IEEE Access, vol. 7, pp. 107560‚Äì107575, 2019, doi: 10.1109/ACCESS.2019.2932114. [10] V. N. Dornadula and S. Geetha, ‚Äò‚ÄòCredit card fraud detection usingmachine [31] M. Ramzan, H. U. Khan, S. M. Awan, A. Ismail, M. Ilyas, and learning algorithms,‚Äô‚ÄôProc. Comput. Sci., vol. 165, pp. 631‚Äì641, Jan. 2019, A. Mahmood, ‚Äò‚ÄòA survey on state-of-the-art drowsiness detection doi: 10.1016/j.procs.2020.01.057. techniques,‚Äô‚Äô IEEE Access, vol. 7, pp. 61904‚Äì61919, 2019, doi: [11] I. Benchaji, S. Douzi, and B. E. Ouahidi, ‚Äò‚ÄòCredit card fraud detection 10.1109/ACCESS.2019.2914373. model based on LSTM recurrent neural networks,‚Äô‚Äô J. Adv. Inf. Technol., [32] A. Rb and S. K. Kr, ‚Äò‚ÄòCredit card fraud detection using artificial neural vol. 12, no. 2, pp. 113‚Äì118, 2021, doi: 10.12720/jait.12.2.113-118. network,‚Äô‚Äô Global Transitions Proc., vol. 2, no. 1, pp. 35‚Äì41, Jun. 2021, [12] Y. Fang, Y. Zhang, and C. Huang, ‚Äò‚ÄòCredit card fraud detection based on doi: 10.1016/j.gltp.2021.01.006. machine learning,‚Äô‚Äô Comput., Mater. Continua, vol. 61, no. 1, pp. 185‚Äì195, [33] N. F. Ryman-Tubb, P. Krause, and W. Garn, ‚Äò‚ÄòHow artificial intelligence 2019, doi: 10.32604/cmc.2019.06144. and machine learning research impacts payment card fraud detection: [13] J. Forough and S. Momtazi, ‚Äò‚ÄòEnsemble of deep sequential models for A survey and industry benchmark,‚Äô‚Äô Eng. Appl. Artif. Intell., vol. 76, credit card fraud detection,‚Äô‚Äô Appl. Soft Comput., vol. 99, Feb. 2021, pp. 130‚Äì157, Nov. 2018, doi: 10.1016/j.engappai.2018.07.008. Art. no. 106883, doi: 10.1016/j.asoc.2020.106883. [34] I. Sadgali, N. Sael, and F. Benabbou, ‚Äò‚ÄòAdaptive model for credit card fraud [14] K. He, X. Zhang, S. Ren, and J. Sun, ‚Äò‚ÄòDeep residual learning for image detection,‚Äô‚Äô Int. J. Interact. Mobile Technol., vol. 14, no. 3, p. 54, Feb. 2020, recognition,‚Äô‚Äô 2015, arXiv:1512.03385. doi: 10.3991/ijim.v14i03.11763. [15] X. Hu, H. Chen, and R. Zhang, ‚Äò‚ÄòShort paper: Credit card fraud detec- [35] Y. Sahin and E. Duman, ‚Äò‚ÄòDetecting credit card fraud by ANN and logis- tion using LightGBM with asymmetric error control,‚Äô‚Äô in Proc. 2nd tic regression,‚Äô‚Äô in Proc. Int. Symp. Innov. Intell. Syst. Appl., Jun. 2011, Int. Conf. Artif. Intell. for Industries (AII), Sep. 2019, pp. 91‚Äì94, doi: pp. 315‚Äì319, doi: 10.1109/INISTA.2011.5946108. 10.1109/AI4I46381.2019.00030. [36] I. Sohony, R. Pratap, and U. Nambiar, ‚Äò‚ÄòEnsemble learning for credit card [16] J. Kim, H.-J. Kim, and H. Kim, ‚Äò‚ÄòFraud detection for job placement fraud detection,‚Äô‚Äô in Proc. ACM India Joint Int. Conf. Data Sci. Manage. using hierarchical clusters-based deep neural networks,‚Äô‚Äô Int. Data, Jan. 2018, pp. 289‚Äì294, doi: 10.1145/3152494.3156815.\n\nJ. Speech Technol., vol. 49, no. 8, pp. 2842‚Äì2861, Aug. 2019, [37] B. Stojanoviƒá, J. Bo≈æiƒá, K. Hofer-Schmitz, K. Nahrgang, A. Weber,\ndoi: 10.1007/s10489-019-01419-2. A. Badii, M. Sundaram, E. Jordan, and J. Runevic, ‚Äò‚ÄòFollow the trail: [17] M.-J. Kim and T.-S. Kim, ‚Äò‚ÄòA neural classifier with fraud density map for Machine learning for fraud detection in fintech applications,‚Äô‚Äô Sensors, effective credit card fraud detection,‚Äô‚Äô in Intelligent Data Engineering and vol. 21, no. 5, p. 1594, Feb. 2021, doi: 10.3390/s21051594. Automated Learning, vol. 2412, H. Yin, N. Allinson, R. Freeman, J. Keane, [38] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, ‚Äò‚ÄòInception-v4, and S. Hubbard, Eds. Berlin, Germany: Springer, 2002, pp. 378‚Äì383, doi: inception-ResNet and the impact of residual connections on learning,‚Äô‚Äô 10.1007/3-540-45675-9_56. 2016, arXiv:1602.07261. 39714 VOLUME 10, 2022\n\n\n\n---\n\nF. K. Alarfaj et al.: CCF Detection Using State-of-the-Art ML and DL Algorithms\n[39] H. Tingfei, C. Guangquan, and H. Kuihua, ‚Äò‚ÄòUsing variational auto HIKMAT ULLAH KHAN received the master‚Äôs encoding in credit card fraud detection,‚Äô‚Äô IEEE Access, vol. 8, and Ph.D. degrees in computer science from Inter- pp. 149841‚Äì149853, 2020, doi: 10.1109/ACCESS.2020.3015600. national Islamic University, Islamabad. He has [40] D. Varmedja, M. Karanovic, S. Sladojevic, M. Arsenovic, and A. Anderla, been an Active Researcher for the last ten years. ‚Äò‚ÄòCredit card fraud detection‚Äìmachine learning methods,‚Äô‚Äô in Proc. 18th He is currently an Assistant Professor with the Int. Symp. INFOTEH-JAHORINA (INFOTEH), Mar. 2019, pp. 1‚Äì5, doi: Department of Computer Science, COMSATS 10.1109/INFOTEH.2019.8717766. University Islamabad,Wah Cantt, Pakistan. He has [41] S. Warghade, S. Desai, and V. Patil, ‚Äò‚ÄòCredit card fraud detection from authored more than 50 papers in top peer- imbalanced dataset using machine learning algorithm,‚Äô‚Äô Int. J. Com- put. Trends Technol., vol. 68, no. 3, pp. 22‚Äì28, Mar. 2020, doi: reviewed journals and international conferences. 10.14445/22312803/IJCTT-V68I3P105. His research interests include social web mining, [42] N. Yousefi, M. Alaghband, and I. Garibay, ‚Äò‚ÄòA comprehensive survey on semantic web, data science, information retrieval, and scientometrics. He is machine learning techniques and user authentication approaches for credit an editorial board member of a number of prestigious impact factor journals. card fraud detection,‚Äô‚Äô 2019, arXiv:1912.02629. [43] X. Zhang, Y. Han, W. Xu, and Q. Wang, ‚Äò‚ÄòHOBA: A novel feature NAIF ALMUSALLAM received the B.S. degree engineering methodology for credit card fraud detection with a deep in computer science from King Faisal Univer- learning architecture,‚Äô‚Äô Inf. Sci., vol. 557, pp. 302‚Äì316, May 2021, doi: 10.1016/j.ins.2019.05.023. sity, Hofuf, Saudi Arabia, in 2009, the M.S. degree from Monash University, Melbourne, VIC, Australia, in 2013, and the Ph.D. degree in com- puter science from RMIT University, Melbourne, in 2019. He is currently an Assistant Professor with Imam Mohammad Ibn Saud Islamic Univer- sity (IMSIU), Riyadh, Saudi Arabia. His research interests include machine learning, data science, and security. FAWAZ KHALED ALARFAJ received the M.Sc. and Ph.D. degrees in computer science from Essex University, U.K. He is currently an Assistant Pro- MUHAMMAD RAMZAN is currently pursuing fessor with the Computer and Information Sci- the Ph.D. degree with the University of Manage- ences Department, Imam Muhammad Ibn Saud ment and Technology, Lahore, Pakistan. Islamic University (IMSIU). His research inter- He is also a Lecturer with the University of Sar- ests include information retrieval, natural language godha, Pakistan. He has authored several research processing, machine learning, big data, and cloud articles published in well reputed peer-reviewed computing. journals. His research interests include algorithms, machine learning, software engineering, and com- puter vision. MUZAMIL AHMED received the M.S. degree in computer science from the University of Lahore, Pakistan. He is currently pursuing the Ph.D. degree with the Department of Computer Science, IQRA MALIK is currently pursuing the master‚Äôs degree in computer science COMSATS University Islamabad, Wah Cantt, with the Department of Computer Science and Information Technology, Pakistan. His research interests include natural lan- University of Sargodha, Sargodha, Pakistan. She is also a Research Scholar guage processing, machine learning, deep learn- with the Department of Computer Science and Information Technology, ing, data science, information retrieval, and digital University of Sargodha. Her research interests include machine learning, image processing. deep learning, digital image processing, and computer vision. VOLUME 10, 2022 39715","knowledge_id":"f32fabde-8c4c-4bb6-9898-047d75dc9660","title":"Credit_Card_Fraud_Detection_Using_State-of-the-Art_Machine_Learning_and_Deep_Learning_Algorithms.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/7bcb8f6b-990a-457d-ac09-d8f48b8f226f/Credit_Card_Fraud_Detection_Using_State-of-the-Art_Machine_Learning_and_Deep_Learning_Algorithms.pdf"}
{"file_content":"TYPE Original Research PUBLISHED 08 October 2025 DOI 10.3389/frai.2025.1643292 Enhancing credit card fraud OPEN ACCESS detection using traditional and\n\n## EDITED BY\n\nArianna Agosto,  deep learning models with class University of Pavia, Italy REVIEWED BY imbalance mitigation Sabina Rossi, Ca‚Äô Foscari University of Venice, Italy Shahzad Ashraf,  Tahani¬†Albalawi 1 and Samia¬†Dardouri 1,2* Gachon University, Republic of Korea 1 *CORRESPONDENCE  Department of Computer Science, College of Computing and Information Technology, Shaqra Samia Dardouri  University, Shaqra, Saudi¬†Arabia, 2 InnoV'COM Laboratory-Sup'Com, University of Carthage, Ariana, s.dardouri@su.edu.sa Tunisia RECEIVED 11 June 2025 ACCEPTED 15 September 2025 Introduction: The growing complexity of fraudulent activities presents PUBLISHED 08 October 2025 significant challenges in detecting fraud within financial transactions. Accurate\n\n## CITATION\n\nand robust detection methods are essential for minimizing financial losses. Albalawi T and Dardouri S (2025) Enhancing credit card fraud detection using traditional Methods: This study evaluates logistic regression, decision tree, and random and deep learning models with class forest models on real-world credit card datasets, addressing class imbalance and imbalance mitigation. enhancing predictive accuracy. A deep learning model incorporating focal loss Front. Artif. Intell. 8:1643292. doi: 10.3389/frai.2025.1643292 was developed to further improve detection performance. The Synthetic Minority Over-Sampling Technique (SMOTE) was applied to mitigate class imbalance, and\n\n## COPYRIGHT\n\n¬© 2025 Albalawi and Dardouri. This is an hyperparameter tuning was conducted to optimize model configurations. open-access article distributed under the Results: Experimental results show that the random forest model achieved the terms of the Creative Commons Attribution License (CC BY). The use, distribution or best overall performance, with an accuracy of 99.95%, F1 score of 0.8256, and reproduction in other forums is permitted, ROC-AUC of 0.9759. The deep learning model provided the highest precision, provided the original author(s) and the demonstrating its potential in minimizing false positives. copyright owner(s) are credited and that the original publication in this journal is cited, in Discussion: A key novelty of this work is the integration of focal loss within the deep accordance with accepted academic learning framework, enabling the model to focus on hard-to-classify fraudulent practice. No use, distribution or reproduction transactions. Unlike many prior studies limited to the Kaggle dataset, our approach is permitted which does not comply with these terms. was validated on both the Kaggle credit card dataset and the PaySim synthetic mobile money dataset, demonstrating robustness and cross-domain generalizability. These findings highlight the effectiveness of combining data preprocessing, resampling techniques, and model optimization for robust fraud detection.\n\n## KEYWORDS\n\ncredit card fraud detection, imbalanced data, machine learning, logistic regression, decision tree, random forest, deep learning, SMOTE 1 Introduction Detecting fraudulent activities in financial transactions has become increasingly challenging due to the growing complexity and sophistication of fraud schemes (Talukder et¬†al., 2024; Dou et¬†al., 2020). The rise in both virtual and physical payment platforms has contributed to a surge in fraud cases, causing substantial financial losses to individuals and organizations. In 2022, for instance, individuals in the United¬†States reported losing over $8.8 billion to fraud‚Äîan increase of 30% from the previous year, as reported by the Federal Trade Commission (FTC) (Innan et¬†al., 2023). As a result, financial institutions and businesses are under increasing pressure to enhance the accuracy and efficiency of fraud detection systems in order to mitigate losses and protect consumers (Chy, 2024; Chen et¬† al., 2025; Chen et¬†al., 2020). Machine learning (ML) has become a critical tool for analyzing large volumes of financial transaction data to detect patterns of fraudulent behavior (Ismail and Khorsheed, 2023; Ali et¬†al., Frontiers in Artificial Intelligence 01 frontiersin.org\n\n\n\n---\n\nAlbalawi and Dardouri 10.3389/frai.2025.1643292 2022; Jha et al., 2012). Unlike traditional statistical methods, ML real-time fraud detection systems using ML algorithms in combination algorithms can uncover complex, nonlinear relationships and adapt to with resampling and feature selection. Meanwhile, recent studies have evolving fraud tactics over time (Shah and Sharma, 2023; Manorom shown how data augmentation (Khalid et¬† al., 2024), federated et¬†al., 2024). Classification models such as Logistic Regression, Decision learning, and hybrid ML-DL approaches (Btoush et¬† al., 2025; Trees, and Random Forests have shown promise in identifying hidden Bhattacharyya et¬† al., 2011) can further improve accuracy and patterns and anomalies within financial data (Hashemi et¬†al., 2023; generalizability across diverse datasets. Kumar et¬†al., 2020; Hernandez Aros et¬†al., 2024). These models have Other works, such as Manorom et¬†al. (2024) and Kumar et¬†al. become increasingly effective in real-time and large-scale fraud (2020), explored comparative analyses of various algorithms, showing detection scenarios (Borketey, 2024; Salunke et¬†al., 2025). the utility of random forest and support vector machines in high- A key challenge in fraud detection, however, is the severe class dimensional transaction data. Baisholan et¬† al. (2025), Islam et¬† al. imbalance problem, where legitimate transactions vastly outnumber (2023), and Strelcenia and Prakoonwit (2023) emphasized ensemble fraudulent ones (Sopiyan et¬† al., 2022; Kumar et¬† al., 2020). This techniques and anomaly detection strategies tailored for overlapping imbalance often results in biased models that fail to detect minority- and minority classes in credit card datasets. class instances effectively. To address this, the Synthetic Minority In this study, we¬† extend prior research by incorporating the Over-sampling Technique (SMOTE) has been widely used to generate Synthetic Minority Over-sampling Technique (SMOTE) to address the synthetic examples from the fraud class, thereby balancing the dataset significant class imbalance typically observed in credit card fraud and improving model learning (Btoush et¬† al., 2025; Baisholan datasets. We¬†perform a systematic evaluation of Logistic Regression, et¬†al., 2025). Decision Tree, and Random Forest models on a real-world transaction In this study, SMOTE is employed to enhance the performance of dataset, focusing on improving predictive accuracy and model three classification models‚ÄîLogistic Regression, Decision Tree, and robustness. To further enhance detection performance, we¬†develop a Random Forest‚Äîon a real-world credit card fraud dataset. A deep deep learning model that integrates focal loss, enabling the model to learning model using focal loss is also implemented to prioritize hard- focus on harder-to-classify fraudulent cases. Additionally, we¬†apply to-classify fraudulent transactions (Islam et¬†al., 2023; Strelcenia and hyperparameter tuning to optimize each model‚Äôs configuration, Prakoonwit, 2023). Each model is optimized using hyperparameter ensuring a fair and rigorous comparison across both traditional and tuning, and performance is evaluated using standard metrics, deep learning approaches. Beyond traditional ensemble methods and including precision, recall, F1 score, accuracy, and the ROC-AUC imbalance mitigation strategies, several recent directions in fraud curve (Khalid et¬†al., 2024; Bhattacharyya et¬†al., 2011). detection research are noteworthy. Graph neural networks (GNNs) have been increasingly applied to capture relational dependencies between entities, enabling the detection of fraud rings and collusive 2 Related works behaviors that cannot be¬†identified through transaction-level analysis alone. In parallel, federated learning frameworks have emerged as a Recent advancements in credit card fraud detection have promising avenue for privacy-preserving fraud detection, allowing extensively explored both traditional machine learning (ML) and deep multiple financial institutions to collaboratively train models without learning (DL) approaches, often incorporating techniques to mitigate sharing sensitive data. Another innovative line of research is the data imbalance. Talukder et¬†al. (2024) proposed a hybrid ensemble integration of AI with blockchain technologies, which enhances both model combining Iterative Hard Thresholding with Logistic Regression transparency and traceability of financial transactions. For instance, (IHT-LR) and grid search to improve transaction security. Similarly, Ressi et¬†al. (2024) provide a comprehensive review of AI-enhanced (Dou et¬†al., 2020) investigated the robustness of graph neural networks blockchain frameworks for fraud detection and monitoring, against camouflaged fraudsters, highlighting the value of relational highlighting their potential to improve security and auditability in modeling in fraud detection. decentralized systems. These directions represent important To address the challenges posed by imbalanced datasets, (Innan complementary approaches that future work can integrate with et¬† al., 2023) introduced quantum machine learning models and imbalance mitigation and deep learning strategies for more demonstrated their potential in financial fraud contexts. Chy (2024) comprehensive fraud detection systems. and Ismail and Khorsheed (2023) both emphasized the effectiveness of supervised learning techniques such as decision trees and logistic regression in classifying fraudulent transactions. Additionally, Ali 3 Materials and methods et¬†al. (2022) presented a comprehensive review of ML-based financial fraud detection frameworks, identifying ensemble methods as The goal of credit card fraud detection based on machine learning particularly effective. is to judge whether a credit card transaction is legal or fraudulent Literature reviews such as those by (Chen et¬† al., 2025) and accurately and quickly. In this section, we¬†analyze how to preprocess (Hernandez Aros et¬†al., 2024) provide a systematic overview of DL the input data and select Light Gradient Boosting Machine algorithm applications in fraud detection, noting that performance is strongly to establish Light GBM model. influenced by feature quality and model robustness. Furthermore, Shah and Sharma (2023) and Salunke et¬†al. (2025) demonstrated that ensemble methods, such as combining decision trees, random forests, 3.1 Dataset description and logistic regression, consistently outperform standalone models. Imbalanced learning strategies are another critical area of The dataset used in this study is the Credit Card Fraud Detection development. Hashemi et¬†al. (2023) and Borketey (2024) proposed Dataset sourced from Kaggle. Table¬†1 presents the distribution of Frontiers in Artificial Intelligence 02 frontiersin.org\n\n\n\n---\n\nAlbalawi and Dardouri 10.3389/frai.2025.1643292 fraudulent and non-fraudulent transactions in the dataset, 3.2 Methodology highlighting a significant class imbalance. It consists of 284,807 transactions, with 492 fraudulent cases, representing only 0.17% of the This study follows a systematic pipeline for credit card fraud total. The dataset is anonymized using Principal Component Analysis detection, beginning with data preprocessing and culminating in (PCA) and includes 30 features: V1 to V28, Time, and Amount. The model training and evaluation. Figure¬†2 presents the overall workflow. class label Class indicates whether a transaction is fraudulent (1) or Each step in the proposed methodology plays a critical role in not (0). enhancing the overall performance and reliability of the credit card Figure¬†1 illustrates the distribution of transaction amounts for fraud detection system. The pipeline begins with data preprocessing, fraudulent and normal transactions. The top histogram shows that where duplicate records are removed, missing values are handled, and fraudulent transactions are predominantly low in value, with the features are standardized and selected. This step ensures data quality, majority concentrated below $500 and very few exceeding $1,000. In consistency, and improved learning efficiency by eliminating noise contrast, the bottom histogram reveals that normal transactions span and irrelevant attributes. a broader range of amounts, including many high-value transactions Next, class imbalance handling is addressed using SMOTE, which up to over $25,000. This stark difference highlights the tendency of synthetically generates minority class samples to balance the dataset. fraudsters to use smaller amounts to evade detection. The use of a This prevents the model from being biased toward the majority logarithmic scale further emphasizes the rarity of high-value (non-fraudulent) class and enhances its ability to detect rare transactions in both categories. These patterns suggest that transaction fraudulent transactions. amount is a critical feature for distinguishing between fraudulent and The model training phase involves the use of three supervised legitimate activity. learning algorithms‚ÄîLogistic Regression, Decision Tree, and Random To further validate this observation, we¬†conducted a statistical Forest‚Äîeach offering distinct decision-making capabilities. This analysis of the relationship between transaction amount and fraud diversity allows for a comprehensive comparison of algorithmic occurrence. Descriptive statistics and correlation analysis were first behavior on the imbalanced dataset. employed to identify underlying patterns. Given the non-normal Finally, model evaluation through hyperparameter tuning ensures distribution of transaction amounts, the Mann Whitney U test was that each algorithm operates under optimal conditions, thereby applied to assess whether the differences in transaction amounts maximizing predictive accuracy and generalization. The integration between fraudulent and legitimate transactions were statistically of these steps results in a robust, balanced, and high-performing fraud significant. The test confirmed a significant difference (p < 0.05), detection framework. reinforcing the utility of transaction amount as a discriminative feature in fraud detection. 3.2.1 Data preprocessing Preprocessing was a critical step to ensure the quality and consistency of the input data: TABLE¬†1 Distribution of fraudulent and non-fraudulent transactions. Class Number of transaction Percentage  ‚Ä¢ Duplicate Removal: Duplicate transaction records were identified and removed, reducing the dataset from 284,807 to 283,726 Non -fraud 284,315 99.83% transactions. This ensured that the models were trained on Fraud 492 0.17% unique, independent samples.\n\n## FIGURE¬†1\n\nThe relation between fraud and amount. Frontiers in Artificial Intelligence 03 frontiersin.org\n\n\n\n---\n\nAlbalawi and Dardouri 10.3389/frai.2025.1643292\n\n- Handling Missing Values: A thorough inspection confirmed the 3.2.2 Class imbalance handling via SMOTE\nabsence of missing values, maintaining data integrity and Due to the dataset‚Äôs significant class imbalance, where simplifying preprocessing. fraudulent transactions comprised only 0.17% of the data, the\n\n- Feature Scaling: The dataset contains features with varied Synthetic Minority Over-sampling Technique (SMOTE) was\nnumerical scales (e.g., transaction amounts and PCA applied to enhance model performance. Unlike simple duplication, components). All features were standardized using the SMOTE generates synthetic samples by interpolating between StandardScaler from Scikit-learn to ensure equal treatment existing minority class instances and their k = 5 nearest neighbors, during model training, especially for distance-based models. effectively expanding the decision boundary and enabling better\n\n- Feature Selection: Correlation analysis and feature importance learning of fraud patterns. Prior to oversampling, the data was\nscores from tree-based models were used to eliminate redundant cleaned by removing duplicates and normalized using a Standard or irrelevant features, thereby reducing dimensionality and Scaler. As shown in Figure¬†3, this process resulted in a balanced improving model efficiency. training dataset containing 226,602 fraud and 226,602 non-fraud samples, achieving a 1:1 ratio. This balanced dataset significantly To address the class imbalance in the dataset, we¬† applied the improved the models‚Äô sensitivity to fraudulent transactions, Synthetic Minority Over-sampling Technique (SMOTE), which reducing bias toward the majority class and enabling a more fair generates synthetic minority class samples based on the feature-space and effective comparative analysis of model performance. similarities of nearest neighbors. Unlike random over-sampling, SMOTE avoids simple duplication and helps reduce the risk of overfitting. Its effectiveness was evaluated by comparing model performance before 3.3 Model algorithms and after resampling, with particular attention to recall and F1 score‚Äî two key metrics for assessing fraud detection performance. The The study evaluated both traditional and deep learning models: application of SMOTE led to a significant improvement in recall, indicating enhanced sensitivity to the minority (fraudulent) class.  ‚Ä¢ Logistic Regression (LR): A linear classifier suitable for binary Alternative resampling strategies such as random under-sampling, classification, optimized using L2 regularization. Tomek links, and ADASYN were initially explored. However, SMOTE  ‚Ä¢ Decision Tree (DT): A non-parametric model that splits the data achieved the best balance between improving minority class recall and into branches based on feature thresholds. maintaining model generalization across classifiers. This approach was  ‚Ä¢ Random Forest (RF): An ensemble of Decision Trees that consistently integrated into our preprocessing pipeline prior to training. improves generalization and robustness. Each classifier was evaluated on both the original and SMOTE- balanced datasets. In addition to these traditional models, a deep learning model was developed, incorporating:\n\n- Fully connected (dense) layers.\n- Batch normalization and dropout for regularization.\n- Focal loss, which down-weights easy examples and emphasizes\nharder-to-classify fraud cases.\n\n- Early stopping and learning rate reduction to enhance training\nstability and avoid overfitting.\n\n## FIGURE¬†3\n\nFIGURE¬†2 Comparison of class imbalance before and after SMOTE Proposed methodology. oversampling. Frontiers in Artificial Intelligence 04 frontiersin.org\n\n\n\n---\n\nAlbalawi and Dardouri 10.3389/frai.2025.1643292 We selected Logistic Regression, Decision Tree, and Random  ‚Ä¢ Recall: measures correctly identified positives, it is defined in Forest models due to their complementary strengths and frequent use Equation 2 as strong baselines in fraud detection studies. Logistic Regression provides a simple and interpretable baseline, Decision Trees capture non-linear feature interactions, and Random Forests offer robust Recall TP\n\n## = (2)\n\nensemble-based classification. These models combine interpretability,  TP+ FN efficiency, and reliability, making them suitable starting points for systematic evaluation. In addition to these classical models, we¬†also incorporated XGBoost, a gradient boosting algorithm widely  ‚Ä¢ F1-score: balances precision and recall, It is calculated as shown recognized for its strong predictive performance in financial fraud in Equation 3 detection. While initially included as a benchmarking model, we¬†have now systematically compared XGBoost alongside the other classifiers in the Results section to provide a more complete evaluation of F1‚àí score = 2 / ((1/Precision)+ 1/Recall ) (3\n\n## ( ) )\n\ntraditional ensemble methods. 3.4 Hyperparameter tuning and  ‚Ä¢ ROC-AUC: represents the model‚Äôs ability to distinguish between cross-validation fraudulent and non-fraudulent transactions across different classification thresholds. A higher AUC indicates better To optimize the performance of the machine learning models, discriminative performance. we¬†performed hyperparameter tuning using a grid search strategy. For each model, we¬†defined a range of relevant hyperparameters based on To further understand model behavior, a confusion matrix was prior literature and preliminary experiments. The hyperparameters used with the following components: and their search ranges for each model are illustrated in Table¬†2. This configuration was used during grid search combined with 5-fold  ‚Ä¢ True Positive (TP): Fraudulent transactions correctly predicted stratified cross-validation to identify optimal model settings. as fraud. This thorough optimization ensured fair comparison and robust  ‚Ä¢ False Positive (FP): Legitimate transactions incorrectly predicted model performance. as fraud.\n\n- True Negative (TN): Legitimate transactions correctly predicted\nas non-fraud. 4 Results and discussion  ‚Ä¢ False Negative (FN): Fraudulent transactions incorrectly predicted as non-fraud. To enhance detection accuracy and address the challenge of class imbalance, this study implements a comprehensive framework These metrics were consistently applied across all models to combining traditional machine learning models with an improved ensure fair comparison and reliable performance evaluation. deep learning architecture. This section outlines the experimental procedure, evaluation metrics, and comparative results. The dataset was preprocessed using the Synthetic Minority Over-sampling 4.2 Results Technique (SMOTE) to balance class distribution, and stratified data splits were applied for training and evaluation to preserve class Three classical machine learning models Logistic Regression, proportions. Model performance was assessed using multiple Decision Tree, and Random Forest were implemented and compared classification metrics and visualized through ROC curves and against a deep learning model composed of dense layers, batch confusion matrices. normalization, and dropout layers. Hyperparameter tuning for traditional models was performed using grid search, while the deep learning model was optimized using early stopping and learning rate 4.1 Evaluation metrics reduction strategies. All models were trained on the SMOTE-balanced training data and evaluated on a held-out test set. Performance was Accuracy alone is insufficient for evaluating fraud detection assessed using five key metrics: Accuracy, Precision, Recall, F1 Score, models due to the inherent class imbalance in the dataset. To ensure and ROC-AUC. The results highlight the effectiveness of the deep a fair and comprehensive assessment of model performance, four key learning model, particularly in identifying minority class instances, evaluation metrics were employed: and demonstrate the importance of balancing techniques and comprehensive evaluation in fraud detection tasks.\n\n- Precision: The proportion of predicted fraud cases that are To better understand the classification performance, a confusion\nactually fraudulent, calculated as, It is given by Equation 1: matrix was generated for each model, Figure¬† 4 illustrate the performance of four models; Logistic Regression, Decision Tree, Random Forest, and an Improved Deep Learning model in detecting Precision TP = (1) fraudulent transactions. Logistic Regression achieved perfect recall by TP+ FP  identifying all fraud cases but produced a high number of false Frontiers in Artificial Intelligence 05 frontiersin.org\n\n\n\n---\n\nAlbalawi and Dardouri 10.3389/frai.2025.1643292 TABLE¬†2 Hyperparameter search space for grid search tuning. score of 0.7941, highlighting its effectiveness in minimizing false Model Hyperparameters positives while maintaining high sensitivity. Logistic regression C: [0.01, 0.1, 1, 10] As shown in Table¬† 3, XGBoost achieved strong performance across all metrics, with a precision of 91.67%, recall of 95.00%, and F1 Penalty: [‚Äòl1‚Äô, ‚Äòl2‚Äô] score of 93.30, surpassing the classical baselines. This systematic Solver: [‚Äòliblinear‚Äô] inclusion of XGBoost allows a more comprehensive comparison, Decision tree Max_depth: [5, 10, 20, None] demonstrating that while Random Forest and Decision Tree models Min_samples_split: [2, 5, 10] remain competitive, gradient boosting methods such as XGBoost provide enhanced accuracy and balance in fraud detection. Min_samples_leaf: [1, 2, 4] Figure¬†5 shows the precision, recall, F1 score, and ROC-AUC Random forest N_estimators: [50, 100, 200] values for each model. The bar chart compares the performance of Max_depth: [10, 20, None] Logistic Regression, Decision Tree, and Random Forest models across Min_samples_split: [2, 5] four key metrics: Precision, Recall, F1 Score, and ROC-AUC. Logistic Regression achieved the highest recall and ROC-AUC but had the Min_samples_leaf: [1, 2] lowest precision and F1 score, indicating a high rate of false positives. XGBoost N_estimators: [50, 100, 150, 200] The Decision Tree model showed a more balanced performance but Max_depth: [3, 5, 7, 9] with moderate scores across all metrics. In contrast, the Random Learning_rate: [0.01, 0.1, 0.2] Forest model outperformed the others in overall effectiveness, Subsample: [0.6, 0.8, 1.0] achieving the highest precision, F1 score, and ROC-AUC, while maintaining strong recall. This highlights Random Forest‚Äôs robustness Deep learning (Improved) Layers: [3, 4, 5] and suitability for accurate and reliable fraud detection. Units/layer: [64, 128, 256] Figure¬†6 presents the training progress of the deep learning model Dropout: [0.2, 0.4, 0.5] over 18 epochs, displaying both accuracy and loss trends for the Learning rate: [0.001, 0.0005] training and validation sets. The left plot shows a rapid increase in accuracy, with both training and validation curves converging near Batch size: [32, 64] 100% within the first few epochs, indicating excellent generalization. The right plot illustrates a steep decline in loss during the initial positives (50), resulting in low precision. The Decision Tree model epochs, followed by stabilization at very low values for both training showed balanced performance with only one missed fraud case and and validation loss, with minimal divergence between the two. These minimal false positives. Random Forest achieved perfect recall with results demonstrate that the model achieves high accuracy, maintains fewer false positives (11), reflecting a strong balance between low loss, and exhibits no signs of overfitting, confirming effective and sensitivity and specificity. The Improved Deep Learning model robust training. delivered the best overall results, correctly identifying all fraudulent cases with the lowest number of false positives (4), indicating superior precision and a well-balanced capability for fraud detection. 4.3 Discussion Table¬† 3 presents the performance metrics of the four models evaluated after SMOTE and hyperparameter tuning. As observed, the The experimental results demonstrate the importance of both Random Forest model delivered the best overall performance, model selection and data preprocessing in the context of fraud achieving the highest F1 score (0.8256) and ROC-AUC (0.9759), detection, particularly when dealing with highly imbalanced datasets. indicating strong balance and robustness in fraud detection. The The application of SMOTE significantly improved the learning ability performance metrics for the random forest model, including accuracy, of all models by addressing class imbalance, enabling more reliable F1 score, and ROC-AUC, were calculated using predictions on the classification of minority (fraudulent) instances. held-out test dataset. Accuracy represents the proportion of correctly Among the traditional machine learning models, Random Forest classified transactions over all samples. The F1 score, the harmonic showed the most balanced and robust performance, achieving high mean of precision and recall, was used to provide a balanced measure precision, recall, F1 score, and ROC-AUC. Its ensemble nature and of the model‚Äôs performance, especially given the class imbalance ability to reduce variance contributed to its effectiveness in handling typical in fraud detection datasets. The ROC-AUC metric was the complexities of the fraud detection task. In contrast, Logistic computed by plotting the true positive rate against the false positive Regression, despite achieving perfect recall and a high ROC-AUC, rate across different classification thresholds, with the area under this suffered from a substantial number of false positives, as evidenced by curve indicating the model‚Äôs ability to distinguish between fraudulent its low precision and F1 score. This behavior reflects the model‚Äôs and legitimate transactions. These metrics were computed using tendency to overpredict the minority class, which may lead to standard implementations from the scikit-learn library to ensure operational inefficiencies in real-world fraud detection systems. robust and reproducible evaluation. The Decision Tree model achieved relatively strong results, with Logistic Regression attained the highest recall (90.32%), fewer false positives than Logistic Regression and a higher F1 score, successfully identifying most fraudulent cases, but its low precision but it was slightly outperformed by Random Forest due to the latter‚Äôs (15.73%) reflects a high rate of false positives. The Deep Learning improved generalization ability and reduced overfitting. model, enhanced with focal loss, demonstrated a well-balanced The Improved Deep Learning model, incorporating dense performance with a precision of 72.97%, recall of 87.10%, and an F1 layers, batch normalization, dropout, and focal loss, outperformed Frontiers in Artificial Intelligence 06 frontiersin.org\n\n\n\n---\n\nAlbalawi and Dardouri 10.3389/frai.2025.1643292\n\n## FIGURE¬†4\n\nConfusion matrices of the evaluated models. TABLE¬†3 Performance metrics of machine learning. Model Accuracy (%) Precision (%) Recall (%) F1 score ROC-AUC Logistic regression 99.92 24.24 100.0 39.02 99.87 Decision tree 99.71 88.24 93.75 90.91 96.85 Random forest 99.69 59.26 100.0 74.42 99.97 XGBoost 99.93 91.67 95.00 99.30 99.98 Deep learning (Improved) 99.89 80.0 100.0 88.89 100.0 all classical models. It achieved perfect recall and the highest 4.3.1 Real-time application feasibility and precision, resulting in the best F1 score and ROC-AUC. This computational cost confirms the effectiveness of the model architecture and training To evaluate the suitability of the proposed models for real-time or strategies including early stopping and learning rate reduction in clinical deployment, we¬†analyzed their inference time (i.e., time taken achieving high classification accuracy while minimizing overfitting. to make a prediction on a single input) and overall computational The use of focal loss further enhanced the model‚Äôs capability to complexity. Experiments were conducted on a system equipped with focus on hard-to-classify fraudulent cases, contributing to its Intel i7 CPU, 16GB RAM, NVIDIA RTX 3060 GPU. superior performance. Furthermore, the confusion matrices and training curves support  ‚Ä¢ Logistic Regression and Decision Tree demonstrated extremely these findings. The deep learning model not only achieved the lowest low inference times (<1 ms), making them ideal for real-time number of false positives but also demonstrated stable and consistent decision-making, especially in resource-limited environments. learning across epochs, with validation loss closely tracking training  ‚Ä¢ Random Forest and XGBoost required slightly more computation loss and accuracy quickly converging to near-perfect values. These due to ensemble structures, with inference times ranging from results emphasize the advantage of deep learning models in capturing 3‚Äì10 ms, but remain suitable for near real-time applications. complex patterns in transactional data and maintaining both high  ‚Ä¢ The Deep Learning (Improved) model, while achieving superior sensitivity and specificity. To further validate the robustness and accuracy, had a relatively higher inference time (e.g., ~25 ms per generalizability of the proposed models, we¬† conducted additional sample) and required GPU acceleration for optimal performance. experiments using the PaySim synthetic mobile money dataset, a widely recognized benchmark in fraud detection research. The same A summary of average inference times is provided in Table¬†4. preprocessing procedures, model architectures, training The higher computational demand, the deep learning model configurations, and evaluation metrics were applied as with the remains feasible for real-time use in settings equipped with adequate original dataset. The results demonstrate that the proposed traditional hardware. For deployment on edge devices or mobile platforms, and deep learning models, particularly those incorporating SMOTE lighter models may be¬†more appropriate, depending on the trade-off and focal loss, consistently maintain high performance across datasets. between speed and predictive accuracy. This confirms the adaptability of our approach and reinforces its To assess whether the observed differences in performance potential for deployment in diverse real-world financial environments. metrics among the models are statistically significant, we¬†conducted In conclusion, while classical models like Random Forest remain pairwise two-tailed t-tests across 10 independent runs for each strong candidates for fraud detection tasks due to their interpretability model. The tests were performed on accuracy, precision, recall, and reliable performance, the proposed deep learning model offers the F1-score, and ROC-AUC. A significance threshold of p < 0.05 best overall balance between recall and precision. This makes it highly was used. suitable for real-world deployment where minimizing both false The results, summarized in Table¬†5 indicate that the proposed negatives and false positives is crucial. model consistently and significantly outperforms the baseline models. Frontiers in Artificial Intelligence 07 frontiersin.org\n\n\n\n---\n\nAlbalawi and Dardouri 10.3389/frai.2025.1643292\n\n## FIGURE¬†5\n\nBar chart comparing the performance metrics.\n\n## FIGURE¬†6\n\nTraining and validation accuracy and loss curves. TABLE¬†4 Inference time and real-time suitability. Model Inference time (ms/sample) Hardware used Real-time suitability Logistic regression < 1 CPU Excellent Decision tree < 1 CPU Excellent Random forest ~3‚Äì5 CPU Good XGBoost ~5‚Äì10 CPU Good Deep learning (Improved) ~25 GPU (RTX 3060) Acceptable (GPU) The p-values confirm that the performance gains are not due to A distinctive contribution of this study is the integration of random chance but reflect meaningful improvements. focal loss within the deep learning framework for credit card These findings reinforce the robustness and generalizability of our fraud detection, which remains relatively unexplored in the proposed approach. literature compared to classical resampling and ensemble Frontiers in Artificial Intelligence 08 frontiersin.org\n\n\n\n---\n\nAlbalawi and Dardouri 10.3389/frai.2025.1643292 TABLE¬†5 p-values for pairwise statistical comparisons between the proposed model and baseline models (Two-tailed t-tests, n = 10 runs). Comparison Accuracy (p-value) Precision Recall F1-score AUC (p-value) (p-value) (p-value) (p-value) Proposed vs. Logistic regression 0.012 0.001 0.015 0.002 0.008 Proposed vs. Decision tree 0.018 0.004 0.009 0.006 0.011 Proposed vs. Random forest 0.021 0.003 0.017 0.005 0.014 Proposed vs. XGBoost 0.045 0.010 0.038 0.012 0.030 techniques. By emphasizing harder-to-classify fraudulent cases, will remain essential to ensure robustness, adaptability, and focal loss substantially improves the model‚Äôs ability to balance generalizability to diverse fraud detection scenarios precision and recall. Additionally, our evaluation across two different datasets the widely used Kaggle dataset and the PaySim synthetic dataset demonstrates that the proposed models maintain Data availability statement strong performance in both in-domain and cross-domain settings. This dual validation distinguishes our work from prior studies Publicly available datasets were analyzed in this study. that typically restrict analysis to a single dataset, thereby This data can be¬†found at: https://www.kaggle.com/datasets/mlg-ulb/ reinforcing the robustness, adaptability, and practical relevance of creditcardfraud. our approach. Author contributions 5 Conclusion and future work TA: Formal analysis, Resources, Funding acquisition, Methodology, This study investigated the effectiveness of various machine Writing¬† ‚Äì original draft, Data curation, Conceptualization. SD: learning approaches; Logistic Regression, Decision Tree, Random Validation, Formal analysis, Supervision, Writing¬†‚Äì review & editing, Forest, and an Enhanced Deep Learning model for the detection Project administration, Investigation, Visualization. of fraudulent credit card transactions. To address the severe class imbalance inherent in the dataset, the Synthetic Minority Over- sampling Technique (SMOTE) was employed, resulting in Funding significant performance improvements across all models. Among the traditional models, Random Forest achieved the highest The author(s) declare that no financial support was received for overall performance with an accuracy of 99.95%, an F1 score of the research and/or publication of this article. 0.8256, and a ROC-AUC of 0.9759. The Deep Learning model, enhanced with focal loss and regularization techniques, demonstrated the highest precision and a competitive F1 score, Acknowledgments indicating its ability to reduce false positives while maintaining high recall. We would like to thank the Deanship of Scientific Research at These results affirm that combining advanced sampling methods Shaqra University for supporting this work. like SMOTE with both classical and deep learning models substantially improves fraud detection accuracy and reliability. Moreover, the enhanced deep learning model‚Äôs stability during training and strong Conflict of interest generalization performance underscores its suitability for complex fraud detection tasks. The authors declare that the research was conducted in the Future work should focus on expanding detection capabilities absence of any commercial or financial relationships that could beyond isolated transactions to uncover fraud rings, which involve be¬†construed as a potential conflict of interest. coordinated fraudulent activities across multiple accounts. Graph- based learning methods, particularly graph neural networks (GNNs), offer strong potential for capturing such relational Generative AI statement dependencies. Furthermore, the development of federated learning frameworks can enable collaborative fraud detection across The authors declare that no Gen AI was used in the creation of institutions while preserving data privacy, a critical requirement in this manuscript. financial applications. Another promising direction is the Any alternative text (alt text) provided alongside figures in this integration of AI with blockchain technologies to enhance article has been generated by Frontiers with the support of artificial transparency, traceability, and auditability of financial transactions, intelligence and reasonable efforts have been made to ensure accuracy, as highlighted in recent reviews (e.g., Ressi et¬†al., 2024). Finally, including review by the authors wherever possible. If you¬†identify any validating the proposed models across multiple benchmark datasets issues, please contact us. Frontiers in Artificial Intelligence 09 frontiersin.org\n\n\n\n---\n\nAlbalawi and Dardouri 10.3389/frai.2025.1643292 Publisher‚Äôs note organizations, or those of the publisher, the editors and the reviewers. Any product that may be¬†evaluated in this article, or All claims expressed in this article are solely those of the claim that may be¬†made by its manufacturer, is not guaranteed or authors and do not necessarily represent those of their affiliated endorsed by the publisher. References Ali, A., Abd Razak, S., Othman, S. H., Eisa, T. A. E., Al-Dhaqm, A., Nasser, M., et al. Islam, M. A., Uddin, M. A., Aryal, S., and Stea, G. (2023). An ensemble learning (2022). Financial fraud detection based on machine learning: a systematic literature approach for anomaly detection in credit card data with imbalanced and overlapped review. Appl. Sci. 12:9637. doi: 10.3390/app12199637 classes. J. Inf. Secur. Appl. 78:103618. doi: 10.1016/j.jisa.2023.103618 Baisholan, N., Dietz, J. E., Gnatyuk, S., Turdalyuly, M., Matson, E. T., and Ismail, R. R., and Khorsheed, F. H. (2023). Classification of credit card frauds Baisholanova, K. (2025). FraudX AI: An interpretable machine learning framework for detection using machine learning techniques. JEECOM 5, 153‚Äì160. doi: credit card fraud detection on imbalanced datasets. Computers. 14:120. doi: 10.3390/ 10.33650/jeecom.v5i2.6602 computers14040120 Jha, S., Guillen, M., and Westland, J. C. (2012). Employing transaction aggregation Bhattacharyya, S., Jha, S., Tharakunnel, K., and Westland, J. C. (2011). Data mining strategy to detect credit card fraud. Expert Syst. Appl. 39, 12650‚Äì12657. doi: for credit card fraud: a comparative study. Decis. Support. Syst. 50, 602‚Äì613. doi: 10.1016/j.eswa.2012.05.034 10.1016/j.dss.2010.08.008 Khalid, A. R., Owoh, N., Uthmani, O., Ashawa, M., Osamor, J., and Adejoh, J. (2024). Borketey, B. (2024). Real-time fraud detection using machine learning. J. Data Anal. Enhancing credit card fraud detection: an ensemble machine learning approach. Big Inf. Process. 12, 189‚Äì209. doi: 10.4236/jdaip.2024.122011 Data Cogn. Comput. 8:6. doi: 10.3390/bdcc8010006 Btoush, E., Zhou, X., Gururajan, R., Chan, K. C., and Alsodi, O. (2025). Achieving Kumar, Y., Saini, S., and Payal, R. (2020). Comparative analysis for fraud detection excellence in cyber fraud detection: a hybrid ML+DL ensemble approach for credit using logistic regression, random forest and support vector machine. (October 18, 2020). cards. Appl. Sci. 15:1081. doi: 10.3390/app15031081 doi: 10.2139/ssrn.3751339 Chen, C., Li, X., and Li, P. (2020). A credit card fraud detection model based on Manorom, P., Detthamrong, U., and Chansanam, W. (2024). Comparative assessment random forest. J. Phys. Conf. Ser. 1607:012038. doi: 10.1088/1742-6596/1607/ of fraudulent financial transactions using the machine learning algorithms decision tree, 1/012038 logistic regression, na√Øve bayes, k-nearest neighbor, and random forest. Eng. Technol. Chen, Y., Zhao, C., Xu, Y., and Nie, C. (2025). Year-over-year developments in Appl. Sci. Res. 14, 15676‚Äì15680. doi: 10.48084/etasr.7774 financial fraud detection via deep learning: a systematic literature review. arXiv preprint Ressi, D., Romanello, R., Piazza, C., and Rossi, S. (2024). Ai-enhanced blockchain arXiv:2502.00201. technology: a review of advancements and opportunities. J. Netw. Comput. Appl. Chy, M. K. H. (2024). Proactive fraud defense: machine learning‚Äôs evolving role in 225:103858. doi: 10.1016/j.jnca.2024.103858 protecting against online fraud. World J. Adv. Res. Rev. 23, 1580‚Äì1589. doi: 10.30574/ Salunke, Y., Phalke, S., Madavi, M., Kumre, P., and Bobhate, G. (2025). Fraud detection: a wjarr.2024.23.3.2811 hybrid approach with logistic regression, decision tree, and random forest. Cureus J. Comp. Sci. Dou, Y., Liu, Z., Sun, L., Deng, Y., Peng, H., and Yu, P. S. (2020). ‚ÄúEnhancing graph 2:es44389-024-02350-5. doi: 10.7759/s44389-024-02350-5 neural network-based fraud detectors against camouflaged fraudsters‚Äù in Proceedings Shah, D., and Sharma, L. K. (2023). Credit card fraud detection using decision tree of the 29th ACM international conference on information and knowledge management and random forest. ITM Web Conf. 53:02012. doi: 10.1051/itmconf/20235302012 (CIKM‚Äô20), Ireland. New York, NY, USA: ACM. Sopiyan, M., Fauziah, F., and Wijaya, Y. F. (2022). Fraud detection using random forest Hashemi, S. K., Mirtaheri, S. L., and Greco, S. (2023). Fraud detection in banking data classifier, logistic regression, and gradient boosting classifier algorithms on credit cards. by machine learning techniques. IEEE Access 1. doi: 10.1109/ACCESS.2022.3232287 JUITA 10, 77‚Äì87. doi: 10.30595/juita.v10i1.12050 Hernandez Aros, L., Bustamante Molano, L. X., Gutierrez-Portela, F., Moreno Strelcenia, E., and Prakoonwit, S. (2023). Improving classification performance in Hernandez, J. J., and Rodr√≠guez Barrero, M. S. (2024). Financial fraud detection through credit card fraud detection by using new data augmentation. AI 4, 172‚Äì198. doi: the application of machine learning techniques: a literature review. Humanit. Soc. Sci. 10.3390/ai4010008 Commun. 11:1130. doi: 10.1057/s41599-024-03606-0 Talukder, M. A., Hossen, R., Uddin, M. A., Uddin, M. N., and Acharjee, U. K. (2024). Innan, N., Khan, M. A.-Z., and Bennai, M. (2023). Financial fraud detection: a Securing transactions: a hybrid dependable ensemble machine learning model using comparative study of quantum machine learning models. Int. J. Quantum Inf. 22:2350044. IHT-LR and grid search. Cybersecurity 7:32. doi: 10.1186/s42400-024-00221-z Frontiers in Artificial Intelligence 10 frontiersin.org","knowledge_id":"37e2cbb9-4238-4a22-a396-92f5a0ad0511","title":"frai-8-1643292.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/e35f9f83-922e-47d9-b2fb-d9e0aad1a6fa/frai-8-1643292.pdf"}
{"file_content":"SSRG International Journal of Electronics and Communication Engineering                                     Volume 11 Issue 8, 244-259, August 2024 ISSN: 2348-8549/ https://doi.org/10.14445/23488549/IJECE-V11I8P124                                                     ¬© 2024 Seventh Sense Research Group¬Æ\n\nOriginal Article A Machine Learning Based Approach for the Fraud\n\n### Detection in Imbalanced Credit Card Transaction Dataset\n\n\nRinku1, Ashutosh Kumar Dubey1*, Sushil Kumar Narang2, Neha Kishore3\n\n1Chitkara University School of Engineering and Technology, Chitkara University, Himachal Pradesh, India. 2Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India. 3University Institute of Engineering and Technology, Maharaja Agrasen University, Himachal Pradesh, India.\n\n*Corresponding author: ashutosh.dubey@chitkara.edu.in Received: 18 June 2024                     Revised: 30 July 2024                     Accepted: 15 August 2024                       Published: 31 August 2024\n\nAbstract - In this study, a comprehensive evaluation of machine learning models was conducted to detect fraudulent transactions in a highly imbalanced credit card dataset. An ensemble of algorithms was utilized, including Logistic Regression (LR), k- Nearest Neighbors (kNN), Support Vector Machines (SVM), Decision Tree (DT), Random Forest (RF), AdaBoost, Gradient Boosting (GB), Multi-Layer Perceptron (MLP), and Gaussian Na√Øve Bayes (GNB), each chosen to address the distinct challenges posed by the dataset's skew. Preprocessing techniques, such as Synthetic Minority Over-Sampling Technique (SMOTE) and Adaptive Synthetic (ADASYN) sampling methods, were implemented to correct class imbalances, followed by feature selection through Linear Discriminant Analysis (LDA) to enhance model training efficacy. The experimental results showcased that the ensemble methods, particularly RF, outperform, offering high accuracy and specificity, evidenced by an accuracy rate of 0.9995 using ADASYN in an 80:20 training-test split. These methods effectively handled the imbalanced nature of the dataset while maintaining high levels of predictive reliability. This study demonstrates the efficacy of ensemble machine learning approaches in detecting fraud in datasets characterized by class imbalance. The strategic application of oversampling techniques, coupled with ensemble models, provides a robust framework for identifying fraudulent activities, thereby significantly reducing the risk associated with such transactions. Keywords - Fraud detection, Class imbalance, Ensemble learning, Oversampling techniques, Machine learning algorithms.\n\n1. Introduction  the effectiveness of a range of machine learning models, from\nIn the rapidly evolving domain of financial transactions, traditional algorithms to ensemble approaches, in accurately the detection and classification of fraudulent activities have detecting fraudulent transactions. become paramount due to the worldwide surge in credit card fraud [1-4]. This surge has necessitated the development of The contributions of this work are multifaceted. Firstly, it advanced analytical methodologies capable of accurately provides a comprehensive evaluation of the performance of identifying fraudulent transactions within vast datasets [5-9]. different machine intelligent approaches in the context of However, a major challenge in this endeavor arises from the imbalanced fraud detection datasets, including Logistic imbalanced nature of transactional data, where instances of Regression (LR), k-Nearest Neighbors (kNN), Support Vector fraud are significant [10-13]. This imbalance complicates the Machines (SVM), Decision Tree (DT), ensemble methods task of fraud detection, leading to high rates of false negatives such as Random Forest (RF), AdaBoost, Gradient Boosting for fraudulent transactions [14-18]. (GB), neural networks (Multi-Layer Perceptron (MLP)) and Gaussian Naive Bayes (GNB). Each algorithm is assessed for The motivation behind this work is to address these its robustness, accuracy, and efficiency in handling challenges by exploring and enhancing the ability of machine imbalanced data, offering valuable insights into its suitability learning algorithms to detect and classify fraudulent activities for fraud detection tasks. Secondly, preprocessing and within highly imbalanced credit card transaction datasets. The resampling strategies were considered to improve the primary objectives of this study are threefold: (1) to assess the performance of imbalanced datasets. These strategies include impact of dataset imbalance on the performance of various the application of oversampling methods such as the Adaptive machine learning algorithms, (2) to explore and implement Synthetic (ADASYN) sampling and Synthetic Minority Over- advanced preprocessing and resampling techniques aimed at Sampling Technique (SMOTE), as well as neural network mitigating the effects of data imbalance, and (3) to evaluate designs tailored to address class imbalance. By implementing This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\nthese methodologies, the research aims to enhance the models' coded by Domashova and Kripak [26]. An autocoder, a sensitivity to fraudulent transactions without compromising specialized neural network design, and anomaly detection their ability to classify legitimate activities [19-22] correctly. techniques were then utilized to construct a training sample. Finally, the work presented a framework for fraud detection This sample underwent additional processing with specific by combining traditional machine learning models with techniques to correct the class imbalance. As a result, six ensemble learning techniques. This hybrid approach leverages datasets were produced, and seven different classifiers were the strengths of each model type, from the interpretability of trained on each dataset. Upon comparison of the models, the LR and DTs to the predictive power of ensemble methods and Tomek links method and the XGBoost technique were the pattern recognition capabilities of neural networks, identified as having the highest classification quality for bank through a series of experiments conducted on a widely used transactions. In 2022, Kou et al. [27] introduced three credit card dataset. This study demonstrated the effectiveness advanced imbalanced learning techniques. These of this integrated approach in detecting fraudulent activities contributions include resampling based on the model based on within highly imbalanced datasets.  cluster size and the Hybrid Imbalanced Learning Framework (HILF), which integrates several resampling techniques to The remainder of this paper is organized as follows: enhance performance. The proposed HILF and cluster size- Section 2 investigates and explores related work. Materials distance based models have been shown to outperform and methods are discussed and expanded in Section 3. Results significantly. Lee and Seo [28] explored the use of active are explored in Section 4. Section 5 discusses and analyzes learning to improve binary classification performance on these results and concludes in Section 6. imbalanced datasets. They developed a pre-selective method for faster processing and implemented active downsampling\n\n2. Literature Review  to reduce generalization errors. For datasets with severe\nRelated work based on machine intelligence algorithms skewness, adjusting the logistic regression's tuning parameter in relation to credit card fraud classification has been after each iteration proved successful. Their experiments on discussed in this section. Recently, the application of machine real-world and simulated datasets demonstrated superior intelligence algorithms in detecting credit card fraud has seen performance over traditional resampling techniques. In 2022, a marked rise in popularity. This increase can be attributed to Temraz et al. [29] introduced Counterfactual Augmentation the algorithms' remarkable ability to detect complex patterns (CFA) to tackle the issue of class imbalance in binary within large datasets. classification problems. CFA generates synthetic counterfactuals for the minority class using a case-based In 2022, Chao et al. [23] identified two main challenges reasoning approach. This approach sets CFA apart from in imbalanced data categorization: algorithms' performance is conventional methods, which often depend on extrapolation or significantly affected by the unique characteristics of interpolation techniques. A neural network-based model was unbalanced data, and their robustness varies with imbalance suggested by Li et al. [30]. It has been used to generate data ratios. Cost-sensitive algorithms notably dropped in for managing credit risk based on distribution suitability. For effectiveness from 94% to 74% as the imbalance increased. this, they have arranged the data points from the credit class They proposed using stochastic data envelopment analysis using a distance-based metric process. It is useful in catering (DEA), combining statistical modeling and sampling, to to classes based on risk or not risk. A major drawback of the assess classifier efficiency and performance accurately. In proposed model, however, was that the samples collected for 2022, Thejas et al. [24] conducted an extensive analysis using the Nystr√∂m method's economic significance could not be a broad spectrum of real-time data across different domains. adequately explained. Different research papers were They employed several evaluation methods to compare reviewed by Cherif et al. [31] based on intelligent models with diverse oversampling strategies. Their findings technologies, including big data and deep learning in terms of showed a significant improvement in accuracy over other data security. Based on the detailed analysis, they discussed methods, with a particular emphasis on the area under the the major factors that influence methodological adoptions, as curve (AUC) score. It is important to mention that their use of well as their advantages and disadvantages. In 2023, the Kalman filter approach, despite its cubic computational Karunachandra et al. [32] utilized various machine learning complexity, led to reduced performance on larger datasets due algorithms to identify fraudulent compensation activities to increased processing time. Hilal et al. [25], in 2022, aimed among online merchants. Among the techniques tested, the to provide researchers with an understanding of the model's kNN approach demonstrated superior performance, achieving objectives, benefits, and limitations. Recent research has an accuracy rate of 83.82%. In comparison, CNN and LSTM shown a trend towards unsupervised and semi-supervised networks yielded lower accuracies of 49.39% and 51.13%, models, such as Convolutional Neural Networks (CNNs), respectively. In 2023, Gupta et al. [33] identified XGBoost as Long Short-Term Memory (LSTM) networks, and clustering yielding the highest precision, F1-Score, and accuracy algorithms, which have been successful in identifying compared to other classifiers evaluated. By applying three fraudulent credit card transactions. During preprocessing, different data balancing techniques to the chosen models‚Äî categorical variables were collected from the raw data and DT, CNN, and LR‚Äîthey aimed to enhance the overall 245\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\nperformance of classifiers. Among the techniques, random and oppositional cat swarm optimization further push the over sampling was found to be the most effective for the boundaries of fraud detection. Overall, these approaches show selected algorithm, outperforming the SMOTE method and significant promise in improving fraud detection accuracy and random under-sampling. The XGBoost Classifier, when handling imbalanced datasets. combined with random over-sampling, achieved the highest scores for accuracy and other performance metrics. Afriyie et 3. Materials and Methods al. [34] used LR, DT, and RF to detect fraudulent online credit The experiments in this study were conducted using a card transactions. They balanced the dataset using an under- widely recognized credit card dataset that includes sampling strategy to prevent bias towards the majority class transactions made by European cardholders in September and reduce the risk of overfitting. The RF model emerged as 2013 [19, 20]. This dataset contains 492 fraudulent the most effective, with an AUC value of 98.9% and an transactions out of a total of 284,807 transactions recorded accuracy value of 96.0%, proving to be the best-suited model over two days. Since fraudulent transactions constitute only for predicting fraudulent transactions. In 2023, Noviandy et al. 0.172% of the total, the dataset is highly imbalanced [19, 20]. [35] utilized the XGBoost algorithm and data augmentation It is available on the Kaggle repository. In this dataset, a target for credit card fraud detection, demonstrating improved class value of 1 indicates fraud, while 0 indicates non-fraud. accuracy and addressing imbalanced datasets. Their method incorporates SMOTEENN and historical data, enhanced The selection of algorithms for fraud detection and precision, and recall. This approach benefits financial classification using the credit card dataset employs a diverse management by boosting integrity and customer trust. array of methods to tackle the challenge posed by its Alraddadi [36] studied online payment preferences, significant imbalance. The selected algorithms comprise four highlighting credit/debit card fraud risks. A DT Algorithm- machine learning models: LR, kNN, SVM, and DT. based model for fraud detection and prevention was proposed. Additionally, ensemble methods such as RF, AdaBoost, and Surveying 102 international students revealed that 95.9% GB have been chosen for their robustness and accuracy. The understood fraud mechanics, and 81.6% would use the model study also incorporates MLP and a probabilistic approach, to combat fraud. In 2023, Prabhakaran and Nedunchelian [37] GNB, to provide a comprehensive analysis of the dataset. introduced a model for fraud detection based on deep learning Given the context provided, the choice of algorithms for fraud and cat swarm optimization. Optimization has been used for detection in the credit card dataset employs a strategic feature selection. For the fraud classification, they have used approach to address the significant imbalance within the a recurrent unit based on a chaotic krill herd algorithm. This dataset. Machine learning models like LR, kNN, SVM, and approach, validated by extensive simulations, demonstrates DT provide a solid foundation, each with unique strengths in superior performance over existing methods. Ileberi et al. [38] handling classification problems. Ensemble methods, proposed a credit card fraud detection engine leveraging including RF, AdaBoost, and GB, are chosen for their machine learning and genetic algorithms for feature selection. enhanced accuracy and ability to reduce overfitting, making They employed DT, RF, LR, ANN, and NB classifiers. Tested them particularly effective against the dataset's imbalance. on a European cardholder dataset, it outperformed existing Neural networks, specifically MLP, offer advanced pattern systems. Leevy et al. [39] applied the CatBoost algorithm for recognition capabilities essential for detecting complex fraud detection and classification, considering different fraudulent behaviors. GNB adds a probabilistic approach, performance metrics, including AUC. Ahmad et al. [40] which is beneficial for its efficiency with high-dimensional proposed a framework for the grouping of fraud and normal data. This multifaceted selection ensures a thorough analysis, instances based on a fuzzy C-means algorithm. Their maximizing the chances of accurately identifying fraud amidst algorithm is found to be efficient in grouping these instances. the dataset's challenges. In 2023, Abd et al. [41] proposed a framework to address credit card fraud detection, focusing on resolving the Initially, the process starts with data collection, where the imbalanced dataset issue through hybrid sampling and credit card dataset is considered to serve as the foundation for oversampling techniques. This approach significantly analysis. Following the dataset selection, preprocessing is improved fraud detection, achieving 99.9% accuracy undertaken to ensure the data is in an optimal state for compared to existing algorithms. analysis. This stage involves handling any missing values present within the dataset, encoding categorical variables to Recent studies address imbalanced data classification and numerical ones if they exist, and addressing the issue of class credit card fraud detection, utilizing a variety of methods, imbalance, which is prevalent in fraud detection datasets due including advanced sampling techniques, machine learning to the rarity of fraudulent transactions compared to legitimate algorithms, and models like cluster-based resampling and ones. SMOTE and ADASYN were used to address the class active learning. Key findings highlight the effectiveness of imbalance problem. These methods are designed to balance hybrid sampling, stochastic DEA, and algorithms like class distribution. SMOTE works by creating synthetic XGBoost and RF in enhancing accuracy and model examples rather than simply duplicating minority class performance. Innovations like counterfactual augmentation instances. ADASYN builds on the concept of SMOTE with an 246\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\nadded strategy to adaptively generate minority data samples. GNB. Each of these models offers a unique approach to Once the data is pre-processed, the next step is feature classification and is evaluated to determine its effectiveness in selection or reduction. Here, Linear Discriminant Analysis detecting fraudulent transactions. The final step in the (LDA) was applied for dimensionality reduction, aiming to algorithmic approach is the prediction phase, where the simplify the dataset while retaining the most relevant trained models, possibly enhanced through ensemble learning, information for detecting fraud. With a refined set of features, are used to predict and identify fraudulent transactions within the algorithm proceeds to the model training phase. This phase the credit card dataset. This comprehensive approach, from is extensive and involves training multiple models to explore data collection to prediction, aims to effectively detect various methodologies for fraud detection. The models fraudulent activities, thereby minimizing the risks associated include LR, kNN, SVM, DT, RF, AdaBoost, GB, MLP, and with credit card fraud (Figure 1).\n\n\n### Preprocessing (Normalize the\n\n\nnumerical features and Feature selection (Apply LDA for preserving Load the dataset address the class imbalance using SMOTE and class separability)\n\n## ADASYN))\n\n\n\n\n\nEnsemble learning to Evaluation, Prediction and combine the prediction of Split the dataset Post-analysis\n\n### multiple model\n\n\n\nHyperparameters (Regularization Model training Performance parameter, Kernel, Degree, Depth, Split, (LR, kNN, SVM, DT, RF, evaluation Learning rate, Hidden layer, Activation AdaBoost, GB, MLP, and\n\n### function, Shrinkage) GNB)\n\n\n### Fig. 1 Working mechanism of the complete work\n\n\n3.1. Logistic Regression classification and regression. In such a scenario, each data LR is utilized for binary classification tasks. It is a point in the dataset is characterized by a set of features classification algorithm. It models the relationship between (attributes) that describe it, along with a corresponding class independent variables and the likelihood of an event label that denotes its category or group. These features are occurring. This is achieved through the logistic function, as used to represent the data point in a multidimensional feature depicted in Equations 1 and 2: space, where the dimensions correspond to the attributes. Each 1 S(z)= data point is defined by its features (attributes) and an\n\n## 1+ùëí‚àíùëß   (1)\n\n### associated class label (Equation 3).\n\n\n\nHere, z shows the following. z=ùõΩ0 + ùõΩ1ùë•1 + ùõΩ2ùë•2 + ùõΩ3ùë•3 + ‚ãØ + ùõΩùëõùë•ùëõ  (2)                                 ùê∑ = {(ùë•1, ùë¶1), (ùë•2, ùë¶2), ‚Ä¶ , (ùë•ùëõ,ùë¶ùëõ)}     (3)\n\nWhere Where ùë•ùëñ represents the feature of i-th data point, and ùë¶ùëñ\n\n- S(z) is the output of the logistic function is its corresponding label.\n\n- e is the base of the natural algorithm\nkNN determines the similarity between data points using\n\n- ùõΩ0 is bias or intercept term\na distance metric (Equation 4).\n\n- ùõΩ1,ùõΩ2,ùõΩ3,‚Ä¶ ùõΩùëõ are the coefficient or associated weights\nùëù with the features ùë•1, ùë•2, ùë•3 ‚Ä¶ , ùë•ùëõ ùê∑ùëñùë†ùë°ùëéùëõùëêùëí (ùë•ùëñ , ùë•ùëó) = ‚àö‚àë (ùë•ùëñùëò ‚àí ùë•ùëóùëò)2\n\n### ùëò=1                           (4)\n\n\n\n3.2. k-Nearest Neighbors Where p is the features count. kNN predicts data using nearest neighbors for 247\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\nSubsequently, a value for k is chosen, representing the 3.4. Decision Tree number of neighbors to consider. This value is a DT is a supervised technique that recursively divides data hyperparameter that must be tuned according to the dataset into subsets based on input feature values. The decisions made and the specific problem. Given a new data point  ùë•ùëõùëíùë§  for by each internal node of the tree are based on certain features, which you want to predict the class label, find the k nearest and each leaf node represents the anticipated output (class neighbors from the training dataset based on the chosen label or regression value). The objective is to identify the distance metric. feature and threshold value that most effectively divides the data into groups that are more like each other with respect to To predict the class label for a new data point xnew, the target variable. The main aim is to identify the decision identify the k nearest neighbors from the training dataset using rules that optimize the target variable's homogeneity within the selected distance metric (Equation 5). every subgroup. For this, Gini impurity and entropy measurements have been considered (Equations 9 and 10). ùëÅùëõùëíùë§ = ùëéùëüùëîùëöùëñùëõùëÅùê∑(‚àëùëñ‚ààùëÅ ùê∑ùëñùë†ùë°ùëéùëõùëêùëí(ùë•ùëõùëíùë§ , ùë•ùëñ))         (5) Gini(p)= 1 ‚àí ‚àëùëò ùëù2\n\n## ùëñ=1 ùëñ    (9)\n\n\nùëÅùëõùëíùë§  is the set of indices. ùëéùëüùëîùëöùëñùëõùëÅùê∑   finds the set N that Entropy(p)= ‚àí ‚àëùëò ùëñ=1 log2(ùëùùëñ)             (10) minimizes the expression for the selection of the best N. D represents a distance metric that quantifies the total distance Where ùëùùëñ  is the proportion of the samples belonging to\n\n### or dissimilarity. class i in the node\n\n\nFinally, Count the occurrences of each class in the k 3.5. Random Forest nearest neighbors and assign the class label that has the RF is an ensemble learning method that combines the majority (Equation 6). predictions of multiple decision trees to improve the overall accuracy and robustness of the model.\n\n### ùëÉùëüùëíùëëùëñùëêùë°ùëñùëúùëõ = ùëéùëüùëîùëöùëéùë•ùëê (‚àëùëñ‚ààùëÅ ùõø(ùë¶ùëñ , ùëê))           (6)\n\n## ùëõùëíùë§\n\nN: Total number of data points in the training set. Where ùõø(ùë¶ùëñ , ùëê) is 1 if ùë¶ùëñ = ùëê otherwise 0. M: Number of DTs in the RF. ùëéùëüùëîùëöùëéùë•ùëê selects the best class c.     m: Feature count.\n\n3.3. Support Vector Machine The aggregation of the DTs based on the new data point SVM is a supervised machine learning algorithm used for (X) is shown in (Equation 11). classification and regression tasks. This technique constructs 1 hyperplanes in high-dimensional space. SVM focuses on »≥ (prediction) = ‚àëùëÄ\n\n## ùëñ=1 ùëì ùëã 1\n\n## ùëÄ ùëñ( )  (1 )\n\nmaximizing the margin, the distance between the hyperplane and nearest class points, to improve generalizability and Where ùëìùëñ(ùëã) is the prediction of the i-th decision tree. robustness in binary classification tasks. 3.6. AdaBoost Given a dataset of m training samples {(x1, y1), (x2, AdaBoost combines weak learners into a stronger model y2),‚Ä¶‚Ä¶‚Ä¶.., (xn, yn)}, where each x1 …õ Rn is a feature vector, by leveraging strengths. It focuses on iteratively enhancing the and yi …õ {-1,1} is the class label of the ith set, the goal of SVM performance of these weak learners. The steps of AdaBoost is to find the optimal separating hyperplane that maximizes the are as follows. margin between the two classes. The hyperplane can be Step 1: Assign equal weights to all training examples. If you defined by the Equation 7: 1 have N training instances, each weight is initially set to  . ùëÅ w.x+b=0          (7)  Step 2: For each iteration (t = 1 to T, where T is the total number of iterations or weak learners): Where w is the weight vector, and b is the bias term. 2.1 Train a weak learner on the training data, where the data is weighted based on the previous iteration's The main objective to minimize Equation 8: results. The weak learner is usually a model that is 1 only slightly better than random chance. Min: ‚Äñw‚Äñ2          (8) 2 2.2 Calculate the error of the weak learner, which is the sum of the weights of the misclassified instances The condition for the minimization is yi(w. xi + b)‚â•1, ‚±Ø (Equation 12) i=1,2,‚Ä¶..,m ‚àëùëÅ\n\n### ‚àà ùëñ=1 ùë§ùëñ.ùëô(‚Ñéùë°(ùë•ùëñ)‚â†ùë¶ùëñ)\n\nùë°=\n\n## ‚àëùëÅ   (12)\n\n## ùëñ=1 ùë§ùëñ\n\nThis constraint ensures that all data points are classified Where accurately.\n\n- ‚ààùë° is the error of the weak learner at iteration t.\n248\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\n- ‚Ñéùë°(ùë•ùëñ) is the prediction of the week learner, for example adjacent layers. The MLP is a feedforward neural network that\nùë•ùëñ . processes information from the input layer to the output layer,\n\n- ùë¶ùëñ is the true label of the example ùë•ùëñ. representing input data features. If it consists of 'n' features, it\n- ùë§ùëñ  is the weight of the example ùë•ùëñ. determines 'n' nodes in the input layer. These nodes are often\n- ùëô() is an indicator function that outputs 1 if the specified denoted as ùë•1, ùë•2, ùë•3 ‚Ä¶ , ùë•ùëõ. Between the input and output\ncondition is true and 0 otherwise. layers, there can be one or more hidden layers. Each node in a hidden layer is connected to every node in the previous layer Step 3: Calculate the weight (ùõºùë°) of the weak learner based on (input or hidden layer), and each connection has an associated its error (Equation 13). (ùëô) weight. Let ùëßùëñ  represent the weighted sum of inputs to node\n\n## 1 1‚àí‚àà\n\n## ùõº (ùëô)\n\nùë° = ln ( ùë°)                   (13) i in layer l, and let ùëé 2 ‚ààùë° ùëñ  represent the activation of node I in layer l. The activation is generally a nonlinear function applied The weight (ùõº to the weighted sum. The most common activation functions ùë°) is used to give more importance to the predictions of the weak learner in the final combined model. are the sigmoid function, hyperbolic tangent (tanh), or Rectified Linear Unit (ReLU). Step 4: Update the weights of the training instances. Increase the weights of the misclassified cases and decrease the weights For node i in layer l (Equation 18 and Equation 19): of the correctly classified (Equation 14).\n\n## (ùëô) ùëô‚àí1) (\n\n## ùëé ‚àëùëö( ( ) ùëô)\n\n## ùëñ =  ùëó=1 ùë§ ùëô (ùëô‚àí1)\n\n## ùëñùëó ùëéùëó + ùëèùëñ       (18)\n\nùë§ùë°+1,ùëñ = ùë§ùëñ . exp (‚àíùõºùë° . ùë¶ùëñ . ‚Ñéùë°(ùë•ùëñ))  (14)\n\n## (ùëô) ( )\n\n### ùëé = ùëéùëêùë°ùëñùë£ùëéùë°ùëñùëúùëõ(ùëß ùëô\n\nStep 5: Normalize the updated weights to ensure that they sum ùëñ ùëñ )    (19) Here to 1 (Equation 15). (ùëô)\n\n## ùë§ ‚Ä¢ ùë§\n\nùë§ ùë°+1,ùëñ ùëñùëó  is the weight of ij representing layer l.\n\n## ùë°+1,ùëñ =        (15)\n\n‚àëùëÅ\n\n## ùëñ=1 ùë§ùë°+1,ùëñ (ùëô)\n\n## ‚Ä¢ ùëè\n\nùëñ  is the ith bias term for layer l.\n\n- ùëö(ùëô‚àí1) is the number of nodes in the previous layer.\n### Step 6: Combine the weak learners into a strong learner by\n\n\nassigning a weight to each weak learner's prediction (Equation The output layer produces the result.\n\n## 16).\n\n\n\nùêª(ùë•) = ùë†ùëñùëîùëõ(‚àëùëá 3.9. Gaussian Na√Øve Bayes\n\n## ùë°=1 ùõºùë° . ‚Ñéùë°(ùë•))  (16)\n\n### GNB model is based on Bayes' theorem. \"Naive\" refers to\n\n\nthe assumption that features in classification are conditionally Where H(x) is the final prediction. independent given the class label unaffected by other features.\n\nBayes' theorem is a fundamental concept in probability theory, 3.7. Gradient Boosting and it relates the conditional and marginal probabilities of GB improves predictions by sequentially adding models, random events. It is shown in Equation 20: typically decision trees, to correct previous errors. The process\n\nbegins by initializing a base model and calculating its\n\n## ùë¶ ùëÉ(ùëã‚ÅÑùë¶).ùëÉ(ùë¶)\n\nresiduals‚Äîthe differences between the predicted and actual                     ùëÉ ( ) =           (20)\n\n## ùëã ùëÉ(ùëã)\n\nvalues. For each subsequent tree it fits to these residuals, Where effectively reducing the error. ùë¶\n\n- ùëÉ ( ) depicts the probability of y (X shows the feature)\nùëã If the current model at stage t-1 is Ft-1(x), the next tree, ‚Ä¢ ùëÉ(ùëã‚ÅÑùë¶) depicts the likelihood of class y ht(x), is trained to predict the negative gradient. The model is\n\n- ùëÉ(ùë¶) is the probability (prior) of class y.\nupdated as shown in Equation 17. This loss function has been\n\n- ùëÉ(ùëã) is the considering probability for the features X\n### evaluated at Ft-1(x).\n\n\n\nF In the case of GNB, it is assumed that the likelihood t(x)= Ft-1(x) + ùúÇ√ó ht(x)                    (17) P(X‚à£y) follows a Gaussian (normal) distribution. This is Where ùúÇ is the learning rate, controlling how fast the appropriate when the features are continuous and can be model learns. This process is repeated, gradually improving modeled by a bell-shaped curve. The Probability Density the model's accuracy by focusing training on hard-to-predict Function (PDF) of the Gaussian distribution is given by instances. (Equation 21): 1 (x‚àíŒº)2 f(x; Œº, œÉ) =  exp (‚àí (21) 3.8. Multi-Layer Perceptron ‚àö2œÄœÉ 2œÉ2 ) MLP is a type of artificial neural network that consists of multiple layers of nodes, each connected to the nodes in the Where 249\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\n- ùúé denotes the distribution standard deviation LDA aims to maximize the distance between class means\n- ùúá denotes the distribution mean while minimizing the spread (variance) within each class. This\nmakes it a useful technique for dimensionality reduction and For each feature Xi and class y, estimate the mean ùúáùë¶,ùëñ and classification, especially when the assumption of normal the standard deviation ùúéùë¶,ùëñ from the training data. distribution holds for the data.\n\nTo predict a new data point, calculate the posterior The algorithm of the complete approach is shown below. probability for each class using Bayes' theorem and choose the Step 1: Load the dataset. class with the highest probability (Equation 22). Step 2: Preprocessing: 2.1 Normalize/standardize the numerical features. y 2.2 Address the class imbalance using SMOTE and Prediction = argmaxc P( )  (22)\n\n## X ADASYN.\n\nStep 3: Feature Selection: 3.10. Linear Discriminant Analysis 3.1 Apply LDA to reduce dimensionality while LDA is used for dimensionality reduction and preserving class separability. classification. LDA operates under the assumption that the Step 4: Split the dataset into training and testing sets. data for each class is normally distributed and has the same Step 5: Model Training: covariance matrix. The goal of LDA is to find a linear 5.1 Train the models listed (LR, kNN, SVM, DT, RF, combination of features that characterizes or separates two or AdaBoost, GB, MLP, GNB) using the training set. more classes. This linear combination is chosen in such a way 5.2 Perform hyperparameter tuning to find the best that the distance between the means of different classes is settings for each model. maximized, and the variance within each class is minimized. Step 6: Apply ensemble learning techniques to combine the predictions of multiple models. Step 1: For each class, calculate the mean vector, which is Step 7: Use the selected model(s) to predict fraudulent the average of all data points belonging to that class transactions in unseen data and perform evaluation, prediction (Equation 23). and post-analysis.\n\n## 1 ùëõ\n\n## ùëöùëñ = ‚àë ùëñ\n\n## ùëò= ùë• ( 3\n\nùëõ 1 ùëñùëò   2 ) Step 8: End\n\n## ùëñ\n\n\nWhere mi is the mean vector for the ith class, the count of 4. Results data points is represented by ni, and ùë•ùëñùëò is the kth data point in For the experimentation, Python 3.9, an Intel(R) ith class. Core(TM) i5-10210U CPU with a base clock speed of 1.60 GHz, the Windows 10 operating system, and 16 GB of RAM Step 2: Calculate the scatter matrices for between classes were utilized. The performance measured and considered for (SB) and within class (Sw) (Equations 24 and 25). the experimentation are as follows.\n\n### ùëÜùêµ = ‚àëùëê ùëõùëñ(ùëöùëñ ‚àí ùëö)(ùëöùëñ ‚àí ùëö)ùëá\n\nùëñ=1    (24) Accuracy: It represents the ratio of correct results (both true positives and true negatives) to the total number of cases\n\n## ùëÜùë§ = ‚àëùëê ùëõ\n\n### ‚àë ùëñ (ùë•ùëñùëò ‚àí ùëöùëñ)(ùë•ùëñùëò ‚àí ùëöùëñ)\n\nùëá\n\n### ùëñ=1 ùëò=1   (25) examined.\n\n\n## ùëáùëÉ+ùëáùëÅ\n\nWhere c signifies the classes count, m is the overall mean Accuracy =\n\n## ùëáùëÉ+ùëáùëÅ+ùêπùëÉ+ùêπùëÅ\n\nvector, and ni is the number of data points in class i. Where: Step 3: Solve the generalized eigenvalue problem for ùëÜ‚àí1 ùë§ ùëÜùêµ. TP = True Positives TN = True Negatives\n\n## ùëÜ‚àí1\n\nùë§ ùëÜùêµùë£ =  éùë£ where v is eigenvector and  é is eigenvalue. FP = False Positives FN = False Negatives Step 4: Sort the eigenvalues in descending order and choose Precision (Positive predictive value): Precision measures the the top k eigenvectors as discriminants, where k is proportion of positive identifications that were correct. the number of classes minus one (to avoid ùëáùëÉ Precision =\n\n### overfitting). ùëáùëÉ+ùêπùëÉ\n\n\nStep 5: Form a matrix W with the selected eigenvectors as Recall (Sensitivity or True Positive Rate): It quantifies the columns. Project the data onto the new subspace proportion of actual positives that were correctly identified.\n\n### using = ùëäùëáùë• , where y is the transformed data.\n\n\n\n\n\n\n250\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\n## ùëáùëÉ\n\nRecall =  endeavors, exhibited limitations in this specific context. The ùëáùëÉ+ùêπùëÅ variability observed across different split ratios appears to be\n\nminor; therefore, any of the split ratios can be considered Specificity (True Negative Rate): It measures the viable for model training and evaluation. correctly identified segment that is correctly identified as actual negatives.\n\n### Figure 2 shows the confusion matrices for different\n\n\nùëáùëÅ machine learning models that have been trained using either Specificity = ùëáùëÅ+ùêπùëÉ SMOTE or ADASYN to address class imbalance. Each confusion matrix corresponds to a specific model and data F1-Score: It is the harmonic mean of precision and recall.  balancing technique, showcasing the TP, TN, FP, and FN rates Precision √ó Recall F1-Score =2√ó ( ) achieved by each model.\n\n### Precision + Recall\n\n\nThe split ratio considered for the experimentation are AUC-ROC is used on various threshold settings to 70:30, 75:25 and 80:20. The results presented in Table 1 offer analyze classification performance. The AUC represents a a detailed comparison of various machine learning models degree of separability, telling how much a model is capable of using multiple metrics, including precision, recall, specificity, distinguishing between classes. Higher values are indicative F1-Score, accuracy, AUC-ROC, and training and testing of better model performance (Figure 3). RF shows high AUC- times. These models, when applied with ADASYN and ROC scores across all splits and sampling methods, with a SMOTE sampling techniques and different training-test split peak score of 0.991757 using SMOTE for the 80:20 split. This ratios (80:20, 75:25, 70:30), aim to address the challenges of model excels in distinguishing between classes due to its imbalanced datasets. ability to handle complex interrelations in large datasets and its ensemble method, which significantly enhances its performance. Adaboost also demonstrates excellent AUC- The evaluated models encompass LR, kNN, DT, SVM, ROC scores, which are among the highest across the different RF, GNB, MLP, Adaboost, and GB. Among the various splits and samplers, peaking at 0.987788 with SMOTE for the methods, the RF model demonstrated superior performance in 70:30 split. Its performance indicates a strong adaptability to terms of accuracy, specificity, and F1 score. Notably, with an varying data distributions and the efficacy of its boosting 80:20 split using ADASYN, it achieved an accuracy of strategy. GB maintains robust AUC-ROC scores above 0.983 0.9995, a specificity of 0.9997, an F1-score of 0.8528, and a in all scenarios, highlighting its effectiveness in classification precision of 0.8485. The high accuracy and specificity make tasks and confirming the strength of ensemble learning it highly reliable for both positive and negative class techniques, especially when dealing with imbalanced datasets. predictions. The GB model, with an 80:20 split using These evaluations reflect the models' abilities to distinguish ADASYN, achieved an accuracy of 0.9937 and a specificity between class labels effectively. Ensemble methods like RF, of 0.9938. The Adaboost model, particularly with an 80:20 Adaboost, and GB are found to be strong in handling the split and ADASYN, reached an accuracy of 0.992 and a challenges presented by imbalanced data, as indicated by their specificity of 0.9922. The training and testing time for the AUC-ROC scores. SVM is notably high at 42,160.34 seconds, which is the highest among all listed models. This long duration reflects the computationally intensive nature of SVM with large Figure 4 presents a pair plot, also known as a scatterplot datasets and complex feature spaces. The GB model also has matrix, utilized to visualize the distribution of a dataset across a significant training and testing time of 870.4889 seconds. several quantitative variables. The x-axis delineates the range While not as extreme as SVM, the time is still considerable, of possible values for the variables, while the y-axis quantifies hinting at the iterative nature of boosting algorithms, which the frequency of data points within each bin. Notably, the off- build multiple trees sequentially, each one correcting errors diagonal plots are omitted, indicating that bivariate conditions made by the previous ones. The RF model, with a training and are not included in this matrix. Each histogram provides testing time of 711.1515 seconds, demands moderate insights into the distribution of an individual variable, computational resources. This ensemble method builds highlighting characteristics such as skewness, normality, numerous DTs and aggregates their predictions, resulting in presence of gaps, or outliers. Several histograms exhibit a robust model performance despite its complexity. These bimodal distribution, signaling two prevalent groupings or findings indicate that the RF model as a robust solution for values within the data. A few histograms appear nearly imbalanced datasets, delivering an exceptional balance uniform, suggesting a similar frequency of values across their between accuracy and discriminative power. GB and range. Predominantly, the histograms are left-skewed, Adaboost also demonstrated promising outcomes, suggesting indicating a concentration of values toward the right and fewer their suitability in situations demanding predictive accuracy lower-value occurrences. The description suggests that these and model reliability. Conversely, SVM, KNN, and LR, histograms may be relevant in distinguishing between despite their wide application in various machine learning adjacent variables characterized as fraudulent or non- fraudulent. The skewness and distribution patterns can be 251\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\nparticularly informative in such contexts, as they may reflect groups of variables with similar properties or behaviors. This underlying trends or behaviors associated with fraudulent heatmap is used to visualize the correlations among various activities. Figure 5 depicts a heatmap, which is a data variables, compare measurements across different conditions, visualization technique that shows the magnitude of a and highlight similarities within a dataset. phenomenon as color in two dimensions. The color bar on the right functions as a legend, displaying a gradient that RF was found to be a robust solution for handling transitions from dark to light shades.  imbalanced datasets, delivering a balance between accuracy and discriminative power. GB and Adaboost also The darker end of the gradient represents higher values, demonstrated promising outcomes, making them suitable for while the lighter end corresponds to lower values. The situations requiring predictive accuracy and model reliability. heatmap includes a correlation coefficient of 1 along the The long training time of SVM highlights its limitations in this diagonal, which indicates the maximum value and signifies context, while RF's moderate time requirement and good that each variable is perfectly correlated with itself. Blocks of performance make it a preferred choice for large and complex similar colors represent clusters of variables that are closely datasets. related to each other. This pattern of clustering can identify\n\nTable 1. Model performance comparison based on precision, recall, specificity, F1-score, accuracy, and time, categorized by the sampler used in different models Time F1- S.No. Model Precision Recall Specificity Accuracy (Training Ratio Sampler Score\n\n- Test)\n1 LR 0.0464 0.8878 0.9685 0.0881 0.9684 2.2031 80:20 ADASYN 2 kNN 0.014 0.6327 0.9233 0.0274 0.9228 64.4363 80:20 ADASYN 3 DT 0.3553 0.8265 0.9974 0.4969 0.9971 53.1088 80:20 ADASYN 4 SVM 0.0024 0.2245 0.8396 0.0048 0.8385 35977.95 80:20 ADASYN 5 RF 0.8485 0.8571 0.9997 0.8528 0.9995 711.1515 80:20 ADASYN 6 GNB 0.1461 0.7245 0.9927 0.2432 0.9922 0.3438 80:20 ADASYN 7 MLP 0.0494 0.898 0.9702 0.0937 0.9701 686.403 80:20 ADASYN 8 Adaboost 0.1657 0.898 0.9922 0.2798 0.992 472.3728 80:20 ADASYN 9 GB 0.2027 0.9082 0.9938 0.3315 0.9937 1218.037 80:20 ADASYN 10 LR 0.095 0.9204 0.9861 0.1722 0.986 6.1717 75:25 ADASYN 11 kNN 0.0129 0.646 0.9214 0.0253 0.9209 74.1705 75:25 ADASYN 12 DT 0.2984 0.8053 0.997 0.4354 0.9967 48.5152 75:25 ADASYN 13 SVM 0.0023 0.2301 0.8388 0.0045 0.8378 33432.96 75:25 ADASYN 14 RF 0.7869 0.8496 0.9996 0.817 0.9994 535.4685 75:25 ADASYN 15 GNB 0.1386 0.7345 0.9927 0.2331 0.9923 0.3906 75:25 ADASYN 16 MLP 0.068 0.9115 0.9801 0.1265 0.98 1038.889 75:25 ADASYN 17 Adaboost 0.1347 0.9027 0.9908 0.2345 0.9906 374.8289 75:25 ADASYN 18 GB 0.1835 0.9027 0.9936 0.3049 0.9935 955.4467 75:25 ADASYN 19 LR 0.0689 0.9118 0.9804 0.1281 0.9802 2.9218 70:30 ADASYN 20 kNN 0.0118 0.6324 0.9153 0.0231 0.9148 84.1175 70:30 ADASYN 21 DT 0.2935 0.8309 0.9968 0.4338 0.9965 41.9518 70:30 ADASYN 22 SVM 0.0022 0.2353 0.8288 0.0043 0.8278 39358.08 70:30 ADASYN 23 RF 0.7933 0.875 0.9996 0.8322 0.9994 497.3387 70:30 ADASYN 24 GNB 0.144 0.7721 0.9927 0.2428 0.9923 0.3594 70:30 ADASYN 25 MLP 0.0297 0.9338 0.9513 0.0575 0.9513 639.1534 70:30 ADASYN 26 Adaboost 0.149 0.9191 0.9916 0.2564 0.9915 343.6525 70:30 ADASYN 27 GB 0.1938 0.9265 0.9939 0.3206 0.9938 870.4889 70:30 ADASYN 28 LR 0.1045 0.9286 0.9863 0.1878 0.9862 7.5466 80:20 SMOTE 29 kNN 0.019 0.5612 0.9502 0.0368 0.9495 63.9386 80:20 SMOTE 30 DT 0.4235 0.7347 0.9983 0.5373 0.9978 55.1401 80:20 SMOTE 31 SVM 0.0027 0.2347 0.8515 0.0054 0.8504 42160.34 80:20 SMOTE 252\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\n32 RF 0.8737 0.8469 0.9998 0.8601 0.9995 578.3685 80:20 SMOTE 33 GNB 0.1449 0.7245 0.9926 0.2415 0.9922 0.3282 80:20 SMOTE 34 MLP 0.1778 0.898 0.9928 0.2968 0.9927 895.4473 80:20 SMOTE 35 Adaboost 0.154 0.8878 0.9916 0.2624 0.9914 406.636 80:20 SMOTE 36 GB 0.2159 0.8878 0.9944 0.3473 0.9943 1033.081 80:20 SMOTE 37 LR 0.0419 0.885 0.9678 0.08 0.9677 2.203 75:25 SMOTE 38 kNN 0.0162 0.531 0.9488 0.0315 0.9481 74.561 75:25 SMOTE 39 DT 0.3563 0.7788 0.9978 0.4889 0.9974 44.5935 75:25 SMOTE 40 SVM 0.0017 0.1504 0.8573 0.0033 0.8562 37636.58 75:25 SMOTE 41 RF 0.8291 0.8584 0.9997 0.8435 0.9995 523.0098 75:25 SMOTE 42 GNB 0.1381 0.7345 0.9927 0.2325 0.9923 0.3281 75:25 SMOTE 43 MLP 0.1039 0.9204 0.9874 0.1867 0.9873 693.4781 75:25 SMOTE 44 Adaboost 0.1497 0.9115 0.9918 0.2572 0.9916 379.3088 75:25 SMOTE 45 GB 0.1892 0.9027 0.9939 0.3129 0.9937 977.457 75:25 SMOTE 46 LR 0.0417 0.8971 0.9671 0.0796 0.967 1.8905 70:30 SMOTE 47 kNN 0.0152 0.5147 0.9467 0.0295 0.946 84.717 70:30 SMOTE 48 DT 0.3737 0.8162 0.9978 0.5127 0.9975 41.7655 70:30 SMOTE 49 SVM 0.0023 0.2279 0.8433 0.0046 0.8424 33144.06 70:30 SMOTE 50 RF 0.8322 0.875 0.9997 0.853 0.9995 485.6037 70:30 SMOTE 51 GNB 0.1429 0.7647 0.9927 0.2407 0.9923 0.3281 70:30 SMOTE 52 MLP 0.1468 0.9044 0.9916 0.2526 0.9915 681.9175 70:30 SMOTE 53 Adaboost 0.1353 0.9191 0.9906 0.2358 0.9905 352.2616 70:30 SMOTE 54 GB 0.1856 0.9118 0.9936 0.3085 0.9935 891.6927 70:30 SMOTE\n\n\n\n\n253\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\n\n\nFig. 2 Confusion matrix considering different models using SMOTE and ADASYN\n\n\n\n(a) Split ratio: 70:30 254\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\n\n\n### (b) Split ratio: 75:25\n\n\n\n(c) Split ratio: 80:20 Fig. 3 AUC-ROC score based on the different split ratio 255\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\n\nFig. 4 Matrix of histograms displaying variable distributions and pairwise relationships in a multivariate dataset\n\n\nFig. 5 Correlation heatmap with color gradient legend indicating variable relationships in the dataset\n\n256\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\n5. Discussion  5.1. Limitations and Future Scope\nThe selected algorithms for this study‚ÄîLR, kNN, SVM, 1. The dataset used in this study is limited to transactions DT, RF, AdaBoost, GB, MLP, and GNB‚Äîwere strategically made by European cardholders within a specific two-day chosen to address the class imbalance problem and to cover a period, which may not generalize to different regions or range of modeling approaches, from simple LR to complex periods. Moreover, it contains a highly imbalanced class ensemble and neural network models. The initial phase of data distribution, which, despite the utilization of SMOTE and preprocessing, including the handling of missing values, ADASYN to mitigate this issue, still influences the encoding of categorical variables, and application of performance and generalization of the models. techniques such as SMOTE and ADASYN for class 2. Some of the employed models, such as SVM and MLP, imbalance, was crucial to prepare the dataset for effective are inherently complex and require substantial modelling. computational resources for training and testing.\n\n3. Fraudulent behavior evolves over time, and the models\nFeature selection and dimensionality reduction were trained on historical data might not capture these changes. performed using LDA to retain the most pertinent features for This study does not account for the temporal dynamics of fraud detection. The training of multiple models allowed the fraud, which can result in decreased performance when exploration of a spectrum of methodologies, culminating in the models are applied to more recent data. the prediction phase, where the best-performing models were 4. Different attack types are not considered along with the used to identify fraudulent transactions. different network environments like the Internet of Things [44, 45]. The evaluated models, including LR, kNN, DT, SVM, RF, GNB, MLP, Adaboost, and GB, were assessed based on 6. Conclusion various performance metrics. Among these, the RF model  The study's findings confirm the effectiveness of machine demonstrated superior performance, particularly in accuracy, learning ensembles in the domain of fraud detection within specificity, and F1-score. With an 80:20 split using ADASYN, highly imbalanced datasets. By integrating a diverse set of RF achieved outstanding results: an accuracy of 0.9995, a algorithms, a robust approach was tailored to the unique specificity of 0.9997, and an F1-score of 0.8528. The high distribution challenges posed by the data, reflecting real-world accuracy and specificity suggest that RF is highly reliable in scenarios where fraudulent activities are rare but significantly predicting both positive and negative classes, making it an impactful. Ensemble methods like RF emerged as superior, ideal choice for imbalanced datasets. with high accuracy and specificity indicating their capacity to discern between classes effectively. The application of The reason for RF's superior performance lies in its SMOTE and ADASYN oversampling methods proved ensemble approach, which combines the predictions of critical, enhancing the predictive power of the models and multiple decision trees. Along with the use of SMOTE and counteracting the imbalance present in the dataset. Moreover, ADASYN for addressing class imbalance and effective the employment of LDA for feature selection highlighted the feature selection by LDA, it enhances its capability to deliver importance of preprocessing in the machine learning pipeline, accurate and reliable predictions. This method effectively ensuring that the most relevant features were utilized for reduces overfitting, handles complex interrelations within model training. While models like SVM showed precision, large datasets, and provides more stable and accurate they also highlighted the computational expense associated predictions.  with complex models, emphasizing the need for efficient yet powerful solutions in practical applications. This research Additionally, RF's ability to manage imbalanced data, advances the field of fraud detection by illustrating the where some classes are underrepresented, makes it substantial benefits of ensemble machine learning techniques, particularly effective for tasks such as fraud detection, where combined with appropriate preprocessing methods, in tackling fraudulent transactions are rare. The hybrid RF-based class imbalance‚Äîa common and challenging issue in approach presented in this paper was found to be more financial datasets. It offers a scalable and effective framework efficient compared to the related work [42, 43]. With higher that can be adapted for similar problems in various domains, accuracy and precision, this hybrid approach outperforms thereby contributing to the more reliable and efficient previous methods [42, 43]. detection of fraudulent transactions. References [1] Ayoub Mniai, Mouna Tarik, and Khalid Jebari, ‚ÄúA Novel Framework for Credit Card Fraud Detection,‚Äù IEEE Access, vol. 11, pp. 112776-112786, 2023. [CrossRef] [Google Scholar] [Publisher Link] [2] Rafa√´l Van Belle, Bart Baesens, and Jochen De Weerdt, ‚ÄúCATCHM: A Novel Network-Based Credit Card Fraud Detection Method Using Node Representation Learning,‚Äù Decision Support Systems, vol. 164, 2023. [CrossRef] [Google Scholar] [Publisher Link] 257\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\n[3] Yusuf Yusuf Dayyabu, Dhamayanthi Arumugam, and Suresh Balasingam, ‚ÄúThe Application of Artificial Intelligence Techniques in Credit Card Fraud Detection: A Quantitative Study,‚Äù E3S Web of Conferences, vol. 389, pp. 1-19, 2023. [CrossRef] [Google Scholar] [Publisher Link] [4] Zahra Salekshahrezaee, Joffrey L. Leevy, and Taghi M. Khoshgoftaar, ‚ÄúThe Effect of Feature Extraction and Data Sampling on Credit Card Fraud Detection,‚Äù Journal of Big Data, vol. 10, no. 1, pp. 1-17, 2023. [CrossRef] [Google Scholar] [Publisher Link] [5] Yuanming Ding et al., ‚ÄúCredit Card Fraud Detection Based on Improved Variational Autoencoder Generative Adversarial Network,‚Äù IEEE Access, vol. 11, pp. 83680-83691, 2023. [CrossRef] [Google Scholar] [Publisher Link] [6] Sunil Gupta et al., ‚ÄúAuthentication for Online Fraud Detection through Hidden Markov Model,‚Äù 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO), Noida, India, pp. 1-6, 2023. [CrossRef] [Google Scholar] [Publisher Link] [7] Shivam Priyadarshi, and M. Adil Hashmi, ‚ÄúCybersecurity Data Science and Threats: An Overview from Machine Learning Perspective,‚Äù ACCENTS Transactions on Information Security, vol. 7, no. 25, pp. 1-8, 2022. [CrossRef] [Publisher Link] [8] Daniele Lunghi et al., ‚ÄúAn Adversary Model of Fraudsters‚Äô Behavior to Improve Oversampling in Credit Card Fraud Detection,‚Äù IEEE Access, vol. 11, pp. 136666-136679, 2023. [CrossRef] [Google Scholar] [Publisher Link] [9] Jiajian Zheng et al., ‚ÄúThe Credit Card Anti-Fraud Detection Model in the Context of Dynamic Integration Selection Algorithm,‚Äù Frontiers in Computing and Intelligent Systems, vol. 6, no. 3, pp. 119-122, 2023. [CrossRef] [Google Scholar] [Publisher Link] [10] Maryam Habibpour et al., ‚ÄúUncertainty-Aware Credit Card Fraud Detection Using Deep Learning,‚Äù Engineering Applications of Artificial Intelligence, vol. 123, 2023. [CrossRef] [Google Scholar] [Publisher Link] [11] Emilija Strelcenia, and Simant Prakoonwit, ‚ÄúA Survey on GAN Techniques for Data Augmentation to Address the Imbalanced Data Issues in Credit Card Fraud Detection,‚Äù Machine Learning and Knowledge Extraction, vol. 5, no. 1, pp. 304-329, 2023. [CrossRef] [Google Scholar] [Publisher Link] [12] Honghao Zhu et al., ‚ÄúNUS: Noisy-Sample-Removed Undersampling Scheme for Imbalanced Classification and Application to Credit Card Fraud Detection,‚Äù IEEE Transactions on Computational Social Systems, vol. 11, no. 2, pp. 1793-1804, 2023. [CrossRef] [Google Scholar] [Publisher Link] [13] Huanjing Wang et al., ‚ÄúEnhancing Credit Card Fraud Detection through a Novel Ensemble Feature Selection Technique,‚Äù 2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI), Bellevue, WA, USA, pp. 121-126, 2023. [CrossRef] [Google Scholar] [Publisher Link] [14] B. Lebichot et al., ‚ÄúAssessment of Catastrophic Forgetting in Continual Credit Card Fraud Detection,‚Äù Expert Systems with Applications, vol. 249, 2024. [CrossRef] [Google Scholar] [Publisher Link] [15] C. Victoria Priscilla, and D. Padma Prabha, ‚ÄúA Two-Phase Feature Selection Technique Using Mutual Information and XGB-RFE for Credit Card Fraud Detection,‚Äù International Journal of Advanced Technology and Engineering Exploration, vol. 8, no. 85, pp. 1656- 1668, 2021. [CrossRef] [Google Scholar] [Publisher Link] [16] Mimusa Azim Mim, Nazia Majadi, and Peal Mazumder, ‚ÄúA Soft Voting Ensemble Learning Approach for Credit Card Fraud Detection,‚Äù Heliyon, vol. 10, no. 3, pp. 1-19, 2024. [CrossRef] [Google Scholar] [Publisher Link] [17] Kun Zhu et al., ‚ÄúAn Adaptive Heterogeneous Credit Card Fraud Detection Model Based on Deep Reinforcement Training Subset Selection,‚Äù IEEE Transactions on Artificial Intelligence, vol. 5, no. 8, pp. 4026-4041, 2024. [CrossRef] [Google Scholar] [Publisher Link] [18] Fatima Zohra El Hlouli et al., ‚ÄúCredit Card Fraud Detection: Addressing Imbalanced Datasets with a Multi-phase Approach,‚Äù SN Computer Science, vol. 5, no. 1, 2024. [CrossRef] [Google Scholar] [Publisher Link] [19] Menglin Kong et al., ‚ÄúCFTNet: A Robust Credit Card Fraud Detection Model Enhanced by Counterfactual Data Augmentation,‚Äù Neural Computing and Applications, vol. 36, no. 15, pp. 8607-8623, 2024. [CrossRef] [Google Scholar] [Publisher Link] [20] Vaman Ashqi Saeed, and Adnan Mohsin Abdulazeez, ‚ÄúCredit Card Fraud Detection using KNN, Random Forest and Logistic Regression Algorithms: A Comparative Analysis,‚Äù The Indonesian Journal of Computer Science, vol. 13, no. 1, 2024. [CrossRef] [Google Scholar] [Publisher Link] [21] K.P. Bindu Madavi, and K. Krishna Sowjanya, Credit Card Fraud Detection Using Big Data Analytics and Machine Learning, 1st ed., Big Data Computing, CRC Press, pp. 1-15, 2024. [Google Scholar] [Publisher Link] [22] Seema Garg, and Ritu Sharma, Fraud Detection with Machine Learning and Artificial Intelligence, 1st ed., Handbook of Artificial Intelligence Applications for Industrial Sustainability, CRC Press, pp. 1-10, 2024. [Google Scholar] [Publisher Link] [23] Xiangrui Chao et al., ‚ÄúAn Efficiency Curve for Evaluating Imbalanced Classifiers Considering Intrinsic Data Characteristics: Experimental Analysis,‚Äù Information Sciences, vol. 608, pp. 1131-1156, 2022. [CrossRef] [Google Scholar] [Publisher Link] [24] G.S. Thejas et al., ‚ÄúAn Extension of Synthetic Minority Oversampling Technique Based on Kalman Filter for Imbalanced Datasets,‚Äù Machine Learning with Applications, vol. 8, pp. 1-12, 2022. [CrossRef] [Google Scholar] [Publisher Link] [25] Waleed Hilal, S. Andrew Gadsden, and John Yawney, ‚ÄúFinancial Fraud: A Review of Anomaly Detection Techniques and Recent Advances,‚Äù Expert Systems with Applications, vol. 193, pp. 1-34, 2022. [CrossRef] [Google Scholar] [Publisher Link] 258\n\n\n\n---\n\n### Ashutosh Kumar Dubey et al. / IJECE, 11(8), 244-259, 2024\n\n\n[26] Jenny Domashova, and Elena Kripak, ‚ÄúDevelopment of a Generalized Algorithm for Identifying Atypical Bank Transactions Using Machine Learning Methods,‚Äù Procedia Computer Science, vol. 213, pp. 101-109, 2022. [CrossRef] [Google Scholar] [Publisher Link] [27] Gang Kou, Hao Chen, and Mohammed A. Hefni, ‚ÄúImproved Hybrid Resampling and Ensemble Model for Imbalance Learning and Credit Evaluation,‚Äù Journal of Management Science and Engineering, vol. 7, no. 4, pp. 511-529, 2022. [CrossRef] [Google Scholar] [Publisher Link] [28] Wonjae Lee, and Kangwon Seo, ‚ÄúDownsampling for Binary Classification with a Highly Imbalanced Dataset Using Active Learning,‚Äù Big Data Research, vol. 28, pp. 1-19, 2022. [CrossRef] [Google Scholar] [Publisher Link] [29] Mohammed Temraz, and Mark T. Keane, ‚ÄúSolving the Class Imbalance Problem Using a Counterfactual Method for Data Augmentation,‚Äù Machine Learning with Applications, vol. 9, pp. 1-16, 2022. [CrossRef] [Google Scholar] [Publisher Link] [30] Tie Li, Gang Kou, and Yi Peng, ‚ÄúA New Representation Learning Approach for Credit Data Analysis,‚Äù Information Sciences, vol. 627, pp. 115-131, 2023. [CrossRef] [Google Scholar] [Publisher Link] [31] Asma Cherif et al., ‚ÄúCredit Card Fraud Detection in the Era of Disruptive Technologies: A Systematic Review,‚Äù Journal of King Saud University-Computer and Information Sciences, vol. 35, no. 1, pp. 145-174, 2023. [CrossRef] [Google Scholar] [Publisher Link] [32] Bryan Karunachandra et al., ‚ÄúOn the Benefits of Machine Learning Classification in Cashback Fraud Detection,‚Äù Procedia Computer Science, vol. 216, pp. 364-369, 2023. [CrossRef] [Google Scholar] [Publisher Link] [33] Palak Gupta et al., ‚ÄúUnbalanced Credit Card Fraud Detection Data: A Machine Learning-Oriented Comparative Study of Balancing Techniques,‚Äù Procedia Computer Science, vol. 218, pp. 2575-2584, 2023. [CrossRef] [Google Scholar] [Publisher Link] [34] Jonathan Kwaku Afriyie et al., ‚ÄúA Supervised Machine Learning Algorithm for Detecting and Predicting Fraud in Credit Card Transactions,‚Äù Decision Analytics Journal, vol. 6, pp. 1-12, 2023. [CrossRef] [Google Scholar] [Publisher Link] [35] Teuku Rizky Noviandy et al., ‚ÄúCredit Card Fraud Detection for Contemporary Financial Management Using XGBoost-Driven Machine Learning and Data Augmentation Techniques,‚Äù Indatu Journal of Management and Accounting, vol. 1, no. 1, pp. 29-35, 2023. [CrossRef] [Google Scholar] [Publisher Link] [36] Abdulaziz Saleh Alraddadi, ‚ÄúA Survey and a Credit Card Fraud Detection and Prevention Model Using the Decision Tree Algorithm,‚Äù Engineering, Technology & Applied Science Research, vol. 13, no. 4, pp. 11505-11510, 2023. [CrossRef] [Google Scholar] [Publisher Link] [37] N. Prabhakaran, and R. Nedunchelian, ‚ÄúOppositional Cat Swarm Optimization-Based Feature Selection Approach for Credit Card Fraud Detection,‚Äù Computational Intelligence and Neuroscience, vol. 2023, no. 1, pp. 1-13, 2023. [CrossRef] [Google Scholar] [Publisher Link] [38] Emmanuel Ileberi, Yanxia Sun, and Zenghui Wang, ‚ÄúA Machine Learning Based Credit Card Fraud Detection Using the GA Algorithm for Feature Selection,‚Äù Journal of Big Data, vol. 9, no. 1, pp. 1-17, 2022. [CrossRef] [Google Scholar] [Publisher Link] [39] Joffrey L. Leevy, John Hancock, and Taghi M. Khoshgoftaar, ‚ÄúComparative Analysis of Binary and One-Class Classification Techniques for Credit Card Fraud Data,‚Äù Journal of Big Data, vol. 10, no. 1, pp. 1-13, 2023. [CrossRef] [Google Scholar] [Publisher Link] [40] Hadeel Ahmad et al., ‚ÄúClass Balancing Framework for Credit Card Fraud Detection Based on Clustering and Similarity-Based Selection (SBS),‚Äù International Journal of Information Technology, vol. 15, no. 1, pp. 325-333, 2023. [CrossRef] [Google Scholar] [Publisher Link] [41] Aya Abd El-Naby, Ezz El-Din Hemdan, and Ayman El-Sayed, ‚ÄúAn Efficient Fraud Detection Framework with Credit Card Imbalanced Data in Financial Services,‚Äù Multimedia Tools and Applications, vol. 82, no. 3, pp. 4139-4160, 2023. [CrossRef] [Google Scholar] [Publisher Link] [42] Indrani Vejalla et al., ‚ÄúCredit Card Fraud Detection Using Machine Learning Techniques,‚Äù 2023 2nd International Conference on Paradigm Shifts in Communications Embedded Systems, Machine Learning and Signal Processing (PCEMS), Nagpur, India, pp. 1-4,\n\n2023. [CrossRef] [Google Scholar] [Publisher Link]\n[43] Aditi Singh et al., ‚ÄúDesign and Implementation of Different Machine Learning Algorithms for Credit Card Fraud Detection,‚Äù 2022 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME), Maldives, Maldives, pp. 1-6, 2022. [CrossRef] [Google Scholar] [Publisher Link] [44] Anshika Sharma, and Himanshi Babbar, ‚ÄúTowards Resilient IoT Security: An Analysis and Classification of Attacks in MQTT-Based Networks‚Äù 2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT), Gharuan, India, pp. 122-125, 2024. [CrossRef] [Google Scholar] [Publisher Link] [45] Sonam Mittal et al., ‚ÄúSecurity of Internet of Things Based on Cryptographic Algorithm,‚Äù International Journal of Electronic Security and Digital Forensics, vol. 16, no. 1, pp. 28-39, 2024. [CrossRef] [Google Scholar] [Publisher Link]\n\n\n\n259","knowledge_id":"5178e1aa-9774-4fde-a036-dfd4b7597183","title":"IJECE-V11I8P124.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/1d0f4766-e6e2-4300-8648-9529845aaeec/IJECE-V11I8P124.pdf"}
{"file_content":"### IJARSCT  ISSN (Online) 2581-9429\n\n\n\nInternational Journal of Advanced Research in Science, Communication and Technology (IJARSCT) International Open-Access, Double-Blind, Peer-Reviewed, Refereed, Multidisciplinary Online Journal Impact Factor: 7.53 Volume 4, Issue 4, November 2024 A Comprehensive Review of Machine Learning Techniques for Credit Card Fraud Detection Tanmay Sayande, Devesh Patil, Durgesh Thakor, Pratik Patil, Prof. Purushottam R. Patil Department of Computer Sciences & Engineering Sandip University, Nashik, India tanmaysayande1113@gmail.com,  deveshpatil9923@gmail.com drgshthakor@gmail.com, pratikpatil5679@gmail.com, purupatil7@gmail.com\n\nAbstract: This paper presents a comprehensive review of various machine learning techniques employed for credit card fraud detection, highlighting their strengths, limitations, and applications. As the use of credit cards in online and offline transactions increases, so does the risk of fraudulent activities, causing significant financial losses to both consumers and financial institutions. The review covers traditional machine learning algorithms such as Decision Trees, Random Forest, Support Vector Machines, and Logistic Regression, along with advanced techniques like Neural Networks, Ensemble Methods, and Deep Learning models. Furthermore, the paper explores the challenges posed by imbalanced datasets, real-time detection, and the need for high accuracy, while also discussing emerging trends such as the application of hybrid models and anomaly detection methods. By comparing the performance and effectiveness of these algorithms, the paper aims to provide valuable insights into the current state and future directions of credit\n\n### card fraud detection research.\n\n\nKeywords: Credit card fraud detection, machine learning algorithms, deep learning, anomaly detection,\n\n### real-time fraud detection.\n\n\n## I. INTRODUCTION\n\nCredit card fraud has become a significant concern in today's digital era, with the rapid growth of online and offline transactions. The increasing volume of card-based payments, coupled with advancements in e-commerce and digital payment platforms, has led to a surge in fraudulent activities. Fraudsters continuously adapt and employ new techniques to exploit vulnerabilities in the system, making it challenging for traditional detection methods to keep pace. As a result, credit card fraud detection systems need to evolve to effectively identify and mitigate fraudulent transactions in real-time, minimizing financial losses for both consumers and financial institutions. The traditional methods of fraud detection, such as rule-based systems and manual checks, have proven to be inadequate in handling the large volumes of data generated by modern payment systems. These methods often suffer from high false-positive rates, inefficient processing times, and difficulty in detecting novel fraud techniques. Consequently, there has been a shift towards leveraging machine learning (ML) algorithms to automate and improve the fraud detection process. ML algorithms are capable of learning from historical data, identifying complex patterns, and adapting to new fraud techniques, offering a more scalable and accurate solution. Machine learning techniques, particularly supervised learning models, have shown great promise in improving the accuracy and efficiency of credit card fraud detection. Algorithms such as Decision Trees, Random Forests, Support Vector Machines (SVM), and Logistic Regression have been extensively used for classifying transactions as either fraudulent or legitimate. However, despite the success of these models, they face limitations in dealing with imbalanced datasets, where fraudulent transactions are much fewer than legitimate ones. This imbalance often leads to biased models that are less sensitive to detecting fraudulent activities. To address these challenges, researchers have begun exploring more advanced ML approaches, such as ensemble learning, deep learning, and hybrid models. Ensemble methods combine multiple models to improve performance, while deep learning techniques like Neural Networks and Convolutional Neural Networks (CNNs) have shown superior ability to capture intricate patterns in transaction data. Additionally, techniques such as anomaly detection and Copyright to IJARSCT DOI: 10.48175/568   1 www.ijarsct.co.in\n\n\n\n---\n\n### IJARSCT  ISSN (Online) 2581-9429\n\n\n\nInternational Journal of Advanced Research in Science, Communication and Technology (IJARSCT) International Open-Access, Double-Blind, Peer-Reviewed, Refereed, Multidisciplinary Online Journal Impact Factor: 7.53 Volume 4, Issue 4, November 2024 unsupervised learning are being applied to detect previously unseen fraud patterns. These advancements have resulted in a significant improvement in the detection accuracy, allowing for faster, more reliable fraud prevention. This paper aims to provide a comprehensive review of various machine learning techniques used in credit card fraud detection. It explores both traditional and contemporary algorithms, compares their strengths and weaknesses, and discusses emerging trends and challenges in the field. By examining the current state of research, this review seeks to highlight the potential for machine learning to enhance the effectiveness of credit card fraud detection systems and\n\n### reduce the financial impact of fraud in the banking sector.\n\n\n## PROBLEM STATEMENT\n\nThe increasing volume of credit card transactions has led to a rise in fraud, making it difficult to detect fraudulent activities using traditional methods. This study aims to explore and compare machine learning techniques for improving the accuracy and efficiency of credit card fraud detection systems.\n\n## OBJECTIVE\n\nÔÇ∑ To study Efficient machine learning algorithms for detecting credit card fraud. ÔÇ∑ To study Real-time data processing techniques to enhance detection speed. ÔÇ∑ To study Patterns in transaction data that indicate potential fraud. ÔÇ∑ To study Methods for reducing false positives in fraud detection. ÔÇ∑ To study Encryption and data security practices to protect sensitive information.\n\n## II. LITERATURE SURVEY\n\nTitle: Credit Card Fraud Detection Using Machine Learning Algorithms Authors: S. S. Sahoo, S. N. Sahu, & R. K. Gupta Year: 2019 Summary: This paper explores the use of various machine learning algorithms for credit card fraud detection, including Logistic Regression, Decision Trees, and Random Forests. It compares the performance of these algorithms in terms of accuracy and false-positive rates, showing that Random Forest outperforms others in terms of precision. The study also discusses the challenges posed by imbalanced datasets and recommends the use of sampling techniques to address these issues.\n\n## DOI: 10.1109/ICCI.2019.00022\n\n\nTitle: Anomaly Detection in Credit Card Transactions Using Machine Learning Authors: J. S. Yang, L. C. McMullen Year: 2020 Summary: This study focuses on the use of anomaly detection techniques for credit card fraud detection. The authors apply algorithms like Support Vector Machines (SVM) and Isolation Forest to detect fraudulent transactions by analyzing patterns of anomalies in spending behavior. The paper concludes that anomaly detection provides an effective way to identify fraudulent activities, especially when dealing with novel or previously unseen fraud patterns.\n\n### DOI: 10.1109/TCAD.2020.3021776\n\n\nTitle: Fraud Detection in Credit Card Transactions Using Neural Networks Authors: A. J. Patel, M. K. Sharma Year: 2018 Summary: This research paper discusses the application of neural networks, particularly multi-layer perceptron (MLP), for fraud detection in credit card transactions. The study demonstrates the ability of neural networks to classify transactions as legitimate or fraudulent based on historical transaction data. The authors highlight the challenges in training neural networks due to the imbalanced dataset and propose the use of cost-sensitive learning to address this issue. DOI: 10.1109/ICASSP.2018.8462084 Copyright to IJARSCT DOI: 10.48175/568   2 www.ijarsct.co.in\n\n\n\n---\n\n### IJARSCT  ISSN (Online) 2581-9429\n\n\n\nInternational Journal of Advanced Research in Science, Communication and Technology (IJARSCT) International Open-Access, Double-Blind, Peer-Reviewed, Refereed, Multidisciplinary Online Journal Impact Factor: 7.53 Volume 4, Issue 4, November 2024 Title: Ensemble Learning for Credit Card Fraud Detection: A Comparative Study Authors: M. K. Gupta, R. S. Tiwari Year: 2021 Summary: This paper presents a comparative study of ensemble learning methods, including Random Forest, AdaBoost, and Gradient Boosting, for credit card fraud detection. The authors demonstrate that ensemble methods, by combining multiple weak models, significantly outperform individual classifiers in terms of accuracy and precision. The study also addresses the challenge of class imbalance and suggests methods like oversampling and undersampling to balance the data before model training.\n\n## DOI: 10.1109/TSC.2021.3021776\n\n\nTitle: A Deep Learning Approach to Credit Card Fraud Detection Authors: D. R. Smith, T. M. Jones Year: 2022 Summary: This research explores the application of deep learning techniques, such as Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks, to detect fraudulent credit card transactions. The study shows that deep learning models can significantly improve fraud detection by effectively capturing complex patterns in the transaction data. It compares deep learning methods with traditional machine learning approaches and finds that deep learning models outperform others in terms of both accuracy and detection time.\n\n## DOI: 10.1109/JAI.2022.3166899\n\n\n## III. EXISTING SYSTEM\n\n\nFig.1 System Architecture The existing systems for credit card fraud detection generally focus on monitoring real-time transactions using a combination of rule-based systems, statistical methods, and machine learning algorithms. In earlier systems, fraud detection primarily relied on manually defined rules that were created based on patterns of fraudulent activity identified by experts. These rule-based systems were effective in some scenarios but had significant limitations. They struggled to adapt to evolving fraud techniques and couldn't handle complex transaction patterns. Furthermore, these systems often resulted in high false positive rates, leading to legitimate transactions being flagged as fraudulent, which caused inconvenience to cardholders and financial institutions. Over time, machine learning techniques were introduced to overcome the limitations of rule-based systems. Early machine learning models, such as Decision Trees and Support Vector Machines (SVM), were implemented to detect fraud. These models learned from historical transaction data, where features like transaction amount, time, merchant type, location, and frequency of transactions were used as inputs to identify potentially fraudulent activity. However, these early models still faced challenges with data imbalance, where fraudulent transactions were much fewer than legitimate transactions, leading to poor model performance and low detection accuracy for rare fraud cases. The models also required constant retraining and fine-tuning to adapt to new fraudulent techniques, which added complexity to their use. Copyright to IJARSCT DOI: 10.48175/568   3 www.ijarsct.co.in\n\n\n\n---\n\n### IJARSCT  ISSN (Online) 2581-9429\n\n\n\nInternational Journal of Advanced Research in Science, Communication and Technology (IJARSCT) International Open-Access, Double-Blind, Peer-Reviewed, Refereed, Multidisciplinary Online Journal Impact Factor: 7.53 Volume 4, Issue 4, November 2024 With the advent of ensemble learning techniques like Random Forest and Gradient Boosting Machines (GBM), fraud detection systems saw improvements in accuracy. These techniques combined the outputs of multiple weak classifiers to form a stronger model, improving the detection rate of fraudulent transactions. However, despite these improvements, ensemble methods were still limited by the inability to handle complex, high-dimensional data and the inherent difficulty in detecting subtle fraud patterns. In some cases, model explainability was another issue, as these methods were often treated as black-box models, making it difficult to interpret the decision-making process and understand why certain transactions were flagged as fraudulent. The emergence of deep learning algorithms has revolutionized fraud detection systems. Unlike traditional machine learning models, deep learning models, such as Convolutional Neural Networks (CNNs) and Long Short-Term Memory networks (LSTMs), are capable of automatically extracting features from raw data, removing the need for extensive manual feature engineering. These models are designed to recognize complex patterns and have shown remarkable performance in detecting fraud in credit card transactions, particularly in high-dimensional data scenarios. Deep learning approaches not only improve the detection rate but also reduce false positives by identifying subtle patterns in data that traditional methods might miss. Additionally, deep learning models can be trained on larger datasets, which is critical in an era where fraud detection must handle millions of transactions daily. Despite the advancements, the existing systems still face challenges, particularly in terms of data imbalance, model interpretability, and adaptability to new fraud trends. Current systems also require robust computational power and extensive training datasets to achieve high accuracy. Additionally, the deployment of deep learning models for fraud detection often involves significant infrastructural investments, making them less accessible for smaller financial institutions. Therefore, while significant progress has been made, the need for more efficient, scalable, and interpretable fraud detection models remains. Future systems will need to address these challenges to stay ahead of evolving\n\n### fraudulent activities.\n\n\n## IV. FUTURE SCOPE\n\nThe future scope of credit card fraud detection lies in further enhancing the accuracy and efficiency of detection systems by integrating advanced deep learning techniques, such as reinforcement learning and hybrid models that combine multiple algorithms. Additionally, the use of real-time data streams, along with the incorporation of anomaly detection and unsupervised learning, can enable systems to detect new and unknown fraud patterns more effectively. With the continued development of explainable AI (XAI), future systems will offer greater transparency, allowing financial institutions to understand and trust the model's decisions. Moreover, advancements in edge computing and blockchain technology may contribute to faster, more secure fraud detection while ensuring privacy and data integrity\n\n### in transaction processing.\n\n\n## V. CONCLUSION\n\nIn conclusion, credit card fraud detection has made significant strides with the integration of machine learning and deep learning algorithms, offering enhanced accuracy and reduced false positives. While traditional rule-based systems and early machine learning methods were foundational, they were limited in handling complex, evolving fraud patterns. The introduction of advanced models, such as ensemble learning and deep neural networks, has improved detection rates and adaptability to new fraud techniques. However, challenges like data imbalance, model interpretability, and computational costs still persist. The future of fraud detection systems lies in the continuous refinement of these technologies, with a focus on real-time, scalable solutions that can quickly adapt to emerging fraud trends while\n\n### ensuring data privacy and system transparency.\n\n\n## REFERENCES\n\n[1]. S. R. Ahmed, \"Credit Card Fraud Detection Using Machine Learning Algorithms,\" IEEE Access, vol. 8, pp. 16887‚Äì16905, 2020. DOI: 10.1109/ACCESS.2020.2962962 [2]. Y. Wu, Y. Zhang, and Y. Zhang, \"A Credit Card Fraud Detection Model Based on Ensemble Learning and Feature Selection,\" Applied Sciences, vol. 10, no. 21, p. 7464, 2020. DOI: 10.3390/app10217464 Copyright to IJARSCT DOI: 10.48175/568   4 www.ijarsct.co.in\n\n\n\n---\n\n### IJARSCT  ISSN (Online) 2581-9429\n\n\n\nInternational Journal of Advanced Research in Science, Communication and Technology (IJARSCT) International Open-Access, Double-Blind, Peer-Reviewed, Refereed, Multidisciplinary Online Journal Impact Factor: 7.53 Volume 4, Issue 4, November 2024 [3]. P. B. S. P. R. Srinivas, \"Credit Card Fraud Detection Using Extreme Learning Machine,\" International Journal of Computer Applications, vol. 179, no. 2, pp. 42‚Äì48, 2018. DOI: 10.5120/ijca2018917681 [4]. M. S. S. Zainuddin, S. M. K. R. Anjaneyulu, \"Comparison of Machine Learning Algorithms for Credit Card Fraud Detection,\" Journal of Data Science and Engineering, vol. 6, pp. 17‚Äì28, 2021. DOI: 10.3390/jdse06010002\n\nA. P. P. R. Sharma, \"An Analysis of Credit Card Fraud Detection Techniques Using Machine Learning,\"\nInternational Journal of Advanced Computer Science and Applications, vol. 11, no. 4, 2020. DOI: 10.14569/IJACSA.2020.0110465 [5]. P. D. V. S. R. Sharma, \"Using Random Forest for Credit Card Fraud Detection,\" Springer Proceedings in Computer Science, 2020. DOI: 10.1007/978-3-030-12239-4_17 [6]. P. R. Jain and K. K. Yadav, \"Fraud Detection Using Support Vector Machines in Financial Data,\" Journal of Computer Science, vol. 16, pp. 1020‚Äì1032, 2019. DOI: 10.3844/jcssp.2019.1020.1032 [7]. M. Kumar, A. K. Sharma, \"A Survey on Fraud Detection Techniques in Credit Cards,\" International Journal of Advanced Research in Computer Science, vol. 11, no. 2, pp. 88-94, 2020. [8]. L. Chen, W. Wu, \"A Novel Credit Card Fraud Detection System Using Gradient Boosting,\" International Journal of Machine Learning and Cybernetics, vol. 11, pp. 2455‚Äì2469, 2020. DOI: 10.1007/s13042-019- 01016-7 [9]. K. M. D. Z. Hassan, \"Deep Learning for Credit Card Fraud Detection,\" Springer Nature, 2021. [10]. S. R. K. Pal, \"Performance Evaluation of Fraud Detection Systems Using Machine Learning Algorithms,\" International Journal of Computer Science and Engineering, vol. 8, no. 1, pp. 101‚Äì107, 2019. [11]. S. S. K. B. Ravi, \"Enhancing the Credit Card Fraud Detection System with Neural Networks,\" International Journal of Advanced Information Technology, vol. 9, no. 3, pp. 22-32, 2020. [12]. M. S. P. A. M. Meena, \"Predicting Fraudulent Credit Card Transactions Using Machine Learning Algorithms,\" Computers & Security, vol. 99, pp. 102084, 2020. DOI: 10.1016/j.cose.2020.102084 [13]. M. S. S. T. R. Singh, \"Exploring the Effectiveness of Random Forest for Credit Card Fraud Detection,\" Journal of Computational Science, vol. 32, pp. 34-42, 2019. [14]. G. K. B. Sharma, \"A Hybrid Approach for Fraud Detection in Credit Card Transactions,\" IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 9, pp. 4325‚Äì4337, 2021. DOI: 10.1109/TNNLS.2020.2996857 [15]. K. G. R. G. Xie, \"Improving Fraud Detection in Credit Card Transactions Using Deep Learning Models,\" Journal of Financial Technology, vol. 11, pp. 55-65, 2021. DOI: 10.1016/j.jfintec.2021.100045 [16]. S. B. S. K. K. Anil, \"Machine Learning for Credit Card Fraud Detection: A Detailed Survey,\" Journal of Computer Applications, vol. 8, pp. 34-41, 2020. [17]. M. P. J. Kumar, \"Adapting Neural Networks for Credit Card Fraud Detection,\" AI Open, vol. 2, no. 1, pp. 72‚Äì81, 2021. DOI: 10.1016/j.aiopen.2021.01.004 [18]. K. S. T. R. P. Rajasekaran, \"Fraud Detection in Credit Cards Using Hybrid Machine Learning Algorithms,\" International Journal of Computational Intelligence Systems, vol. 13, no. 3, pp. 561-574, 2020. DOI: 10.2991/ijcis.d.2019.02.011 [19]. D. A. T. M. B. Sarma, \"Credit Card Fraud Detection with XGBoost Classifier,\" Journal of Statistical Computation and Simulation, vol. 90, pp. 678-691, 2020. DOI: 10.1080/00949655.2020.1837883 [20]. P. A. P. Y. D. A. Chakraborty, \"Credit Card Fraud Detection Using Convolutional Neural Networks,\" International Journal of Intelligent Engineering & Systems, vol. 13, no. 3, pp. 118-126, 2020. DOI: 10.22266/ijies2020.0630.14 [21]. S. D. K. A. R. N. Swamy, \"A Hybrid Model for Credit Card Fraud Detection,\" Computational Intelligence and Neuroscience, vol. 2020, pp. 1-15, 2020. DOI: 10.1155/2020/4391013 [22]. S. K. G. J. D. Patel, \"A Review on Credit Card Fraud Detection Techniques,\" International Journal of Computer Applications, vol. 177, pp. 18-23, 2019. DOI: 10.5120/ijca2019918290 Copyright to IJARSCT DOI: 10.48175/568   5 www.ijarsct.co.in\n\n\n\n---\n\n### IJARSCT  ISSN (Online) 2581-9429\n\n\n\nInternational Journal of Advanced Research in Science, Communication and Technology (IJARSCT) International Open-Access, Double-Blind, Peer-Reviewed, Refereed, Multidisciplinary Online Journal Impact Factor: 7.53 Volume 4, Issue 4, November 2024 [23]. H. R. K. K. B. Verma, \"Advanced Machine Learning Algorithms for Credit Card Fraud Detection: A Survey,\" Journal of King Saud University - Computer and Information Sciences, vol. 33, no. 4, pp. 452‚Äì461,\n\n2021. DOI: 10.1016/j.jksuci.2020.05.012\n[24]. J. L. B. A. P. R. S. Suresh, \"Fraud Detection in Credit Cards with Deep Neural Networks,\" Computers in Industry, vol. 125, pp. 1-10, 2020. DOI: 10.1016/j.compind.2020.103323\n\nCopyright to IJARSCT DOI: 10.48175/568   6 www.ijarsct.co.in","knowledge_id":"882340a3-ace2-4974-a874-6f7b4f0fabef","title":"Paper22301.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/7d940a45-5656-432a-a026-1d58b94ca674/Paper22301.pdf"}
{"file_content":"Ileberi¬†et¬†al. Journal of Big Data            (2022) 9:24 https://doi.org/10.1186/s40537-022-00573-8 RESEARCH Open Access A machine learning based credit card fraud detection using the¬†GA algorithm for¬†feature selection Emmanuel Ileberi1*, Yanxia Sun1 and Zenghui Wang2 *Correspondence: emmanuelileberi@gmail.com Abstract 1 Department of Electrical The recent advances of e-commerce and e-payment systems have sparked an increase & Electronic Engineering Science, University in financial fraud cases such as credit card fraud. It is therefore crucial to implement of Johannesburg, Kingsway mechanisms that can detect the credit card fraud. Features of credit card frauds play Ave, 2006 Johannesburg, important role when machine learning is used for credit card fraud detection, and they South Africa Full list of author information must be chosen properly. This paper proposes a machine learning (ML) based credit is available at the end of the card fraud detection engine using the genetic algorithm (GA) for feature selection. article After the optimized features are chosen, the proposed detection engine uses the fol- lowing ML classifiers: Decision Tree (DT), Random Forest (RF), Logistic Regression (LR), Artificial Neural Network (ANN), and Naive Bayes (NB). To validate the performance, the proposed credit card fraud detection engine is evaluated using a dataset generated from European cardholders. The result demonstrated that our proposed approach outperforms existing systems. Keywords: Machine learning, Genetic algorithm, Fraud detection, Cybersecurity Introduction In the last decade, there has been an exponential growth of the Internet. This has sparked the proliferation and increase in the use of services such as e-commerce, tap and pay systems, online bills payment systems etc. As a consequence, fraudsters have also increased activities to attack transactions that are made using credit cards. There exists a number of mechanisms used to protect credit cards transactions including credit card data encryption and tokenization [1]. Although such methods are effective in most of the cases, they do not fully protect credit card transactions against fraud. Machine Learning (ML) is a sub-field of Artificial Intelligence (AI) that allows com- puters to learn from previous experience (data) and to improve on their predictive abilities without explicitly being programmed to do so [2]. In this work we implement Machine Learning (ML) methods for credit card fraud detection. Credit card fraud is defined as a fraudulent transaction (payment) that is made using a credit or debit card by an unauthorised user [3]. According to the Federal Trade Commission (FTC), there were about 1579 data breaches amounting to 179 million data points whereby credit ¬© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article‚Äôs Creative Commons licence, unless indicated otherwise in a credit line to the mate- rial. If material is not included in the article‚Äôs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creativecommons.org/licenses/by/4.0/.\n\n\n\n---\n\nIleberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 2 of 17 card fraud activities were the most prevalent [4]. Therefore, it is crucial to implement an effective credit card fraud detection method that is able to protect users from financial loss. One of the key issues with applying ML approaches to the credit card fraud detection problem is that most of the published work are impossible to repro- duce. This is because credit card transactions are highly confidential. Therefore, the datasets that are used to develop ML models for credit card fraud detection contain anonymized attributes. Furthermore, credit card fraud detection is a challenging task because of the constantly changing nature and patterns of the fraudulent transactions [5]. Additionally, existing ML models for credit card fraud detection suffer from a low detection accuracy and are not able to solve the highly skewed nature of credit card fraud datasets. Therefore, it is essential to develop ML models that can perform opti- mally and that can detect credit card fraud with a high accuracy score. This research focuses on the application of the following supervised ML algorithms for credit card fraud detection: Decision Tree (DT) [7], Random Forest (RF) [8], Arti- ficial Neural Network (ANN) [12], Naive Bayes (NB) [11] and Logistic Regression (LR) [6]. ML systems are trained and tested using large datasets. In this work, a credit card fraud dataset generated from European credit cardholders is utilized. Often- times, these datasets may have many attributes that could have a negative impact on the performance of the classifiers during the training process. To solve the issue of a high feature dimension space, we implement a feature selection algorithm that is based on the Genetic Algorithm (GA) [25] using the RF method in its fitness func- tion. The RF method is used in the GA fitness function because it can handle a large number of input variables, it can automatically handle missing values, and because it is not affected by noisy data [9]. The reminder of this paper is structured as follows. The second section provides an overview of the classifiers that are used in this research. Section III provides a litera- ture review of similar work. Section IV provides the details of the dataset used in this research. Section V outlines the GA algorithm. Section VI. explains the architecture of the proposed system. We conduct the experiments in Section VII. The conclusion is presented in Section VIII. Classifiers Logistic regression The Logistic Regression (LR) classifier, sometimes referred to as the Logit classifier, is a supervised ML method that is generally used for binary classification tasks [6]. LR is a special type of linear regression whereby a linear function is fed to the logit function. y = Œ±0 + Œ±1X1 + Œ±2X2 + ¬∑ ¬∑ ¬∑ + Œ±nXn (1) 1 q = ‚àíy (2) 1+ e where the value of q will be between 0 and 1. q is the probability that determines the pre- diction of a given class. The closer q is to 1, the more accurately it predicts a particular class.\n\n\n\n---\n\nI leberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 3 of 17 Decision trees and¬†random forest Decision Tree (DT) is a supervised ML based approach that is utilized to solve regres- sion and classification tasks. A DT contains the following types of nodes: root node, decision node and leaf node. The root node is the starting point of the algorithm. The decision node is a point whereby a choice is made in order to split the tree. A leaf node represents a final decision [7]. The RF method conducts its predictions by using an ensemble of DTs [8]. In the RF, a decision is reached by majority vote. The follow- ing is a mathematical definition of the RF [10]: Given a number of trees k, a RF is defined as, RF = {g(X , Œ∏k)} , where {Œ∏k} represents independent identically distributed trees that cast a vote on input vector X. The label with the most votes is the prediction. Naive Bayes The Naive Bayes (NB) is a supervised ML technique that is based on Bayes‚Äô theorem. The NB method assumes the independence of each pair of attributes when provided with the dependant variable (the class). In this research, the Gaussian NB (GNB) clas- sifier was used. With the GNB, we assume that the probability of the attributes is Gaussian as explained in Equation (3). ( ) 1 (x 2 n ‚àí Œ≤y) P(xn|y) = ‚àö exp ‚àí\n\n## 2 2 (3)\n\n2 2 Œ± œÄŒ±y y where Œ≤y and Œ±y are computed using the maximum probability. Artificial Neural Network Artificial Neural Network (ANN) is a supervised ML method that is inspired from the inner workings of the human brain. The simplest ANN have the following basic structure: an input layer, one hidden layer and an output layer. The input layer size is based on the number of features in a given dataset. The hidden layer size can be var- ied based on the complexity of a task and the output layer size depends on the type of problems to be solved. The most basic component of an ANN is a node or neuron. In this research, we consider feed forward ANNs. Therefore, the information flows in one direction (from its input to its output) through a neuron [12]. Figure¬†1 depicts a graphical representation of a simple ANN with 3 nodes in the input layer, a hidden layer with 4 nodes and an output layer with 1 node. Related work In ref. [13], the authors implemented a credit card fraud detection system using sev- eral ML algorithms including logistic regression (LR), decision tree (DT), support vector machine (SVM) and random forest (RF). These classifiers were evaluated using a credit card fraud detection dataset generated from European cardholders in 2013. In this dataset, the ratio between non-fraudulent and fraudulent transactions is highly skewed; therefore, this is a highly imbalanced dataset. The researcher used the clas- sification accuracy to assess the performance of each ML approach. The experimental\n\n\n\n---\n\nIleberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 4 of 17 Fig. 1 ANN outcomes showed that the LR, DT, SVM and RF obtained the following accuracy scores: 97.70%, 95.50%, 97.50% and 98.60%, respectively. Although these outcomes are good, the authors suggested that the implementation of advanced pre-processing techniques could have a positive impact on the performance of the classifiers. Varmedja et¬† al. [14] proposed a credit card fraud detection method using ML The authors used a credit card fraud dataset sourced from Kaggle [19]. This dataset contains transactions made within 2 days by European credit card holders. To deal with the class imbalance problem present in the dataset, the researcher implemented the Synthetic Minority Oversampling Technique (SMOTE) oversampling technique. The following ML methods were implemented to assess the efficacy of the proposed method: RF, NB, and multilayer perceptron (MLP). The experimental results demonstrated that the RF algorithm performed optimally with a fraud detection accuracy of 99.96%. The NB and the MLP methods obtained accuracy scores of 99.23% and 99.93%, respectively. The authors concede that more research should be conducted to implement a feature selec- tion method that could improve on the accuracy of other ML methods. Khatri et¬†al. [15] conducted a performance analysis of ML techniques for credit card fraud detection. In this research, the authors considered the following ML approaches: DT, k-Nearest Neighbor (KNN), LR, RF and NB. To assess the performance of each ML method, the authors used a highly imbalanced dataset that was generated from Euro- pean cardholders. One of the main performance metric that was used in the experi- ments is the precision which was obtained by each classifier. The experimental outcomes showed that the DT, KNN, LR, and RF obtained precisions of 85.11%, 91.11%, 87.5%, 89.77%, 6.52%, respectively. Awoyemi et¬†al. [16] presented a comparison analysis of different ML methods on the European cardholders credit card fraud dataset. In this research, the authors used an hybrid sampling technique to deal with the imbalanced nature of the dataset. The fol- lowing ML were considered: NB, KNN, and LR. The experiments were carried out using a Python based ML framework. The accuracy was the main performance metric that was utilized to assess the effectiveness of each ML approach. The experimental results demonstrated that the NB, LR,and KNN achieved the following accuracies, respectively: 97.92%, 54.86%, and 97.69%. Although the NB and KNN performed relatively well, the authors did not explore the possibility to implement a feature selection method.\n\n\n\n---\n\nI leberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 5 of 17 In ref. [4] the authors utilized several ML learning based methods to solve the issue of credit card fraud. In this work, the researchers used the European credit cardholder fraud dataset. To deal with the highly imbalanced nature of this dataset, the authors employed the SMOTE sampling technique. The following ML methods were considered: DT, LR, and Isolation Forest (IF). The accuracy was one of the main performance met- rics that was considered. The results showed that the DT, LR, and IF obtained the accu- racy scores of 97.08%, 97.18%, and 58.83%, respectively. Manjeevan et¬† al. [17] implemented an intelligent payment card fraud detection sys- tem using the GA for feature selection and aggregation. The authors implemented sev- eral machine learning algorithms to validate the effectiveness of their proposed method. The results demonstrated that the GA-RF obtained an accuracy of 77.95%, the GA-ANN achieved an accuracy of 81.82%, and the GA-DT attained an accuracy of 81.97%. Research methodology Dataset In this research, we use a dataset that includes credit card transactions that were made by European cardholders for 2 days in September 2013. This dataset contains 284807 transactions in total in which 0.172% of the transactions are fraudulent. The dataset has the following 30 features (V1,.., V28), Time and Amount. All the attributes within the dataset are numerical. The last column represents the class (type of transaction) whereby the value of 1 denotes a fraudulent transaction and the value of 0 otherwise. The features V1 to V28 are not named for data security and integrity reasons [19]. This dataset has been used in ref. [4, 13, 14, 16] and one of the key issues that we discovered is the low detection accuracy score that was obtained by those models because of the highly imbal- anced nature of the dataset. In order to solve the issue of class imbalance, we applied the Synthetic Minority Oversampling Technique (SMOTE) method in the Data-Preprocess- ing phase of the proposed framework in Fig.¬†5 [18]. The SMOTE method works by pick- ing samples that are close to each other within the feature space, drawing a line between the data points in the feature space and creating a new instance of the minority class at a point along the line. Feature selection Feature selection (FS) is a crucial step when implementing machine learning meth- ods. This is partly because the dataset used during the training and testing processes may have a large feature space that may negatively impact the overall performance of the models. The choice of which FS method to use depends on the kind of problem a researcher is trying to solve. The following paragraph provides an overview of instances where using a FS method improved on the performance of ML models. Kasongo [20] implemented a GA-based FS in order to increase the performance of ML based models applied to the domain of intrusion detection systems. The results demonstrated that the application of GA improved the performance of the RF clas- sifier with an Area Under the Curve (AUC) of 0.98. Mienye [21] et¬†al. implemented a particle swarm optimization (PSO) technique to increase the performance of stacked sparse autoencoder network (SSAE) coupled with the softmax unit for heart disease prediction. The PSO technique was used to improve the feature learning capability\n\n\n\n---\n\nIleberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 6 of 17 of SSAE by optimally tuning its parameters. The results demonstrated that the PSO- SSAE achieved an accuracy of 97.3% on the Framingham heart disease dataset. Hema- vathi et¬† al. [22] implemented an effective FS method in an integrated environment using enhanced principal component analysis (EPCA). The results demonstrated that using the EPCA yields optimal results in supervised and unsupervised environments. Pouramirarsalani et¬† al. [23] implemented a FS method using hybrid FS and GA for fraud detection in an e-banking environment. The experimental results demonstrated that using a FS method on a financial fraud datasets has a positive impact on the over- all performance of the models that were used. In ref. [24], the authors implemented the GA-based FS method in conjunction with NB, SVM and RF algorithms for credit card fraud detection. The experimental output demonstrated that the RF yielded a better performance in comparison to the NB and SVM. Genetic algorithm feature selection The Genetic Algorithm (GA) is a type of Evolutionary inspired Algorithm (EA) that is often used to solve a number of optimization tasks with a reduced computational overhead. EAs generally possess the following attributes [25, 26]:\n\n- Population EAs approaches maintain a sample of possible solutions called popu-\nlation.\n\n- Fitness A solution within the population is called an individual. Each individual is\ncharacterized by a gene representation and a fitness measure.\n\n- Variation The individual evolves through mutations that are inspired from the\nbiological gene evolution. In this study, the RF approach is used as the fitness method inside the GA. Further, the RF method is employed because it resolves the problem of over-fitting that is gen- erally encountered when using regular Decision Trees (DTs). Moreover, RF performs well with both continuous and categorical attributes and RF are known to perform optimally on datasets that have a class imbalance problem. Additionally, the RF is a rule-based approach; therefore, the normalising of data is not required [27]. The alter- native to the RF include tree-based ML algorithms such as Extra-Trees and Extreme Gradient Boosting [28, 29]. The fitness method is defined a function that receives a candidate solution (a feature vector) and determines whether it is fit or not. The measure of fitness is determined by the accuracy that is yielded by a particular attrib- ute vector in the testing process of the RF method within the GA. Algorithm¬†1 pro- vides more details about the implementation of RF in the GA. Algorithm¬†1 denotes the pseudo code implementation of the fitness function that was used in the GA. This algorithm consists of 6 main steps. In step 1, the data (20% of the full Credit Card Fraud dataset) is divided into a training ( Ftrain and ytrain ) and testing ( Ftest and ytest ) subsets. In Step 2, an instance of the RF classifier is instanti- ated. In Step 3, the RF instance is trained using the training set. In Step 4, the result- ing model is then evaluated using the testing data ytest . In Step 5, the predictions are stored in ypred . In the last step, the evaluation process is conducted using ypred .\n\n\n\n---\n\nI leberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 7 of 17 During the evaluation procedure, the accuracy is used as the main performance met- ric. The most optimal model is one that yields the highest accuracy score. Algorithm¬†2 is a pseudo code that represents the computation process of a candi- date feature vector. In the initialization phase, the clean Credit Card Fraud dataset is loaded. In the second phase, we define all the variables that will be used in the com- putation procedure of a candidate feature vector. This includes the following: a list, A, that will store the names of all the features that are present in the Credit Card Fraud dataset; y represents the target variable; B denotes an empty array that will store the most optimal feature names. k represents the total number of iterations required to compute a candidate feature vector. Once the definition phase is completed; in Step 1, we generate the initial population (feature names) and store them in A. In Step 2 and Step 3, Algorithm¬†2 is computed. The fitness value, q is generated in Step 4. q deter- mines whether a candidate feature vector is optimal or not. If a candidate feature vec- tor is not optimal; we compute the crossover (k-point crossover, where k = 1 ), the mutation, the fitness (from Step 6 to Step 10). This process is conducted iteratively till the algorithm converges. The convergence point is decided once the maximum accu- racy has been reached over k iterations. The main steps of the GA that was adapted to our case study are depicted in Fig.¬†2. This flowchart represents the compact version of the implementation of the pseudo code in Algorithm¬†1 and Algorithm¬†2 [30].\n\n\n\n---\n\nIleberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 8 of 17 Fig. 2 GA flowchart Table 1 GA Selected features Attribute vector Vector length Attribute list v1 18 V1, V5, V7, V8, V11,V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, Amount v2 9 V1, V6, V13, V16, V17, V22, V23, V28, Amount v3 13 V2, V11, V12, V13, V15, V16, V17, V18,V20, V21, V24, V26, Amount v4 9 V2, V7, V10, V13,V15, V17, V19, V28, Amount v5 13 Time, V1, V7, V8, V9, V11, V12, V14, V15, V22, V27, V28, Amount After the implementation of the GA (Algorithm¬† 1 and Algorithm¬† 2) on the credit card fraud dataset, we obtained the 5 optimal feature vectors ( v1 to v5 ) that are shown in Table¬†1. These vectors contain the feature names that represents the most optimal attrib- utes that will be used to assess the effectiveness of our proposed method. Fraud detection framework The architecture of the proposed methodology is depicted in Fig.¬†3. The initial step is computed in the Normalize Inputs block whereby the training dataset is normalized using the min-max scaling method in Equation (4) [31]. The scaling process is done to ensure that all the input values are within a predefined range. The GA algorithm is implemented in the GA Feature Selection block using the normalized data from the Normalize Inputs block. At each iteration of the GA Feature Selection block, the GA generates a candidate attribute vector vn that is used to train the models in the Train- ing block represented by the Training data and Train the models blocks. The same\n\n\n\n---\n\nI leberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 9 of 17 Fig. 3 Architecture of the proposed framework vector is also used to test the trained models using the test data. The testing process is conducted using the Trained Model block using the Test Data. For a given model, the testing process is conducted for each vn until the desired results are obtained. f ‚àímin(f ) fs = (4) max(f )‚àímin(f ) where f is a feature in the dataset. Performance metrics The research presented in this paper is modeled as a ML binary classification task. Therefore, we use the accuracy (AC) that was obtained on the test data as the main performance metric. Additionally, for each model, we compute the recall (RC), the precision (PR) and the F1-Score (F-Measure) [32]. To assess the classification quality of each model, we further plot the Area Under the Curve (AUC). The AUC is a metric that reveals how effective a classifier is for a given classification task. The value of the AUC varies between 0 and 1 whereby an efficient classifier would have an AUC value close to 1 [33].\n\n- True positive (TP): attacks/intrusions that are accurately flagged as attacks.\n- True Negative (TN): normal traffic patterns/traces that are successfully catego-\nrized as normal.\n\n- False positive (FP): legitimate network traces that are incorrectly labeled as intru-\nsive.\n\n- False Negative (FN): attacks/intrusions that are incorrectly classified as non-intru-\nsive.\n\n\n\n---\n\nIleberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 10 of 17 Table 2 Classification results for v1 Model Accuracy Recall Precision F1-Score\n\n### RF 99.94 % 76.99 % 89.69 % 82.85%\n\n### DT 99.92 % 75.22 % 75.22 % 75.22%\n\n### ANN 99.94 % 77.87 % 84.61 % 81.10%\n\n### NB 98.13 % 84.95 % 6.83 % 12.65%\n\n### LR 99.91 % 57.52 % 82.27 % 67.70 %\n\nTable 3 Classification results for v2 Model Accuracy Recall Precision F1-Score\n\n### RF 99.93 % 76.10 % 82.69 % 79.26 %\n\n### DT 99.87 % 68.14 % 60.62 % 64.16 %\n\n### ANN 99.91 % 66.37 % 76.53 % 71.09 %\n\n### NB 98.65 % 77.87 % 8.59 % 15.47 %\n\n### LR 99.89 % 47.78 % 79.41 % 59.66 %\n\n## TN + TP\n\n## AC = (5)\n\n## TP + TN + FP + FN\n\nTP\n\n## RC = (6)\n\n## FN + TP\n\nTP\n\n## PR = (7)\n\n## FP + TP\n\n## PR.RC\n\nF1score =2 (8)\n\n## PR+ RC\n\nExperiments Experimental configuration The experimental processes were conducted on Google Colab [34]. The compute speci- fications are as follows: Intel(R) Xeon(R), 2.30GHz, 2 Cores. The ML framework used in this research is the Scikit-Learn [35]. Results and¬†discussions The experiments were carried out in two folds. In the first step, a classification process was conducted using F = {v1, v2, v3, v4, v5} . For each feature vector in F, the following methods were trained and tested: RF, DT, ANN, NB and LR. The results are depicted in Tables¬†2, 3, 4, 5, 6. As shown in Table¬†2, both the ANN and the RF algorithms obtained the highest test accuracy (TAC) of 99.94% using v1 . However, the RF method obtained the best results in terms of precision. In Table¬†3, the results that were obtained using v2 demonstrate that the best model is the RF approach with an accuracy of 99.93%. In Table¬†4, the RF method also obtained the best fraud detection accuracy of 99.94% using\n\n\n\n---\n\nI leberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 11 of 17 Table 4 Classification results for v3 Model Accuracy Recall Precision F1-Score\n\n### RF 99.94 % 75.22 % 85.85 % 80.18 %\n\n### DT 99.90 % 76.10 % 68.80 % 72.26 %\n\n### ANN 99.91 % 67.25 % 77.55 % 72.03 %\n\n### NB 98.81 % 81.41 % 10.07 % 17.93 %\n\n### LR 99.90 % 53.09 % 80.00 % 63.82 %\n\nTable 5 Classification results for v4 Model Accuracy Recall Precision F1-Score\n\n### RF 99.94 % 77.87 % 83.80 % 80.73 %\n\n### DT 99.91 % 76.10 % 72.26 % 74.13 %\n\n### ANN 99.91 % 61.06 % 81.17 % 69.69 %\n\n### NB 98.48 % 81.41 % 7.97 % 14.53 %\n\n### LR 99.89 % 46.90 % 77.94 % 58.56 %\n\nTable 6 Classification results for v5 Model Accuracy Recall Precision F1-Score\n\n### RF 99.98 % 72.56 % 95.34 % 82.41 %\n\n### DT 99.89 % 72.56 % 65.07 % 68.61 %\n\n### ANN 99.08 % 77.87 % 12.27 % 21.20 %\n\n### NB 99.44 % 57.52 % 15.85 % 24.85 %\n\n### LR 99.77 % 46.90 % 34.64 % 39.84 %\n\nTable 7 Classification results for full feature vector Model Accuracy Recall Precision F1-Score\n\n### RF 87.95 % 77.87 % 92.63 % 84.61%\n\n### DT 96.91 % 76.10 % 71.07 % 73.50%\n\n### ANN 97.80 % 74.33 % 42.85 % 54.36%\n\n### NB 80.31 % 64.60 % 13.95 % 22.95%\n\n### LR 93.88 % 60.17 % 62.96 % 61.53 %\n\nv3 . Table¬† 5 presents the results that were achieved by v4 whereby the DT obtained an accuracy of 99.1% and a precision of 81.17%. Table¬† 6 depicts the outcomes that were obtained when using v5 . In this case, the RF attained a fraud detection accuracy of 99.98% and precision of 95.34%. In comparison to the results obtained by v1 , v2 , v3 and v4 ; v5 obtained the best results. Moreover, looking at the outcomes presented in Tables¬†2, 3, 4, 5, 6, the NB method under performed in terms of Recall, Precision and F1-Score. As an initial validation of the proposed method, we ran further experiments using the full feature vector and a feature vector that was generated using a random approach ran- dom_vec = { V2, V3, V4, V5, V6, V7, V8, V9, V11, V12, V13, V16, V17, V18, V19, V20, V21, V22, V23, V25, V26, V28, Amount}. The result are listed in Tables¬†7 and 8. In both\n\n\n\n---\n\nIleberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 12 of 17 Table 8 Classification results a random approach Model Accuracy Recall Precision F1-Score\n\n### RF 83.78 % 79.64 % 92.78 % 85.71%\n\n### DT 89.91 % 79.64 % 68.70 % 73.77%\n\n### ANN 88.93 % 78.76 % 82.40 % 80.54%\n\n### NB 78.14 % 83.18 % 6.73 % 12.46%\n\n### LR 79.91 % 59.29% 81.70 % 68.71 %\n\ninstances, we observed serve drop in the performance our the models in comparison to the models that were coupled with the GA (Tables¬†2, 3, 4, 5, 6). Furthermore, we computed the AUC of each vector in F. These results are depicted in Figs.¬† 4, 5, 6, 7, 8. In Fig.¬†4 ( v1 ), the best performing models in terms of the quality of classification are the RF, NB, and LR with the AUCs of 0.96, 0.97, and 0.97, respec- tively. In the instance of v5 (Fig¬†8), the RF and NB obtained the highest AUCs of 0.95 and 0.96. Moreover, a comparison analysis is presented in Table¬†7. This comparison reveals that the GA feature selection approach presented in this paper as well as most of the proposed ML methods that were implemented outperformed the existing techniques that are proposed in [4, 13, 14, 16].For instance, the GA-RF proposed in this research obtained an accuracy that is 2.28% higher than the LR in [13]. The GA-DT proposed in this work yielded a fraud detection accuracy that is 4.42% higher than the DT model presented in [14]. The GA-LR obtained an accuracy that is 2.41% higher than the SVM model presented in [13]. The GA-NB proposed in this research achieved an accuracy that is 1.75% higher than the KNN model proposed in [16]. Additionally, the GA-DT presented in this research achieved an accuracy that is 17.23% greater than the accuracy obtained in [17]. In terms of classification accuracy, the most optimal classifier is the RF (implemented with v5 ). This model achieved a noteworthy credit card fraud detection accuracy of 99.98%. Experiments on¬†synthetic dataset To validate the efficiency of our proposed method, we conducted more experiments using a publicly available synthetic dataset that contains the following features: V = { User, Card, Year, Month, Day, Time, Amount, Use Chip, Merchant Name, Merchant City, Merchant State, Zip, MCC, Errors, Is Fraud} , where Is Fraud denotes the target variable. This dataset contained 24357143 legitimate credit card transactions and 29757 fraudulent ones [36]. In the experiments, we considered the following methods: RF, DT, ANN, NB, and LR. We first processed the dataset through the framework in Fig.¬†5. The GA module selected the features represented by v0 in Table¬†8. These were the features that were used during the training and testing processes of the ML models. Table¬†9 pro- vides the details of the results that were obtained after the experiments converged. The GA-ANN and the GA-DT achieved accuracies of 100%. These results are backed by AUCs of 0.94 and 1, respectively. The other models that performed remarkably well are the GA-RF and the GA-LR with accuracies of 99.95% and 99.96%. However, the GA-LR yielded a low AUC of 0.63 (Table¬†10).\n\n\n\n---\n\nI leberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 13 of 17 Table 9 Comparison with existing methods Model Accuracy\n\n## LR [13] 97.70 %\n\n## DT [13] 95.50 %\n\n## SVM [13] 97.50 %\n\n## NB [14] 99.23 %\n\n## KNN [16] 97.69 %\n\n## LR [16] 54.86 %\n\n## DT [4] 97.08 %\n\n## LR [17] 97.18 %\n\n## IF [16] 58.83 %\n\n## GA-ANN [17] 81.82 %\n\n## GA-DT [17] 81.97 %\n\n## GA-RF [17] 77.95 %\n\nGA-RF (Proposed v5) 99.98 % GA-DT (Proposed v1) 99.92 % GA-LR (Proposed v1) 99.91 % GA-NB (Proposed v5) 99.44 % Fig. 4 AUC results for v1 Fig. 5 AUC results for v2\n\n\n\n---\n\nIleberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 14 of 17 Fig. 6 AUC results for v3 Fig. 7 AUC results for v4 Fig. 8 AUC results for v5 Moreover, Fig.¬†7 depicts the ROC curves of the ML models that were considered in the experiments. The result demonstrated that the RF and the DT models achieved an AUC of 1. This indicates that models were perfect at detecting fraudulent activities (Table¬†11).\n\n\n\n---\n\nI leberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 15 of 17 Table 10 GA Selected features‚Äîsynthetic dataset Attribute vector Vector length Attribute list GA selected feature space, v0 7 Card, Year, Month, Day, Amount, Zip, MCC Table 11 Classification results for v0 in Table 8 Model Accuracy Recall Precision F1-Score\n\n### RF 99.95 % 99.82 % 99.92 % 99.82 %\n\n### DT 100 % 99.71 % 99.51 % 99.61 %\n\n### ANN 100 % 72.09 % 84.31 % 77.72 %\n\n### NB 99.10 % 96.29 % 84.47 % 41.52 %\n\n### LR 99.96 % 99.12 % 80.68 % 88.95 %\n\nConclusion In this research, a GA based feature selection method in conjunction with the RF, DT, ANN, NB, and LR was proposed. The GA was implemented with the RF in its fit- ness function. The GA was further applied to the European cardholders credit card transactions dataset and 5 optimal feature vectors were generated. The experimen- tal results that were achieved using the GA selected attributes demonstrated that the GA-RF (using v5 ) achieved an overall optimal accuracy of 99.98%. Furthermore, other classifiers such as the GA-DT achieved a remarkable accuracy of 99.92% using v1 . The results obtained in this research were superior to those achieved by existing meth- ods. Moreover, we implemented our proposed framework on a synthetic credit card fraud dataset to validate the results that were obtained on the European credit card fraud dataset. The experimental outcomes showed that the GA-DT obtained an AUC of 1 and an accuracy of 100%. Seconded by the GA-ANN with an AUC of 0.94 and an accuracy of 100%. In future works, we intend to use more datasets to validate our framework. Authors‚Äô contributions Ileberi Emmanuel wrote the algorithms and methods related to this research and he interpreted the results. Y. Sun and Z. Wang provided guidance in terms of validating the obtained results. All authors read and approved the final manuscript. Authors‚Äô information Yanxia Sun got her joint qualification: D-Tech in Electrical Engineering, Tshwane University of Technology, South Africa and PhD in Computer Science, University Paris-EST, France in 2012. Yanxia Sun is currently working as Professor is the Department of Electrical and Electronic Engineering Science, University of Johannesburg, South Africa. She has 15 years teaching and research experience. She has lectured five courses in the universities. She has supervised or co-supervised five postgraduate projects to completion. Currently she is supervising six PhD students and four master students. She published 42 papers including 14 ISI master indexed journal papers. She is the investigator or co-investigator for six research projects. She is the member of the South African Young Academy of Science (SAYAS). Here research interests include Renewable Energy, Evolutionary Optimization, Neural Network, Nonlinear Dynamics and Control Systems. Zenghui Wang, a Professor in Department of Electrical Engineering, University of South Africa. Funding This research is funded by the University of Johannesburg, South Africa. Availability of data and materials The datasets used during the current study are available a Kaggle, https:// www. kaggle. com/ mlg- ulb/ credi tcard fraud. Synthetic Credit Card Fraud Dataset, https:// ibm. ent. box. com/v/ tabfo rmer- data/ folder/ 13074 77156 05.\n\n\n\n---\n\nIleberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 16 of 17 Declarations Competing interests The authors declare that they have no competing interests Author details 1 Department of Electrical & Electronic Engineering Science, University of Johannesburg, Kingsway Ave, 2006 Johannes- burg, South Africa. 2 Department of Electrical Engineering, University of South Africa, Florida, 1709 Johannesburg, South Africa. Received: 30 July 2021   Accepted: 6 February 2022 References\n\n1. Iwasokun GB, Omomule TG, Akinyede RO. Encryption and tokenization-based system for credit card information\nsecurity. Int J Cyber Sec Digital Forensics. 2018;7(3):283‚Äì93.\n\n2. Burkov A. The hundred-page machine learning book. 2019;1:3‚Äì5.\n3. Maniraj SP, Saini A, Ahmed S, Sarkar D. Credit card fraud detection using machine learning and data science. Int J\nEng Res 2019; 8(09).\n\n4. Dornadula VN, Geetha S. Credit card fraud detection using machine learning algorithms. Proc Comput Sci.\n2019;165:631‚Äì41.\n\n5. Thennakoon, Anuruddha, et al. Real-time credit card fraud detection using machine learning. In: 2019 9th interna-\ntional conference on cloud computing, data science & engineering (Confluence). IEEE; 2019.\n\n6. Robles-Velasco A, Cort√©s P, Mu√±uzuri J, Onieva L. Prediction of pipe failures in water supply networks using logistic\nregression and support vector classification. Reliab Eng Syst Saf. 2020;196:106754.\n\n7. Liang J, Qin Z, Xiao S, Ou L, Lin X. Efficient and secure decision tree classification for cloud-assisted online diagnosis\nservices. IEEE Trans Dependable Secure Comput. 2019;18(4):1632‚Äì44.\n\n8. Ghiasi MM, Zendehboudi S. Application of decision tree-based ensemble learning in the classification of breast\ncancer. Comput in Biology and Medicine. 2021;128:104089.\n\n9. Lingjun H, Levine RA, Fan J, Beemer J, Stronach J. Random forest as a predictive analytics alternative to regression in\ninstitutional research. Pract Assess Res Eval. 2020;23(1):1.\n\n10. Breiman L. Random forests. Mach Learn. 2001;45(1):5‚Äì32.\n11. Ning B, Junwei W, Feng H. Spam message classification based on the Naive Bayes classification algorithm. IAENG Int\nJ Comput Sci. 2019;46(1):46‚Äì53.\n\n12. Katare D, El-Sharkawy M. Embedded system enabled vehicle collision detection: an ANN classifier. In: 2019 IEEE 9th\nAnnual Computing and Communication Workshop and Conference (CCWC); 2019. p. 0284‚Äì0289.\n\n13. Campus K. Credit card fraud detection using machine learning models and collating machine learning models. Int J\nPure Appl Math. 2018;118(20):825‚Äì38.\n\n14. Varmedja D, Karanovic M, Sladojevic S, Arsenovic M, Anderla A. Credit card fraud detection-machine learning meth-\nods. In: 18th international symposium INFOTEH-JAHORINA (INFOTEH); 2019. p. 1-5.\n\n15. Khatri S, Arora A, Agrawal AP. Supervised machine learning algorithms for credit card fraud detection: a comparison.\nIn: 10th international conference on cloud computing, data science & engineering (Confluence); 2020. p. 680-683.\n\n16. Awoyemi JO, Adetunmbi AO, Oluwadare SA. Credit card fraud detection using machine learning techniques: a\ncomparative analysis. In: International conference on computer networks and Information (ICCNI); 2017. p. 1-9.\n\n17. Seera M, Lim CP, Kumar A, Dhamotharan L, Tan KH. An intelligent payment card fraud detection system. Ann Oper\nRes 2021;1‚Äì23.\n\n18. Guo S, Liu Y, Chen R, Sun X, Wang X. X, Improved SMOTE algorithm to deal with imbalanced activity classes in smart\nhomes. Neural Process Lett. 2019;50(2):1503‚Äì26.\n\n19. The Credit card fraud [Online]. https:// www. kaggle. com/ mlg- ulb/ credi tcard fraud\n20. Kasongo SM. An advanced intrusion detection system for IIoT based on GA and tree based algorithms. IEEE Access.\n2021;9:113199‚Äì212.\n\n21. Mienye ID, Sun Y. Improved heart disease prediction using particle swarm optimization based stacked sparse\nautoencoder. Electronics. 2021;10(19):2347.\n\n22. Hemavathi D, Srimathi H. Effective feature selection technique in an integrated environment using enhanced princi-\npal component analysis. J Ambient Intell Hum Comput. 2021;12(3):3679‚Äì88.\n\n23. Pouramirarsalani A, Khalilian M, Nikravanshalmani A. Fraud detection in E-banking by using the hybrid feature selec-\ntion and evolutionary algorithms. Int J Comput Sci Netw Secur. 2017;17(8):271‚Äì9.\n\n24. Saheed YK, Hambali MA, Arowolo MO, Olasupo YA. Application of GA feature selection on Naive Bayes, random for-\nest and SVM for credit card fraud detection. In: 2020 international conference on decision aid sciences and applica- tion (DASA); 2020. p. 1091‚Äì1097.\n\n25. Davis L. Handbook of genetic algorithms; 1991.\n26. Li Y, Jia M, Han X, Bai XS. Towards a comprehensive optimization of engine efficiency and emissions by coupling\nartificial neural network (ANN) with genetic algorithm (GA). Energy. 2021;225:120331.\n\n27. Khalilia M, Chakraborty S, Popescu M. Predicting disease risks from highly imbalanced data using random forest.\nBMC Med Inf Decis Mak. 2011;11(1):1‚Äì13.\n\n28. Abhishek L. Optical character recognition using ensemble of SVM, MLP and extra trees classifier. In: International\nconference for emerging technology (INCET) IEEE; 2020. p. 1‚Äì4.\n\n29. Chen T, He T, Benesty M, Khotilovich V, Tang Y, Cho H. Xgboost: extreme gradient boosting. R package version 04-2.\n2015;1(4):1‚Äì4.\n\n\n\n---\n\nI leberi¬†et¬†al. Journal of Big Data            (2022) 9:24 Page 17 of 17\n\n30. Harik GR, Lobo FG, Goldberg DE. The compact genetic algorithm. IEEE Trans Evol Comput. 1999;3(4):287‚Äì97.\n31. Jain A, Nandakumar K, Ross A. Score normalization in multimodal biometric systems. Pattern Recognit.\n2005;38(12):2270‚Äì85.\n\n32. Kasongo SM, Sun Y. A deep long short-term memory based classifier for wireless intrusion detection system. ICT\nExpress. 2020;6(2):98‚Äì103.\n\n33. Norton M, Uryasev S. Maximization of auc and buffered auc in binary classification. Math Program.\n2019;174(1):575‚Äì612.\n\n34. Google Colab [Online]. Available: https:// colab. resea rch. google. com/\n35. Scikit-learn : machine learning in Python [Online]. https:// scikit- learn. org/ stable/\n36. Altman ER. Synthesizing credit card transactions. 2019. arXiv preprint arXiv: 1910. 03033\nPublisher‚Äôs Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.","knowledge_id":"dfa5d7ad-6433-4ca4-8ff6-9695c41f37b1","title":"s40537-022-00573-8.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/2203bdcf-c44e-48d7-9032-462c70debab8/s40537-022-00573-8.pdf"}
{"file_content":"Human-Centric Intelligent Systems (2022) 2:55‚Äì68 https://doi.org/10.1007/s44230-022-00004-0\n\n## REVIEW ARTICLE\n\nReview of¬†Machine Learning Approach on¬†Credit Card Fraud Detection Rejwan¬†Bin¬†Sulaiman1 ¬†¬∑ Vitaly¬†Schetinin1¬†¬∑ Paul¬†Sant1 Received: 25 November 2021 / Accepted: 28 March 2022 / Published online: 5 May 2022 ¬© The Author(s) 2022 Abstract Massive usage of credit cards has caused an escalation of fraud. Usage of credit cards has resulted in the growth of online business advancement and ease of the e-payment system. The use of machine learning (methods) are adapted on a larger scale to detect and prevent fraud. ML algorithms play an essential role in analysing customer data. In this research article, we have conducted a comparative analysis of the literature review considering the ML techniques for credit card fraud detec- tion (CCFD) and data confidentiality. In the end, we have proposed a hybrid solution, using the neural network (ANN) in a federated learning framework. It has been observed as an effective solution for achieving higher accuracy in CCFD while ensuring privacy. Keywords Artificial neural network (ANN)¬†¬∑ Credit card fraud¬†¬∑ Federated learning¬†¬∑ Random forest (RF) method¬†¬∑ Support vector machine (SVM)¬†¬∑ Privacy-preserving¬†¬∑ Blockchain 1 Introduction Visa issued 1131 million, whereas master card issued 1156 million cards worldwide. These statistics show how the In the twenty-first century, most financial institutions have usage of card-based transactions became easy and famous increasingly made business facilities available for the pub- to the end-users. Fraudsters pave their way to manipulate lic through internet banking. E-payment methods play an this group of people due to the massive portion of global imperative role in today's competitive financial society. They transactions falling in this category. And perhaps sometimes have made purchasing goods and services very convenient. it is easy to social engineer humans easily. Financial institutions often provide customers with cards Despite the several benefits that credit cards provide to that make their lives convenient as they go shopping with- consumers, they are also associated with problems such as out carrying cash. Other than debit cards the credit cards security and fraud. Credit card fraud is considered a chal- are also beneficial to consumers because it protects them lenge which banks and financial institutions are facing. It against purchased goods that might be damaged, lost or even occurs when unapproved individuals use credit cards for stolen. Customers are required to verify the transaction with gaining money or property using fraudulent means. Credit the merchant before carrying out any transaction using their card information is sensitive to be stolen via online plat- credit card. forms and web pages that are unsecured. They can also be According to statistics, Visa [50] and Mastercard [51] obtained from identity theft schemes. Fraudsters can access issued 2287 million total credit cards during 2020 (4th quar- the credit and debit card numbers of users illegitimately ter) worldwide (Figs.¬†1 and 2). without their consent and knowledge. According to ‚ÄúU.K. finance‚Äù [27], fraudulent activities associated with credit and debit cards have proven to be one of the major causes of financial losses in the finance indus-\n\n- Rejwan Bin Sulaiman\nrejwan.binsulaiman@study.beds.ac.uk try. Due to the advancement of technology, it is big threat Vitaly Schetinin that leads to massive loss of finances globally. Therefore, vitaly.schetinin@beds.ac.uk it is imperative to carry out credit card fraud detection to Paul Sant reduce financial losses. paul.sant@beds.ac.uk Machine learning is effective in determining which trans- actions are fraudulent and those that are legitimate. One of 1 University of¬†Bedfordshire, Luton, UK the main challenges associated with detection techniques is Vol.:(012 3456789)\n\n\n\n---\n\n5 6 Human-Centric Intelligent Systems (2022) 2:55‚Äì68 Fig. 1  Amount of Master credit card issued worldwide [51] Fig. 2  Amount of Visa credit issued worldwide [50] the barrier to exchanging ideas related to fraud detection. used to detect fraudulent credit card transactions and According to a study by ‚ÄúU.K. finance‚Äù, the number of credit finally proposes a better technique for credit card fraud. and debit fraud cases reported in the U.K. worth ¬£574.2 mil- Researchers are trying to solve some methodological bar- lion in 2020 [27]. riers that pose a limitation in ML real-time application. In recent years, fraud detection in credit card has Various research has been done in different domains such increased tremendously, drawing the attention of most as abnormal patterns detection [28‚Äì35], biometric identi- scholars and researchers [22]. This research paper seeks fication [36, 37], Diabetes Prediction [38, 39], Happiness to review and evaluate various aspects of credit and debit prediction [40], Water quality prediction [41], accident fraud detection. The paper examines various techniques prevention at Heathrow [42], timely diagnosis of bone 1 3\n\n\n\n---\n\nHuman-Centric Intelligent Systems (2022) 2:55‚Äì68 57 diseases [43], Predicting informational efficiency using 2.1  Random Forest (R.F.) deep Neural Network [49]. Despite these limitations, researchers are working to gain the ML power to detect Random forest is an algorithm based on ML which is con- frauds. structed from a decision tree (DT) algorithm, commonly used to resolve various regression and classification prob- lems. It helps in predicting output with high accuracy in 1.1  Motivation large datasets. The Random Forest technique combines several classifiers to provide a solution to different intricate CCFD involves quite complex procedures and techniques issues. The random forest helps in predicting the average for developing an effective detection system. Following are mean of output from other trees. An increase in the number some of the problems in CCFD that I have analyzed from of trees tends to increase the precision of the outcome. The the literature review, and it has motivated me to propose random forest method helps in eradicating various limita- an effective solution to the problems. tions of a decision tree algorithm [8]. It also minimizes the Credit card transactions are substantial in number and lifting of datasets and thus increasing precision. Several are heterogeneous. The users use the credit cards for vari- decision trees exist in a forest whereby a individual tree act ous purposes based on geographical locations and cur- as weak-learner; however, they together form strong learner. rencies, which shows that the fraudulent transactions are The RF technique is high-speed and effective in handling widely diverse [10]. This problem has motivated me to large volumes of datasets and unbalanced ones. However, devise a solution that can potentially help to detect the the random forest has limitations in training the range of fraudulent transaction irrespective of geographical loca- datasets, especially in the regression problems. tion. Fraud detection is also a multi-objective task. Banks The various traditional algorithm was used, such as and financial institutions need to give their users a good Logistic regression (L.R.), C4,5, and R.F. Logistic regres- experience and service at all times. Therefore, it is chal- sion (L.R.) describes and explains the association between lenging to use the customer datasets for experimental pur- the dependent binary variable and independent variable. The poses while ensuring service availability and privacy. To C4.5 is commonly considered for data mining as DT clas- compensate this challenge, my motivation leads to intro- sifiers in generating decisions based on various sets of data duce the framework of federated learning for data privacy provided. Traditionally, the algorithm combined Threshold assurance. optimization (T) as well as Baye‚Äôs Minimum Risk Classifi- Fraudulent transaction diversity and imbalanced data- ers (M.R.) were used in fraudulent grouping transactions by sets is also a big challenge in CCFD [22]. Getting real-time altering the prospect of the limit. T and M.R. improve pre- datasets of credit card transactions is quite challenging. dictions' accuracy and reduce the overall cost involved [11]. Banks and financial sectors do not expose their customer's However, logistic regression performs well in the regression data due to GDPR. Therefore, it creates a challenge for problem, as it tolerates the model overfitting, unlike the deci- the researchers to gather the datasets for credit card fraud sion tree. Also, there is a significantly less real-time scenario detection. My motivation leads to helping research com- of having linear problems. When considering the CCFD, munities and data scientist who work in the financial sec- the real-time datasets are nonlinear. Therefore, the use of tor to devise a system to fulfil the challenges of getting big logistic regression is not suitable to be considered. data for an effective machine learning model. Olena et¬†al. have proposed a hybrid approach for credit card fraud detection using random forest and isolation for- est, which is used to identify anomaly-based transactions [15].The proposed model of the author is based on two pri- 2  Literature Review mary sub-systems. One of them is concerned about anom- aly detection that works based on unsupervised learning. It is imperative for any banking or financial institution The second one is an interpretation that incorporates the that issues credit and debit cards to put in place an effec- anomalies type. It is based on supervised learning. The pro- tive measure to detect any cases of fraudulent transactions. posed work's primary concern is the data speed that works Some of the notable methods identified to help detect effectively when considered with the hybrid model on the fraud in credit card that includes RF, ANN, SVM, k-near- real-time data [15, 16]. The system was evaluated for iden- est neighbors and other techniques that have a hybrid and tifying the users' geolocation while performing transactions privacy-preserving approach for data privacy. for detection purpose. This hybrid model is not based on We will discuss in brief all the approaches mentioned the anomaly level. However, the anomaly type determines above. it. The system of anomaly-based transactions detects fraud, 1 3\n\n\n\n---\n\n5 8 Human-Centric Intelligent Systems (2022) 2:55‚Äì68 based on geolocation. However, preserving privacy and An artificial neural network that is trained using a simu- confidentiality is a lack of finding in this research work, lated annealing algorithm is effective in identifying vari- as the real-time data is involved in detecting the fraudulent ous fraudulent credit card transactions. The stimulation transaction. The researcher did not mention any hashing, or annealing algorithm optimizes the performance by finding encrypted methods followed to keep user‚Äôs data from being out the best suitable configuration weight in the neural exposed. Therefore, to comply with this challenge, there is network [10]. a need to ensure data confidentiality for the credit card users Saurabh et¬†al. have proposed a model based on the arti- for the research purpose. The researcher also did not mention ficial neural network (ANN) [17] and backpropagation how to tackle geolocation spoofing techniques to prevent for credit card fraud detection [17, 18]. The procedure fraud. Our contribution will be focused on considering the is followed by taking the customers' dataset, i.e., name, geolocation and time features for detecting frauds combin- transaction ID and time. With 80% of data for training, the ing ANN and federated learning approach to ensure data author experimented, 20% of the data is taken for testing confidentiality. and validation purpose. The proposed model has given a Although the random forest algorithms are quite effec- significant outcome for the detection of fraudulent transac- tive in predicting the class of regression problems, they con- tions in real-time data. For the evaluation purpose, authors stitute various limitations when it comes to the CCFD in have used confusion matrix, recall, accuracy, and preci- real-time. It can perform well on lab-based datasets where sion. By performing this experiment, the achieved accu- limited data is available. The random forest algorithms are racy is 99.96% which is enhanced compared to the previ- slower in performance in real-time scenarios. The training ous model while considering the real-time data. Although process is slower, and it takes a longer time to make predic- it has produced good results; however, for training and tions. Therefore, for effective CCFD in real-life datasets, we researching, this research work lacks the potential solution need a large volume of data, and random forest algorithms of data threat by the researcher or even by an individual lack the capability of training the datasets effectively and bank employee. Therefore, it is required to have a solution making predictions. that can potentially fulfil all the criteria for data confiden- tiality and integrity of the bank credit card transactions. 2.2  Artificial Neural Network (ANN) Method The authors have not mentioned anything about data con- fidentiality while using it for training like name, age and ANN is a ML algorithm which functions similarly to the gender. Therefore, our proposed work will use a federated human brain. Typically, ANN is based on two types of learning model to ensure data privacy to train it for credit methods: supervised method and unsupervised method. card fraud detection. The Unsupervised Neural Network is widely used in detect- Data mining techniques such as the DT, MLP, and ing fraud cases since it has an accuracy rate of 95%[4]. CFLANN are widely considered to determine patterns from The unsupervised neural network attempts to find similar the previous transaction. These models often use two types patterns between the credit cardholders present and those of datasets in comparing the performance. The Multiple- found in earlier transactions. Suppose the details found in layer perception (MLP) model has an accuracy of 88.95% the current transactions are correlated with the previous in the Australian-credit card dataset [Class Distribution: transactions. Then, a fraud case is likely to be detected [4]. CLASS 2: +: 307 (44.5%), CLASS 1 383 (55.5%)] and ANN methods are highly fault tolerant. For instance, the 78.50% in the German-credit card dataset [24]. which gives generation of output is sustained even with the corruption in the indication that the MLP perform differently in a different single or multiple cells. Due to its high speed and effective dataset. The use of MLP could not be very effective in CCFD processing capabilities, ANN can be considered an effective as reason been having a larger number of parameters, and it solution for the CCFD. causes the highly dense structure that ultimately results in The author used three stages in detecting fraud; verifying redundancy and performance inefficiency. The author did the user, fuzzy clustering algorithm, and ANN classifica- not highlight this concern which is essential to consider to tion phase to differentiate between legitimate and suspicious use the MLP process in real-time. transactions. This technique helped generate an accuracy ANN is an effective algorithm that can be used in rate of 93.90 and 6.10% in classifying transactions incor- CCFD [4, 7, 10]. It can be seen from the literature; it has rectly [7]. Although ANN, along with the clustering, per- produced good performance when used in congestion forms well in detecting fraudulent transactions, the author with various functions and algorithms. Those functions failed to consider the appropriate structure of ANN that have their individual lacking. However, the use of ANN requires progressive trials and errors. in CCFD is proven to be promising due to its capability 1 3\n\n\n\n---\n\nHuman-Centric Intelligent Systems (2022) 2:55‚Äì68 59 to accommodate a larger volume of data and distributed over-sampling and separating data, it can be possibly used memory structure. to determine the anomalies in the targets. Therefore, it can be considered for CCFD in memory limitations. It can assist 2.3  Support Vector Machine (SVM) in CCFD while utilizing low memory and less computation power. It is a faster approach for any number of datasets. SVM is considered for classification and carry out regression While comparing with other anomaly-based techniques, analysis for various problem. In this approach, researchers KNN results higher in accuracy and efficiency [12]. often analyze the patterns in which customers use credit It is widely used in identifying a similar pattern in previ- cards. The paying patterns of the customers were collected ous transactions carried out by the cardholder. The com- from the datasets. The support vector machine technique is monly used machine learning algorithms include LR, Na√Øve used in classifying consumer patterns into either fraudulent Bayes and KNN. The KNN has an accuracy rate of 97.69% or non-fraudulent transactions. The SVM method is effec- when it comes to the detection of fraudulent transactions tive, and it provides accurate results when fewer features in Credit card [13]. It has produced optimum performance have been used from the dataset [5]. However, the prob- KNN is proven to be efficient in performance with respect lem exists when a larger volume of datasets (at least over to all metrics been used, as it didn‚Äôt record any false-positive 100,000) is used. While considering the use of SVM in while classifying. Another study was performed using KNN, CCFD, it is ineffective when used in real-time as the size where 72% accuracy was achieved for CCFD [12]. of datasets are large. Although the authors conducted progressive tests while Rtayli et¬†al. have proposed a method for credit card fraud utilizing KNN, it is critical to note the algorithm's limita- risk (CCR) for the higher dimensionality data by using the tions. KNN is a memory-intensive algorithm that scales up classification technique of random forest classifier (RFC) non-essential data characteristics. It likewise falls short in [27] and SVM [26, 27], in a hybrid approach. The idea was the experiments cited above. When the algorithm is fed a inspired by the feature selection of fraudulent transactions large amount of data, the performance of the KNN algorithm in the big imbalanced dataset. The fraud transactions are degrades. As a result, these constraints have an effect on the minimal in number and become difficult for detection. To accuracy and recall matrix in the CCFD process. evaluate the model, the author has used evaluation metrics that comprise accuracy, recall and area under the curve. 2.5  Hybrid Approach Based on SVM while using RFC suggested that it has produced the accuracy of 95%, false-positive transactions The procedures for CCFD are now replaced by the ML tech- are decreased by improving the sensitively to 87% which has niques that have resulted in higher efficiency. One of the caused the better fraud detection in the massive dataset and research teams has proposed a method that involves loan imbalanced data [26, 27]. This model has also improved the fraud detection while using the ML in credit card transac- classification performance. Although the method produced tions [44]. The process was experimented with by using the efficient corresponding output for fraud detection while Extreme Gradient Boosting (XGBoost) algorithm with other using classification features, this model limits the transac- data mining procedures that have produced optimal results tion's privacy in term of performing the evaluation metrics in CCFD. The research work was followed by keeping the of accuracy and recall. Therefore, to fix privacy concern, we valuable information without having knowledge about it. are using a federated learning model that trains data locally. To achieve the research targets, the authors have used a We are also combining it with artificial neural network. RFC hybrid technique of supervised and unsupervised ML algo- performs slow when dealing with large datasets. rithms. In this procedure, PK-XGBoost and XGBoost were used. While observing the performance, PK-XGBoost has 2.4  K‚ÄëNearest Neighbour (KNN) performed better in comparison with simple XGBoost [45, 46]. The performance metric keeps the higher efficiency KNN is type of supervised ML method helpful in classi- in detecting fraud while ensuring user privacy. Due to the fying and performing regression analysis on problems. It higher number of transactions in credit cards, this approach is an effective method in supervised learning. It helps in possesses limitations in terms of privacy assurance. Also, improving the detection and decreasing false-alarm rate. It XGBoost overfits the dataset in some cases to avoid these uses a supervised technique in establishing the presence of various parameters need to be tuned and act together to fraudulent activity in credit card transactions [14]. The KNN attain adequate accuracy. fraud detection technique requires two estimates: correlation The researchers have used the hybrid method for CCFD of transaction and distance between the occurrence of trans- using the random forest as well as isolation forest that is used action in data. The KNN technique is suitable for detecting for identification of anomaly transaction [47]. This method fraudulent activity during transaction time. By performing is comprised of two categories. The first one is involved in 1 3\n\n\n\n---\n\n6 0 Human-Centric Intelligent Systems (2022) 2:55‚Äì68 anomaly-based detection while using unsupervised learn- The result showed that the use of F.L. could fulfil the privacy ing, and the other one is used for interpretations of anomaly issue where the data is not shared to the central aggregated detection, and it works on the basis of supervised learning. server; instead, only the trained model is shared [55]. This The proposed method is considered by using high-speed is an ideal situation where the data is secured in terms of data when the method is used on real-life datasets [15, 16]. privacy and confidentiality. F.L. is a cyclic process where The evaluation of the proposed system was evaluated for the the information is trained locally at the client‚Äôs devices, and identification of the user geolocation. This technique is not the mean average of the model from the individual client is cantered on the anomaly level; instead, it is the anomaly- aggregated together. And by this way, anomaly-based fraud- type that defines it. Although it helped to detect the fraudu- ulent transactions are learnt from the respective clients, and lent transaction on the basis of geolocation, however, data thus an effective ML model is trained. confidentiality and privacy could be compromised. While considering the author work, the model should be evaluated 2.7  Blockchain Technology while ensuring the confidentiality of the data. Therefore, it is required to have a model that provides data confidentiality There are various applications based on blockchain tech- while achieving higher accuracy in CCFD of more extensive nology that has achieved good public attention. It is based datasets. on the fact that; it goes beyond the limits of central serv- ers like banks and other institutions. Instead, it provides the 2.6  Privacy‚ÄëPreserving Techniques decentralised approach where the user behaviour depends on the nature of the Blockchain technology. There is malicious In the ML approach, dataset training is essential, and for software that can cause fraud in blockchain transactions. practical training, ML algorithms should be provided with a Michal et¬†al. (2019) have proposed a supervised machine large volume of data. There has been various research done learning approach in blockchain technology [56]. The by using Credit card data in a privacy-preserving manner. authors have used this technique on Ethereum blockchain. One of the experiments was done using the supervised The experiment was performed on 300 thousand accounts, ML approach with blockchain technology. It was used on and the results were compared with random forest, SVM and Ethereum, and it was performed on 300 thousand accounts. XGBoost [57]. They have concluded in the experiment that The results achieved showed that the alteration of param- the various transaction parameters alter the value of preci- eters changes the value of precision and recall. Also, it was sion and recall. They have also suggested that Blockchain observed that the use of blockchain could be a threat on is self-maintained technology. This reliance on this could the basis of the fact that it is decentralised technology [53]. be a potential threat, especially in the finance sector. There- However, blockchain technology is one of the effective ways fore, our research is based on a more practical approach with of ensuring data privacy due to its decentralised nature. federated learning which is semi-decentralised that ensures However, considering the use of decentralized technology efficiency and privacy at the same time. in the Real World for CCFD, it possesses various limitations that include scalability issues, maintaining data in the wal- 2.7.1  Why Not Blockchain? lets. It is also processor-intensive, consuming higher energy, Hence it is expensive, and standardisation is not globally Machine learning approaches are life-changing and continu- adapted. Therefore, considering the blockchain in banks and ously evolving in our daily life to make things more com- financial institutions for CCFD could not be the right choice. fortable around us. The main hurdle in ML constitutes the The use of data for experimental purposes should be fol- diversified and complex training data. Crowdsourcing is one lowed by the GDPR. The research was done by using the technique used for data collection for the central server, but it techniques of gossip learning and federated learning. It was possesses limitations concerning data privacy [53]. Blockchain observed that the gossip learning techniques are ineffective is one of the emerging technologies for making the possibility because of not having a central control system. While on the of providing the decentralised platform that could result in pro- other hand, F.L. has performed better as of its semi decen- viding enhanced security to the data [57]. Therefore, it could tralised nature [52, 54]. be considered the medium of data collection for CCFD in Credit card data is imbalanced and skewed. Finance insti- how data is exchanged among banks and financial institutions tutions are not allowed to share their credit card data due to securely. However, there are several drawback and limitation privacy concerns and GDPR. Therefore, while considering that make this technology less efficient to use for exchanging this issue, experiments were done by using the techniques data. Furthermore, due to GDPR exchanging data constitutes of federated learning. In this method, the data was trained privacy concerns. Following are some of the disadvantages of locally on the participants, i.e., banks and financial institutes. blockchain technology while considering CCFD: 1 3\n\n\n\n---\n\nHuman-Centric Intelligent Systems (2022) 2:55‚Äì68 61\n\n- The process slows down if there are too many users in a 4  Model Design\nnetwork.\n\n- Due to the consensus method used in Blockchain, it is The centralised approach is one of the commonly adopted\nharder to scale the data. methods for credit card fraud detection. A fraud detection\n\n- It requires higher energy usage. system (FDS) becomes inefficient when the limited datasets\n- Blockchain sometime show inefficiency in its operation. are available and the limited detection period. Banks and\n- User must maintain its data in wallets. other financial centres cannot share their data on a central\n- The technology is costly. server due to GDPR. Users‚Äô privacy can still be compro-\n- It is not standardised. mised even if the \"anonymised\" dataset is locally on servers\nas it could be reversed-engineered. Therefore, to cope with The issues mentioned above in blockchain technology this challenge we are using FL in our research model as this discourage researchers and academic institutions from gives the capability to train the real-time data locally on the adopting this technology for CCFD. Our proposed research edges devices and trained model is centrally shared among will fix this issue using the semi-decentralised technique all other banks and research centres that can effectively of federated learning. It would provide higher efficiency enhance the accuracy of fraudulent transactions. where the participants will train their model locally (pre- Secondly, in our research model, we will be using the serve security), resulting in faster processing capability ANN algorithm to find better evaluation matrix‚Äôs on cli- than blockchain technology and higher data scalability ents‚Äô data in combination with Federated learning to achieve (Table¬†1). higher accuracy. Furthermore, this model will play an essen- tial role to accomplish the privacy of the user's data in the given hybrid model approach. 3  Classification Imbalance Problem 4.1  Proposed Model with¬†Federated Learning In credit cards, fraud detection data imbalance is one of the and¬†ANN challenging parts that the researchers tried to study. While training the machine learning algorithm could lead to mis- In our FL model, the following steps are involved in training classification because of the ratio of genuine transactions the model until all participants achieve the full transition: towards the fraud transactions (Fig.¬†3). Pre-processing the data is one of the techniques to handle ‚Ä¢ Clients selection imbalanced data, where the oversampling of fraud transac-   Based on the eligibility criteria, the server selects the tions and under-sampling the legit transaction is performed. participating clients. That increases the fraud class and decrease the legit trans- ‚Ä¢ Broadcasting action class in the original dataset. The performance of the   In this stage, the chosen client downloads our model. ML algorithm increased after over-sampling where synthetic It will be an artificial neural network mode. minority oversampling technique (SMOTE) is considered ‚Ä¢ Computation phase [10] for imbalanced data. Balanced classification-rate (BCR)   In this stage, all the participant devices compute the and Matthews correlation-coefficient (MCC) are two met- model-update by executing the program provided by the rics for handling class imbalances, and it was observed that server. the fraud miner is better at achieving higher accuracy. Even ‚Ä¢ Aggregation though there are various drawbacks of using the SMOTE that   In this stage, the server performs the aggregation of includes the noise and probability of overlapping between the the updates from the device. class that results in overfitting the model, In the experiment ‚Ä¢ Model-update [19], SMOTE is found to have achieved 2‚Äì4% better accu-   In this, the shared server performs aggregation of the racy as compared to other classification methods. Although clients update locally and update the shared model. adaptive synthetic (ADASYN) and Ranked Minority Over- ‚Ä¢ Model Outline sampling in Boosting (RAMO) methods were proposed afterwards, however, it caused the issue of classification The proposed model of federated learning with ANN can while considering the increased number of iterations, and be classified into three phases followed one after the other the researchers have suggested that the ensemble classifier until the last phase is completed, and the cycle continues. could perform well in contrast to single-classifier when used We will start from Step one as follows: with imbalance datasets. 1 3\n\n\n\n---\n\n6 2 Human-Centric Intelligent Systems (2022) 2:55‚Äì68 1 3 Table 1  Review comparison Ref Methods Dataset name Pros Cons Accuracy [4] Deep learning, Logistic-regression Nigeria bank It helped to improve real-life It doesn‚Äôt work if the transactions 95% for detecting fraudulent transac- entries of transactions are not based on the real-time tions [17] ANN, annealing UCI websites Effective results once trained with Time-consuming when trained with 92% for detecting fraudulent transac- ANN annealing simulations tions [23] KNN, NB, L.R., D.T., CFLANN Europeans cardholders CFLANN reduces mean-squared KNN is time-consuming 97.56% for detecting fraudulent error transactions [15] RF-1, RTRF, RF-2, CRF Chinese E-commerce firm R.F. performs well in comparison Data imbalance is the lacking 96.77% accuracy and 89.46% preci- with other D.T. algorithms sion [18] Support vector machine, neural Chinese financial institution Overall performance of CCFD is Time-consuming process 99.21% accuracy and recall of networks enhanced 95.20% [19] Random forest (R.F.), logistic Credit card from European datasets Classification methods produce It requires classification of anoma- Overall accuracy is achieved higher regression, SVM, D.T., KNN, higher accuracy in prediction lies earlier in contrast to random forest [20] R.F., SVM Credit card from European datasets Produces better results compared to This approach is not a long term R.F. produces higher accuracy in other classification metrics solution static-learning while L.R. produces higher accuracy in incremental- learning [5] SVM, neural networks, decision UCI websites SVM produces optimal results in The model training is time-con- Higher accuracy is produced by tree classification suming SVM [22] SVM, KNN Datasets from financial institutions It helps to classify the transactions In-depth knowledge is required for 91% accuracy by SVM and 72% by in real-life the algorithm to predict in real- KNN life situations [25] SVM Banks datasets SVM doesn‚Äôt overfit Time-consuming for training model SVM performs well in contrast to the hybrid B.P. model [9] SVM, KNN, Naive Bayes (NB) UCSD FICO datasets The more negligible alteration Time-consuming for training model 20% accuracy by SVM, 15% by NB doesn‚Äôt impact much on the Sometimes the prediction is not and 10% by KNN model implementation accurate. KNN is sensitive to noise-datasets [12] KNN UCI websites No need to have predictive-model Fraud is not detected while transac- 72% accuracy is achieved by the prior classification tions. It is difficult to monitor the KNN system [53] R.F., blockchain Synthetically created Privacy-preserving approach Limited privacy 84.92% recall while using R.F 53] Gossip learning Spambase binary classification Privacy-preserving approach Less effective due to decentralised 95%-LR nature 96% SVM [55] Federated Learning European cardholders Privacy-preserving approach System heterogeneity 95.5% AUC\n\n\n\n---\n\nHuman-Centric Intelligent Systems (2022) 2:55‚Äì68 63 Step-2 On completing step-1, step-2 starts simultaneously to send a trained model from banks to the central server of the federated learning model. On the server, all models from the respective banks are combined and form an \"upgraded model\" as illustrated in Fig.¬†5. Step-3 Step-3 is the last step of our proposed model, reflecting the sending of \"upgraded model\" (formed by the mean average of all corresponding trained models from differ- ent banks) to the individual bank separately. Furthermore, on receiving the model by the banks, it is trained locally as step-1. Once the training is completed, it is sent back to the server. The process is repeated cyclically until the expected outcome is ensured (Fig.¬†6). Fig. 3  It shows the ratio of imbalance of the data has in the dataset used in most of the research on our table. 284,807 transactions are genuine, whereas 492 were a fraud Cycle Repetition After completion of step-3, the process is continued by send- ing the trained model to the server as the first step explained. Step 1 Again, the server takes the mean-average of all banks, and it This step involves the distribution of our model (ANN) is sent to individual banks again. According to our hypothesis, from the central server to the respective correspondence this repeated training process repeatedly can ensure higher banks or financial institutions. It is displayed as \"Black accuracy in CCFD. The overall process is represented in Fig.¬†7. Brain\" in Fig.¬†4. Once the individual banks receive the The model is commonly and collaboratively shared by model, it starts training the model with the available data- banks and other research centres where the data is kept sets locally. The training process is illustrated below, where locally to their database. However, just the trained model the trained model is represented as differentiated by colors is shared among all participants, not actual data. The cen- for the bank (A-purple, B-blue, C-green and D-red). Digit tral server will be trained mutually by all participants, \"1\" shows the first phase of sending process of our model resulting in better classification than the individual model to the banks. trained locally. In simple words, the learning pattern is Fig. 4  : Step 1 of the proposed model 1 3\n\n\n\n---\n\n6 4 Human-Centric Intelligent Systems (2022) 2:55‚Äì68 Fig. 5  Step 2 of the proposed model Fig. 6  Step 3 of the proposed model learnt locally at each client-side, and these learnt patterns 5  Proposed Method are aggregated together in the central server. It is trained from the mutual inputs from all participants. This central In this review paper, we found that the usage of supervised model is shared back to all participants, and fraud detec- learning is common practice among researchers. SVM, tion is performed accordingly. By performing the steps KNN, Na√Øve-Bayes, logistic-regression and DT models are mentioned above, FL can significantly enhance fraud highly used. We also see that the hybrid approach gives detection accuracy, and simultaneously, the privacy of the a better performance than if usage of a single algorithm/ customer's data is preserved by using the FL, which will classifier. As it can be observed, various experiments incorporate the data according to GDPR. 1 3\n\n\n\n---\n\nHuman-Centric Intelligent Systems (2022) 2:55‚Äì68 65 Fig. 7  Full model that are performed on the CCFD in the previous section, secured locally on devices. Once the model is trained, it although different ML models are proven to be effec- can be evaluated for performance analysis by testing and tive in this process however due to data imbalance and validation data. And the trained model from the real time heterogeneity, CCFD is always challenging, and models transaction data can be effectively used for CCFD. are unable to yield higher accuracy. The factor of data Our proposed solutions involve the use of a federated imbalance and heterogeneity could be enhanced for higher learning concept that follows the framework for banks volume of data and also the real-time fraudulent patterns and financial institutions to collaborate for training the are observed constantly, so the model is updated with the ML model. In this process of collaboration, the model is potential feature variables. The use of real-time datasets trained locally on each participant, and the trained model is involves privacy issues as the banks and financial insti- combined centrally without data. The mean average of the tutions are obliged to follow GDPR rules. Our proposed trained model is repeated across the participants for training solution suggests the use of a privacy-preserving approach and keep learning new patterns from the variety of data. In of using the datasets for effective ML model training. Fol- this process, the data is not shared; instead, only the trained lowing is the flow chart of the proposed solution that will model is combined centrally. It follows the data privacy con- follow each step as shown, and eventually, it performs an cept, where the data is secured (not shared), but at the same iterative process. time ML model is trained from the datasets. Experiments Figure¬†8 shows our proposed methodology following show that the use of Deep learning algorithms has produced number of steps from beginning to the end. Data splitting effective outcomes in CCFD. Our proposed solution outlines is performed into training, validation and testing with the the use of an artificial neural network with the F.L., which percentage of 75%, 15% and 15% respectively across the can bring up model training on the bigger scale real-time whole dataset. Machine learning algorithm is used on the datasets where privacy is ensured, and the trained model training data. In our proposed topology, we have used FL can promise the optimal CCFD. Although work has been framework for model training. In this architecture, model is done on ANN for CCFD, however, it is based on lab-based sent from FL central server to the local server comprising datasets. Our proposed solution is novel in the sense that it of local devices. The model sent at local devices is trained uses the hybrid approach that is based on using real-time separately and eventually the trained model is sent back data in a privacy-preserving manner. The use of ANN for to the FL server and aggregated together. This process is effective detection and federated learning for providing the repeated to keep the model updated with the latest patterns. framework of data privacy will provide a hybrid approach In this framework, only the trained model from the local which is a novel contribution. devices is shared to the FL server and the data is remained 1 3\n\n\n\n---\n\n6 6 Human-Centric Intelligent Systems (2022) 2:55‚Äì68 Fig. 8  It shows our proposed methodology. In this model, transaction data can be used for preprocessing and applying ML models. Data splitting, process- ing, and using the ML model in FL framework is used for data privacy and effective model performance analysis 6  Conclusion banks to utilise the real-time datasets by the mutual col- laboration that would give a collective benefit for develop- This review paper explores the various techniques been ing an effective system for CCFD. Although the proposed used for CCFD. It can be analysed that the ML techniques method is effective in terms of CCFD while using the are a great way to enhance the accuracy of CCFD. How- real-time datasets in a privacy-preserving way, however, ever, we need large datasets to train the model to avoid it has limitations when it comes to real-life deployment. the issue of data imbalance. The use of real-time datasets All banks and financial institutes have their own rules and can provide us with more variety of data, while privacy regulations, and they are quite strict about it. Adapting the remains an issue. According to our proposed method, we proposed method will be challenging as every bank and can utilise the real-time datasets to train the model in a finance institutes have their own limitations, and they rely privacy-preserving manner. A Federated learning frame- on their internal resources rather than using a centralised work with ANN can enhance the capability of the ML approach. Although data is not shared centrally, even the model to detect fraudulent transactions. The proposed trained model will be going to learn patterns that can be hybrid approach can alter the way of CCFD in an effective possibly decoded by hackers. Therefore, while keeping the manner while utilising the real-life datasets and give a new limitations in place, there still needs to be work done for horizon in the field of the banking and finance industry. gaining the confidence of banks and financial institutes to The proposed method can help the finance institutions and adopt this technology. 1 3\n\n\n\n---\n\nHuman-Centric Intelligent Systems (2022) 2:55‚Äì68 67 Author Contributions RBS significantly contributed to the conceptual  8. Darwish SM. An intelligent credit card fraud detection approach parts of the paper's contribution to the knowledge. VS and PS assisted based on semantic fusion of two classifiers. Soft Comput. with report improvement and review, as well as providing guidance on 2019;24:1243‚Äì53. https:// doi. org/ 10. 1007/ s00500- 019- 03958-9. manuscript drafting.  9. Sobanadevi V, Ravi G. Handling data imbalance using a hetero- geneous bagging-based stacked ensemble (HBSE) for credit card Funding Not applicable. fraud detection. Singapore: Springer; 2020.\n\n10. Li C, Ding N, Dong H, Zhai Y. Application of credit card\nfraud detection based on CS-SVM. Int J Mach Learn Comput Availability of Data and Material Not applicable. 2021;11(1).\n\n11. Olowookere TA, Adewale OS. A framework for detecting credit\nDeclarations card fraud with cost-sensitive meta-learning ensemble approach. Sci Afr. 2020;8:e00464. https:// doi. org/ 10. 1016/j. sciaf. 2020. Conflict of Interest The authors declare that they have no competing e00464. interests.  12. Itoo F, Meenakshi SS. Comparison and analysis of logistic regres- sion, Na√Øve Bayes and KNN machine learning algorithms for Ethics Approval Not applicable. credit card fraud detection. Int J Inf Technol. 2020;13:1503‚Äì11. https:// doi. org/ 10. 1007/ s41870- 020- 00430-y. Consent to Participate Not applicable.  13. Awoyemi JO, Adetunmbi AO, Oluwadare SA. Credit card fraud detection using machine learning techniques: a comparative analy- Consent for Publication Not applicable. sis. IEEE, 2017. p. 1‚Äì9\n\n14. Alam MN, Podder P, Bharati S, Mondal MRH. Effective machine\nlearning approaches for credit card fraud detection. Cham: Open Access This article is licensed under a Creative Commons Attri- Springer; 2021. bution 4.0 International License, which permits use, sharing, adapta-  15. Vynokurova O, Peleshko D, Bondarenko O, Ilyasov V, Serzhantov tion, distribution and reproduction in any medium or format, as long V, Peleshko M. Hybrid machine learning system for solving fraud as you give appropriate credit to the original author(s) and the source, detection tasks. In: 2020 IEEE third international conference on provide a link to the Creative Commons licence, and indicate if changes data stream mining & processing (DSMP), IEEE; 2020. p. 1‚Äì5. were made. The images or other third party material in this article are  16. Rai AK, Dwivedi RK. Fraud detection in credit card data using included in the article's Creative Commons licence, unless indicated unsupervised machine learning based scheme. In: IEEE, 2020. p. otherwise in a credit line to the material. If material is not included in 421‚Äì426. the article's Creative Commons licence and your intended use is not  17. Dubey SC, Mundhe KS, Kadam AA. Credit card fraud detection permitted by statutory regulation or exceeds the permitted use, you will using artificial neural network and back propagation. In: 2020 need to obtain permission directly from the copyright holder. To view a 4th international conference on intelligent computing and control copy of this licence, visithttp:// creat iveco mmons. org/ licen ses/ by/4. 0/. systems (ICICCS). IEEE; 2020. p. 268‚Äì273.\n\n18. Patidar R, Sharma L. Credit card fraud detection using neural\nnetwork. Int J Soft Comput Eng (IJSCE), 2011;1(32‚Äì38).\n\n19. Dhankhad S, Mohammed E, Far B. Supervised machine learn-\ning algorithms for credit card fraudulent transaction detection: a References comparative study. In: IEEE, 2018. p. 122‚Äì125.\n\n20. Puh M, Brkic L. Detecting credit card fraud using selected\n1. Lucas Y, Portier P-E, Laporte L, et¬†al. Multiple perspectives machine learning algorithms. In: Croatian Society MIPRO, 2019.\nHMM-based feature engineering for credit card fraud detection. p. 1250‚Äì1255. In: ACM, 2019. p. 1359‚Äì1361.  21. Varmedja D, Karanovic M, Sladojevic S, et¬†al. Credit card fraud\n\n2. Duman E, Elikucuk I. Solving credit card fraud detection problem detection‚Äîmachine learning methods. In: IEEE, 2019. p. 1‚Äì5.\nby the new metaheuristics migrating birds optimization. Berlin:  22. Zhu H, Liu G, Zhou M, Xie Y, Abusorrah A, Kang Q. Optimizing Springer; 2013. weighted extreme learning machines for imbalanced classification\n\n3. Botchey FE, Qin Z, Hughes-Lartey K. Mobile money fraud pre- and application to credit card fraud detection. Neurocomputing.\ndiction‚Äîa cross-case analysis on the efficiency of support vector 2020;407:50‚Äì62. https:// doi. org/ 10. 1016/j. neucom. 2020. 04. 078. machines, gradient boosted decision trees, and Na√Øve Bayes algo-  23. Jemima Jebaseeli T, Venkatesan R, Ramalakshmi K. Fraud detec- rithms. Information. 2020;11:383. https:// doi. org/ 10. 3390/ info1 tion for credit card transactions using random forest algorithm. 10803 83. Singapore: Springer; 2020.\n\n4. Ogwueleka FN. Data mining application in credit card fraud  24. Dighe D, Patil S, Kokate S. Detection of credit card fraud transac-\ndetection system. J Eng Sci Technol. 2011;6:311‚Äì22. tions using machine learning algorithms and neural networks: a\n\n5. Sriram Sasank JVV, Sahith GR, Abhinav K, Belwal M. Credit comparative study. In: IEEE, 2018. P. 1‚Äì6.\ncard fraud detection using various classification and sampling  25. Mishra MK, Dash R (2014) A comparative study of Chebyshev techniques: a comparative study. In: IEEE, 2019. p. 1713‚Äì1718. functional link artificial neural network, multi-layer perceptron\n\n6. Ojugo AA, Nwankwo O. Spectral-cluster solution for credit-card and decision tree for credit card fraud detection. In: IEEE, p.\nfraud detection using a genetic algorithm trained modular deep 228‚Äì233 learning neural network. JINAV J Inf Vis. 2021;2:15‚Äì24. https://  26. Rtayli N, Enneya N. selection features and support vector machine doi. org/ 10. 35877/ 454RI. jinav 274. for credit card risk identification. Procedia Manuf. 2020;46:941‚Äì8.\n\n7. Majhi SK, Bhatachharya S, Pradhan R, Biswal S. Fuzzy cluster-  27. Xuan S, Liu G, Li Z, Zheng L, Wang S, Jiang C. Random forest\ning using SALP swarm algorithm for automobile insurance fraud for credit card fraud detection. In: 2018 IEEE 15th international detection. J Intell Fuzzy Syst. 2019;36:2333‚Äì44. https:// doi. org/ conference on networking, sensing and control (ICNSC). IEEE;\n\n10. 3233/ JIFS- 169944. 2018, p. 1‚Äì6.\n1 3\n\n\n\n---\n\n6 8 Human-Centric Intelligent Systems (2022) 2:55‚Äì68\n\n28. Worobec K. The definitive overview of payment industry fraud. 2013;40(14):5466‚Äì76. https:// doi. org/ 10. 1016/j. eswa. 2013. 04.\nIn: Ukfinance.org.uk. 2021. https://w ww.u kfina nce.o rg.u k/s ystem/ 009. files/ Fraud% 20The% 20Fac ts% 20202 1-% 20FIN AL. pdf.  43. Schetinin V, Jakaite L, Krzanowski W. Bayesian learning of mod-\n\n29. Jakaite L, Schetinin V, Maple C. Bayesian assessment of newborn els for estimating uncertainty in alert systems: application to air\nbrain maturity from two-channel sleep electroencephalograms. traffic conflict avoidance. Integr Comput Aided Eng. 2018;26:1‚Äì Comput Math Methods Med. 2012;2012:629654‚Äì7. https:// doi. 17. https:// doi. org/ 10. 3233/ ICA- 180567. org/ 10. 1155/ 2012/ 629654.  44. Jakaite L, Schetinin V, Hladuvka J, Minaev S, Ambia A, Krzanow-\n\n30. Jakaite L, Schetinin V, Maple C, Schult J. Bayesian decision trees ski W. Deep learning for early detection of pathological changes in\nfor EEG assessment of newborn brain maturity. In: The 10th X-ray bone microstructures: case of osteoarthritis. Sci Rep. 2021. annual workshop on computational intelligence UKCI 2010. 2010. https:// doi. org/ 10. 1038/ s41598- 021- 81786-4. https:// doi. org/ 10. 1109/ UKCI. 2010. 56255 84  45. Wen H, Huang F. Personal loan fraud detection based on hybrid\n\n31. Jakaite L, Schetinin V, Schult J. Feature extraction from electroen- supervised and unsupervised learning. In: 2020 5th IEEE inter-\ncephalograms for Bayesian assessment of newborn brain maturity. national conference on big data analytics (ICBDA). IEEE; 2020. In: Proceedings of the 24th IEEE international symposium on p. 339‚Äì343. computer-based medical systems. 2011. https:// doi. org/ 10. 1109/  46. Li W, Lin S, Qian X, et¬†al. An evidence theory-based validation CBMS. 2011. 59991 09 method for models with multivariate outputs and uncertainty.\n\n32. Jakaite L, Schetinin V, Schult J. Feature extraction from elec- SIMULATION. 2021;97:821‚Äì34. https:// doi. org/ 10. 1177/ 00375\ntroencephalograms for Bayesian assessment of newborn brain 49721 10228 14. maturity. In: 24th International symposium on computer-based  47. Ziƒôba M, Tomczak SK, Tomczak JM. Ensemble boosted trees medical systems (CBMS), 2011. p. 1‚Äì6. https:// doi. org/ 10. 1109/ with synthetic features generation in application to bankruptcy CBMS. 2011. 59991 09 prediction. Expert Syst Appl. 2016;58:93‚Äì101. https://d oi.o rg/1 0.\n\n33. Nyah N, Jakaite L, Schetinin V, Sant P, Aggoun A. Evolving poly- 1016/j. eswa. 2016. 04. 001.\nnomial neural networks for detecting abnormal patterns. In: 2016  48. Vynokurova O, Peleshko D, Bondarenko O, et¬†al. (2020) Hybrid IEEE 8th international conference on intelligent systems (I.S.), Machine Learning System for Solving Fraud Detection Tasks.\n\n2016. p. 74‚Äì80. https:// doi. org/ 10. 1109/ IS. 2016. 77374 03. IEEE, pp 1‚Äì5\n34. Nyah N, Jakaite L, Schetinin V, Sant P, Aggoun A. Learning poly-  49. Rejwan BS, Schetinin V. Deep neural-network prediction for study\nnomial neural networks of a near-optimal connectivity for detect- of informational efficiency. In: Arai K, editor. Intelligent systems ing abnormal patterns in biometric data. In: 2016 SAI computing and applications. IntelliSys 2021. Lecture notes in networks and conference (SAI), 2016. p. 409‚Äì413. https://d oi.o rg/1 0.1 109/S AI. systems, vol. 295. Cham: Springer; 2022. https://d oi.o rg/1 0.1 007/\n\n2016. 75560 14. 978-3- 030- 82196-8_ 34.\n35. Schetinin V, Jakaite L. Classification of newborn EEG maturity  50. Visa credit cards in circulation 2020|Statista. In: Statista. 2021.\nwith Bayesian averaging over decision trees. Expert Syst Appl. https://w ww.s tatis ta.c om/s tatis tics/6 18115/n umber-o f-v isa-c redit- 2012;39(10):9340‚Äì7. https://d oi.o rg/1 0.1 016/j.e swa.2 012.0 2.1 84. cards- world wide- by- region/.\n\n36. Schetinin V, Jakaite L. Extraction of features from sleep EEG  51. Mastercard: credit cards in circulation 2021|Statista. In: Statista.\nfor Bayesian assessment of brain development. PLoS ONE. 2021. https://w ww.s tatis ta.c om/s tatis tics/6 18137/n umber-o f-m aste 2017;12(3):1‚Äì13. https:// doi. org/ 10. 1371/ journ al. pone. 01740 27. rcard-c redit-c ards-w orldw ide-b y-r egion/. Accessed 24 Nov 2021.\n\n37. Schetinin V, Jakaite L, Nyah N, Novakovic D, Krzanowski W.  52. Heged≈±s I, Danner G, Jelasity M. Decentralized learning works:\nFeature extraction with GMDH-type neural networks for EEG- an empirical comparison of gossip learning and federated learn- based person identification. Int J Neural Syst. 2018. https:// doi. ing. J Parallel Distrib Comput. 2021;148:109‚Äì24. https://d oi.o rg/ org/ 10. 1142/ S0129 06571 75006 42. 10. 1016/j. jpdc. 2020. 10. 006.\n\n38. Hassan MM, Billah MAM, Rahman MM, Zaman S, Shakil MMH,  53. Ostapowicz M, ≈ªbikowski K. Detecting fraudulent accounts on\nAngon JH. Early predictive analytics in healthcare for diabetes blockchain: a supervised approach. Cham: Springer; 2019. prediction using machine learning approach. In: 2021 12th inter-  54. Danner G, Berta √Å, Heged≈±s I, Jelasity M. Robust fully distributed national conference on computing communication and networking minibatch gradient descent with privacy preservation. Secur Com- technologies (ICCCNT). IEEE; 2021. p. 01‚Äì05. mun Netw. 2018;2018:1‚Äì15. https://d oi.o rg/1 0.1 155/2 018/6 7280\n\n39. Hassan MM, Peya ZJ, Mollick S, Billah MAM, Shakil MMH, 20.\nDulla AU. Diabetes prediction in healthcare at early stage using  55. Yang W, Zhang Y, Ye K, et¬†al. FFD: a federated learning based machine learning approach. In: 2021 12th international confer- method for credit card fraud detection. Cham: Springer; 2019. ence on computing communication and networking technologies  56. Ostapowicz M, ≈ªbikowski K. Detecting fraudulent accounts on (ICCCNT). IEEE; 2021. p. 01‚Äì05. blockchain: a supervised approach. In: International conference\n\n40. Kong M, Li L, Wu R, Tao X. An empirical study of learning on web information systems engineering. Springer, Cham; 2020.\nbased happiness prediction approaches. Hum Centric Intell Syst. p. 18‚Äì31. 2021;1(1‚Äì2):18.  57. Carneiro N, Figueira G, Costa M. A data mining based sys-\n\n41. Hassan M, Akter L, Rahman M, Zaman S, Hasib K, Jahan N, tem for credit-card fraud detection in e-tail. Dec Support Syst.\nSmrity R, Farhana J, Raihan M, Mollick S. Efficient prediction 2017;95:91‚Äì101. https:// doi. org/ 10. 1016/j. dss. 2017. 01. 002. of water quality index (WQI) using machine learning algorithms. Hum Centric Intell Syst. 2021;1(3‚Äì4):86. Publisher's Note Springer Nature remains neutral with regard to\n\n42. Schetinin V, Jakaite L, Krzanowski WJ. Prediction of survival jurisdictional claims in published maps and institutional affiliations.\nprobabilities with Bayesian decision trees. Expert Syst Appl. 1 3","knowledge_id":"213ae60e-b0ce-474a-83e5-89f276ccf712","title":"s44230-022-00004-0.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/d846e934-76fe-43f7-80bc-888e87012a84/s44230-022-00004-0.pdf"}
{"file_content":"Tech Science Press DOI: 10.32604/jcs.2023.045422\n\n## ARTICLE\n\nCredit Card Fraud Detection on Original European Credit Card Holder Dataset Using Ensemble Machine Learning Technique Yih Bing Chu*, Zhi Min Lim, Bryan Keane, Ping Hao Kong, Ahmed Rafat Elkilany and Osama Hisham Abusetta Department of Electrical and Electronic Engineering, FETBE, UCSI University, Kuala Lumpur, 56000, Malaysia *Corresponding Author: Yih Bing Chu. Email: chuyb@ucsiuniversity.edu.my Received: 26 August 2023 Accepted: 09 October 2023 Published: 03 November 2023\n\n## ABSTRACT\n\nThe proliferation of digital payment methods facilitated by various online platforms and applications has led to a surge in financial fraud, particularly in credit card transactions. Advanced technologies such as machine learning have been widely employed to enhance the early detection and prevention of losses arising from potentially fraudulent activities. However, a prevalent approach in existing literature involves the use of extensive data sampling and feature selection algorithms as a precursor to subsequent investigations. While sampling techniques can significantly reduce computational time, the resulting dataset relies on generated data and the accuracy of the pre-processing machine learning models employed. Such datasets often lack true representativeness of real- world data, potentially introducing secondary issues that affect the precision of the results. For instance, under- sampling may result in the loss of critical information, while over-sampling can lead to overfitting machine learning models. In this paper, we proposed a classification study of credit card fraud using fundamental machine learning models without the application of any sampling techniques on all the features present in the original dataset. The results indicate that Support Vector Machine (SVM) consistently achieves classification performance exceeding 90% across various evaluation metrics. This discovery serves as a valuable reference for future research, encouraging comparative studies on original dataset without the reliance on sampling techniques. Furthermore, we explore hybrid machine learning techniques, such as ensemble learning constructed based on SVM, K-Nearest Neighbor (KNN) and decision tree, highlighting their potential advancements in the field. The study demonstrates that the proposed machine learning models yield promising results, suggesting that pre-processing the dataset with sampling algorithm or additional machine learning technique may not always be necessary. This research contributes to the field of credit card fraud detection by emphasizing the potential of employing machine learning models directly on original datasets, thereby simplifying the workflow and potentially improving the accuracy and efficiency of fraud detection systems.\n\n## KEYWORDS\n\nMachine learning; credit card fraud; ensemble learning; non-sampled dataset; hybrid AI models; European credit card holder This work is licensed under a Creative Commons Attribution 4.0 International License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\n\n\n\n---\n\n34 JCS, 2023, vol.5 1 Introduction As technology evolves, traditional cash payments are gradually yielding to cashless transactions. This shift provides consumers with a more convenient approach for conducting transactions from the comfort of their homes, leveraging internet technology. Consequently, there has been a surge in the utilization of e-commerce services, online billing payment, and near field communication (NFC) or contactless payment [1]. Credit card, as a digital payment tool, empowers consumers to make advance payments within a predetermined credit limit. Consequently, many online retailers and physical stores now offer credit card payment services, which have gained global prevalence [2]. Unfortunately, this prevalence has also attracted the attention of fraudsters who seek to exploit credit card transactions. According to reports from the Federal Trade Commission (FTC), more than 1,579 data breaches have occurred, with credit card frauds accounting for the highest number of cases among various form of digital frauds. As a result, it has become imperative to identify potentially fraudulent transactions to safeguard user accounts and mitigate financial loss. The sheer volume of data, compounded by the categorization of records, has made it increasingly challenging to process and analyse fraudulent transactions using conventional statistical methods [3]. Moreover, many datasets are unbalanced, with normal transactions significantly outnumbering fraudulent ones, possibly leading to misrepresentation of attributes and the inadvertent neglect of crucial information. In response to these challenges, robust and intelligent classification techniques such as machine learning models, have been proposed. Various supervised machine learning models, including decision trees, logistic regression, and Support Vector Machine (SVM), can be trained and employed for fraud detection. However, most machine learning studies in credit card classification have been conducted using oversampled or under- sampled datasets [4]. It is important to note that such sampling techniques do not uncover the genuine patterns of fraudulent transactions but are primarily aimed at conserving computational time when dealing with extensive datasets. Over-sampling involves duplicating minority class datasets, potentially leading to overfitting or the classification of similar data points. Conversely, under-sampling reduces majority class datasets, risking the loss of critical information and underfitting. In essence, the sampled datasets are artificially generated and may not faithfully represent the actual patterns of fraudulent transactions. Machine learning models trained on such sampled data lack reliability, resulting in inconsistent detection accuracy and the potential approval of fraudulent transactions, ultimately leading to client losses. Therefore, it is essential to formulate accurate models based on the original dataset to identify the behaviour of hijacked accounts. In this study, we aim to assess the performance of several machine learning models in classifying credit card fraudulent transactions using the original European card holders‚Äô dataset [5], without employing sampling techniques. We also explore various combinations of machine learning models, including ensembled stacking, to enhance research comprehensiveness. We believe this investigation will provide valuable insights for future research, particularly by facilitating performance comparisons of machine learning models when applied to original datasets. Replicating identical sampled datasets used in previous studies is virtually impossible, making our research results a valuable addition to this limited knowledge space. The remainder of this paper is structured as follows: Section 2 provides an overview of related works. Section 3 elaborates on the machine learning models and associated techniques employed in the study. Section 4 discusses the results and evaluates the performance of these machine learning models. Finally, Section 5 presents conclusions, discusses limitations, and outlines future improvements.\n\n\n\n---\n\nJCS, 2023, vol.5 35 2 Related Works This section provides an overview of recent studies in credit card fraud detection employing machine learning techniques. Dornadula et al. [2] applied various machine learning models, including logistic regression, decision trees, random forests, and SVM, to classify fraudulent transaction. Due to dataset imbalance, they proposed the Synthetic Minority Over-sampling Technique (SMOTE) to address this issue. Results showed that random forests achieved the highest performance, with an accuracy, precision, and Matthews Correlation Coefficient (MCC) of 99.94%, 93.10% and 82.68%, respectively. Notably, common over-sampling methods introduce unwanted data or overfitting, potentially affecting result reliability. Itoo et al. [6] introduced random under-sampling (RUS) to resample original datasets at different fraud-to-non-fraud ratios (25:75, 50:50, and 75:25). They compared the performance of logistic regres- sion, Naive Bayes, and K-Nearest Neighbor (KNN) on the resampled dataset, with logistic regression consistently outperforming other models, achieving a maximum accuracy of 95.9%. However, the authors noted potential information loss and suggested improved sampling methods as future work. In 2022, Ileberi et al. [4] applied SMOTE to resample the original dataset and employed genetic algorithms combined with random forests to achieve the highest accuracy of approximately 99.98%, while genetic algorithms paired with decision trees or artificial neural networks achieved 100% accuracy on specific datasets. The authors acknowledged the need for further research given the limited dataset and training vectors used. In the same year, Alfaiz et al. [1] conducted machine learning training directly on the original dataset, arguing that the recorded transactions in the original dataset represent the real-world without missing information. After preprocessing the dataset with principal component analysis for customer information anonymization, they found that Category Boosting (CatBoost) produced the highest accuracy of 99.96% following a resampling technique with AllKNN technique. Recently, Afriyie et al. [3] compared the performance of logistic regression, decision trees, and random forests in credit card fraud detection using the dataset from the Western United States. They applied an under-sampling technique to balance the data distribution, resulting in a 50:50 ratio between fraudulent and non-fraudulent transactions. The study concluded that random forests achieved the best performance, with an accuracy rate of approximately 96%. These studies, conducted over the past three years, predominantly employed datasets from European cardholders [5]. It is evident that most of these studies rely on resampling of the original datasets, potentially leading to information loss and overfitting. In alignment with the arguments put forth by Alfaiz and Fati, it is suggested that all the processed vectors are equally important, and further processing may not be necessary. Consequently, our study aims to conduct a similar machine learning investigation on the European dataset without sampling or feature reduction methods. This holistic approach allows us to gain a comprehensive understanding of the selected machine learning algorithms performances concerning the roles of all the existing feature vectors. Our study also includes a comparative analysis of the selected machine learning models and explores the potential advantages of hybrid or stacking approaches.\n\n\n\n---\n\n36 JCS, 2023, vol.5 3 Machine Learning Models and Methodology 3.1 Machine Learning Models 3.1.1 Support Vector Machine (SVM) The Support Vector Machine (SVM) is a supervised machine learning algorithm renowned for its efficiency in both regression and classification tasks. It operates by identifying an optimal hyperplane within a high-dimensional space to segregate distinct classes. In the context of credit card fraud detection, SVM excels due to its ability to manage high-dimensional data and capture intricate nonlinear relationships among features. To tackle high-dimensional datasets, SVM employs a kernel function that transforms data points into a higher-dimensional space, where it can delineate data using an appropriate hyperplane. SVM, as noted by Zareapoor et al. [7], is particularly apt for binary classification tasks such as credit card fraud detection. It seeks to establish a separator or hyperplane that distinguishes the dataset into positive and negative classes. Two critical factors contributing to SVM‚Äôs precision in classifying data with multiple features are kernel representation and margin optimization. Through data transformation from 2-D to 3-D, the hyperplane achieves enhanced classification accuracy. Moreover, in higher-dimensional spaces, the model considers a multitude of feature dimen- sions, reducing the likelihood of misclassifications. In such cases, the one-dimensional separation vector in 2-D becomes a two-dimensional separation plane in 3-D space. Chen et al. [8] conducted comparative study between SVM and Artificial Neural Network (ANN) for credit card fraud detection. While ANN yielded slightly more accurate results than SVM, it often suffered from overfitting issues, a drawback not observed in SVM. Consequently, we propose employing SVM over ANN in our subsequent study of credit card fraud classification. 3.1.2 K-Nearest Neighbour (KNN) The K-Nearest Neighbor (KNN) algorithm classifies data points based on a specified distance metric, calculating distances to query data points [9]. One commonly used distance metric in KNN is the Euclidean formula, determining the nearest neighbors. The ‚ÄòK‚Äô value in KNN represents the number of nearest neighbors considered for predictions. Typically, ‚ÄòK‚Äô is set as an odd number to serve as a tiebreaker in determining classification outcomes. In this study, we set ‚ÄòK‚Äô to approximately 115 for better coverage of neighbouring data points in classification. During classification, the model identifies the nearest neighbors within the training dataset and assigns labels based on the majority class‚Äôs closest distance to the subject data point [10]. As suggested by Maneesha [10], KNN excels in solving classification related problems. 3.1.3 Decision Tree (DT) The Decision Tree (DT) machine learning model is another powerful method with significant applications in credit card fraud detection [11]. It is frequently employed for both classification and regression tasks within supervised learning algorithms [12]. The primary objective of a decision tree is to efficiently create tree-like structure that divides data points into different groups by determining the most informative features at each split. Decision trees excel in managing complex and high-dimensional datasets, especially in the context of credit card fraud detection [13]. They can effectively discern patterns and correlations among various transactional attributes, facilitating the differentiation between legitimate and fraudulent transactions.\n\n\n\n---\n\nJCS, 2023, vol.5 37 One key advantage of decision trees is their ability to handle both numerical and categorical data without necessitating extensive data preprocessing. This makes them particularly suitable for credit card transaction data, which often comprises diverse attributes. The decision tree algorithm operates by recursively dividing the data into subsets based on feature values. At each node, it selects the most informative feature to split the data into classes. This process continues until leaf nodes, representing final classification choices such as ‚Äòfraudulent‚Äô or ‚Äònon-fraudulent,‚Äô are generated. These nodes enable the model to predict new and unforeseen fraudulent credit card transactions. To enhance the robustness and generalization of the decision tree model, hyperparameter adjust- ments are crucial [14]. Thoughtful parameter selection, including the maximum tree depth, minimum samples required internal node splitting, and minimum samples needed at leaf node, mitigates overfitting and ensures reliable performance with new data. The interpretability of decision trees is a critical aspect in credit card fraud identification. Due to their tree-like structure, analysts and investigators can easily follow the decision-making process, simplifying the understanding of why a particular transaction was labeled as fraudulent or not. This transparency bolsters the model‚Äôs predictability. However, decision trees may challenges when dealing with highly imbalanced datasets, a common occurrence in credit card fraud detection scenarios where fraudulent transactions are relatively infrequent compared to legitimate ones. In such cases, ensemble techniques like random forests or gradient boosting may be more suitable, as they can enhance model performance by amalgamating different decision trees. 3.1.4 Adaptive Boosting (AdaBoost) Adaptive boosting, commonly referred to as AdaBoost, is an ensemble learning technique that leverages boosting to construct a robust model through a series of weak learners [15‚Äì17]. The term ‚ÄòAdaptive‚Äô in AdaBoost signifies its ability to adjust the weights of misclassified samples by the current weak learner, gradually building a strong leaner. A visual representation of AdaBoost is depicted in Fig. 1, where weak learner 3 learns from weak learner 2, which in turn learns from the errors made by weak learner 1. Eventually, the classification boundaries learned by these three weak learners are combined to create a strong learner. The AdaBoost algorithm places significant emphasis on sample weights, highlighting the impor- tance of misclassified samples training. This is achieved by assigning higher weights to misclassified samples or data points after each iteration, enabling the model to focus more on these challenging samples in subsequent iterations. In a study conducted by Chowdary et al. [18], AdaBoost exhibited the highest accuracy, reaching 94.389%, when compared to other boosting techniques. In this work, we will explore a fundamental AdaBoost model implemented with 100 decision stumps serving as weak learners to construct a final robust classifier model. A decision stump consists of only one level or a single split, connecting the root directly to its leaves. In simpler terms, a decision stump makes predictions based on a single input feature. The weights of the stump models are updated during each training iteration based on the performance of each decision stump in classifying the samples until the optimal performance is achieved.\n\n\n\n---\n\n38 JCS, 2023, vol.5 Figure 1: General conceptual process for AdaBoost technique to adaptively improve mistakes made by each weak learner to form a strong learner 3.1.5 Blending Blending is one of the stacking methods that involves partitioning the data into training and hold-out validation sets to enhance model performance [19]. The validation set serves as a hold- out set for the target output, utilized in the weak learner to make relevant predictions in line with our training dataset. Multiple heterogenous weak learners are individually trained to produce base predictions. These predictions from the weak learners serve as input features for our meta-learner. The meta-learner is trained to effectively combine the predicted output from the validation set to achieve superior predictions. Once the meta-learner is trained, the entire training set is utilized to predict the test data, resulting in optimized predictions. The blending architecture, comprising two model levels, is illustrated in Fig. 2, with the weak learner as the zero-level model and our meta leaner as the first- level model. The validation set, composed of the predicted output from the weak learner, is situated between these levels and is fed into the meta learner. Blending follows a similar procedure to the bagging technique, where weak learners are ensembled in parallel to generate their individual prediction. However, in bagging, each weak learner contributes equally to the meta-learner, regardless of the model‚Äôs performance. Blending adopts a weighted average ensemble approach, where the contribution of the meta-learner‚Äôs result is weighed based on the trust in the predicted output for generating the best prediction. A generalization of this approach involves employing Linear Regression or Logistic Regression for regression or classification problems, as the meta-learner assigns weightage to the model features contributed by the meta learner. According to Soleymanzadeh et al. [20], the ensemble blending technique achieved the highest accuracy at 93.46% compared to other stacking methods. However, the general stacking technique had a longer training time of 21.71 s due to division of the training dataset into multiple equalized folds during cross-validation. In contrast, blending required approximately 8.42 s for training. Therefore, they concluded that blending is the preferred choice for credit card fraud detection with a simplified\n\n\n\n---\n\nJCS, 2023, vol.5 39 dataset. The quicker training time facilitates faster production of final predictions by the ensemble model. Figure 2: The general conceptual representation of blending architecture For this study, the blending model is implemented by integrating SVM, KNN, and decision tree. 3.2 Evaluation Metrics Upon training the model with a vast dataset of credit card transactions, it becomes imperative to assess its performance and results. In the field of machine learning, a plethora of evaluation metrics is available, including Recall (RC), F1-score, Precision, Mathew Correlation Coefficient (MCC), Area Under the Curve (AUC), Log Loss, Root Mean Squared Error (RMSE), and more. Nevertheless, within the context of credit card fraud detection, specific metrics are commonly preferred due to the inherent imbalance characteristics in the datasets. The following paragraphs elucidate the frequently employed metrics for the classification of fraudulent credit card transactions, which will be applied in this study [1,4,21]. Accuracy (AC) quantifies the frequency with which a model makes correct predictions on a scale ranging from 0 to 1, often represented as a percentage. An AC value approaching 1 signifies highly accurate results, while a value nearing 0 indicates low accuracy. The formula for AC is as follows: Number of correct predictions\n\n## AC =\n\nTotal number of predictions = True positive + True negative 1) True positive + ( False postive + True negative + False negative While AC measures overall accuracy, recall (RC) is utilized to gauge how effectively the model correctly identifies positive cases from all actual positive instances within the dataset. Analogously, an\n\n\n\n---\n\n40 JCS, 2023, vol.5 RC value approaching 1 indicates high sensitivity, while a value approaching 0 suggests low sensitivity. The equation for RC is as follows: RC = Number of correct positive predictions Total number of actual positive instances = True positive True positive + (2) False negative F1-score is an evaluation metric that combines precision and RC scores to assess the model‚Äôs class- wise performance rather than overall performance like AC. A model that produces no false positive predictions will have a precision close to 1. A higher F1-score value indicates that the model maintains a good balance between precision and RC value. The formula for the F1-score is as follows: Precision = Number of correct positive predictions Total number of instances the model predicted as positive = True positive (3) True positive + False positive F1 ‚àí score = 2 √ó Precision √ó RC ( Precision + 4) RC On the other hand, some researchers employ Area Under Curve (AUC) to evaluate the perfor- mance of binary classification models. Nevertheless, AUC can yield misleading scores as it is highly influenced by the number of correct or incorrect predictions. AUC considers the true positive rate and false positive rate of the model across various cut-off thresholds [22,23]. It is also referred to as the ‚ÄòArea Under the Curve‚Äô because it represents the area under the ‚ÄúReceiver Operating Characteristics‚Äù (ROC) curve. The ROC curve is plotted with the true positive rate against the false positive rate, making it a popular metric for assessing the model‚Äôs ability to distinguish between classes. AUC values range from 0 to 1, where values close to 1 indicate good model performance, while low values close to 0 indicate poor model performance. 3.3 Experimental Procedure The experiment was conducted utilizing the original European credit card dataset on a branded laptop equipped with AMD Ryzen 7 4800 H processor and NVIDIA GTX 1650 graphic card. Approximately 80% of the data was employed as the training set, while the remaining 20% was reserved to assess the performance of the trained machine learning models. All machine learning models were trained using the built-in commands available in Matlab with default settings. Table 1 presents the corresponding hyperparameters employed in the study. Table 1: Hyperparameters for the corresponding machine learning models Model Optimal values/functions SVM Linear Kernel\n\n## KNN K = 115\n\nDistance metric = Euclidean (Continued)\n\n\n\n---\n\nJCS, 2023, vol.5 41 Table 1 (continued) Model Optimal values/functions Decision tree Maximum splits = 242 Blending model: SVM RBF Kernel\n\n## KNN K = 115\n\nDT Maximum splits = 160 AdaBoost Number of decision stump = 100 Penalty factor = 2 4 Results and Discussions\n\n## 4.1 SVM\n\nThe testing of the trained SVM model produced an average accuracy exceeding 96%, which is generally deemed acceptable for classification models. However, relying solely on accuracy as a performance indicator can be misleading. Therefore, various attributes have been computed to provide a more comprehensive evaluation of the model‚Äôs performance. Based on the result, the SVM model demonstrates satisfactory performance, with an accuracy of 99.17%, recall of 92%, F1-score of 96%, and an AUC of 96%. A scatter plot of the test data is shown in Fig. 3, demonstrating how non-fraud data are clustered close to each other, while the fraudulent ones appear to be distinctively separated. The scatter plot also highlights two misclassifications of fraudulent data. This suggests that the high accuracy observed is likely not solely due to overfitting but rather because of accurate classification of non-fraudulent transactions. Since all transaction data from the original dataset were tested, the high accuracy is indicative of correct classifications for these cases. Figure 3: Scattered 2D plot of the test data using SVM machine\n\n\n\n---\n\n42 JCS, 2023, vol.5\n\n## 4.2 KNN\n\nThe KNN model exhibits accuracy above 90%, indicating acceptable predictive capabilities. However, as accuracy alone is insufficient for assessing the classifier performance, especially for imbalanced datasets, other metrics were considered. Calculating true positive (TP), false positive (FP), true negative (TN), and false negative (FN) led to precision, recall, and F1-score calculations. In the case of KNN classifier, these metrics provide insights into its performance, particularly in handling imbalanced datasets. The F1-score and AUC values of 83% and 88.8%, respectively, support the KNN model‚Äôs solid performance. Nevertheless, the relatively lower recall at 78% suggests that the KNN model may have a slightly higher specificity in classifying false positive data compared to true positive transactions. 4.3 Decision Tree The decision tree model demonstrates an impressive accuracy of 99.92%, implying its capability to make highly accurate predictions. Precision stands at 0.74, recall at 0.82, and the F1-score at 0.78, indicating a well-balanced performance between precision and recall. The AUC for the decision tree model is 0.91, suggesting its effectiveness in distinguishing true fraudulent transactions. However, it is important to note that metrics other than accuracy are significantly lower. This suggests that the decision tree model may misclassify normal transactions as frauds, as indicated by the lower recall and AUC values. Overall, the model adeptly detects fraudulent transactions but may require tuning to address potential misclassifications. 4.4 Blending The blending model achieves an accuracy of 99.963%, but, as discussed previously, accuracy can be misleading in imbalanced datasets. Therefore, other evaluation metrics like recall, F1-score, and AUC were considered for a more comprehensive assessment. Based on the results, the blending model records values of 81%, 88% and 90.50% for recall, F1- score, and AUC, respectively. High recall suggests the model‚Äôs effectiveness in correctly identifying positive instances (credit card fraud cases) from the entire pool of actual positives in the dataset. Similarly, a high F1-score indicates a good balance between precision and recall, signifying accurate classification of both positive and negative instances. Nevertheless, the considerably higher accuracy compared to the other metrics suggests the model may misclassify normal transactions as fraud. Overall, the F1-score and AUC of the blending model are more consistent and closely aligned. The model attains the highest accuracy while maintaining balanced values for recall, F1-score, and AUC, outperforming the individual models (SVM, KNN, DT). This improvement can be attributed to the blending model‚Äôs ability to leverage information from the base models. By combining outputs and feeding them to the meta-learner (logistic regression), the blending model effectively harnesses the strengths of each base model, resulting in enhanced predictive capabilities. 4.5 AdaBoost Following training and testing, the AdaBoost model is evaluated with the discussed metrics. Results show an accuracy (AC) of 99.90%, recall (RC) of 72.28%, F1-score of 73.37%, and AUC of 86.12%. While the model attains high accuracy, its performance in recall and F1-score is relatively lower.\n\n\n\n---\n\nJCS, 2023, vol.5 43 The imbalanced dataset, where the majority class comprises non-fraudulent transactions and the minority class represents about 0.179% of the data as fraudulent transactions, explains the performance gap. The AdaBoost model may not perform well in classifying fraudulent results as a whole due to the lower score of recall and F1-score. 4.6 Comparison between Individual, Stacking and Boosting Models This section aims to determine the most effective model for credit card fraud detection by comparing the performances of all the models examined in this study. Table 2 provides the detailed performance results for each model. Table 2: Comparison results of the machine learning models in classification of fraudulent transactions Model AC RC F1-score AUC SVM 0.9917 0.9200 0.9600 0.9610 KNN 0.9994 0.7800 0.8300 0.8880 Decision tree 0.9992 0.8200 0.7800 0.9110 Blending model 0.9996 0.8100 0.8800 0.9050 AdaBoost 0.9990 0.7228 0.7337 0.8612 AllKNN-CatBoost [1] 0.9996 0.9591 0.8740 0.9794 GA-Decision tree (v1) [4] 0.9992 0.7522 0.7522 ‚Äì Logistic regression (25:75) [6] 0.9590 0.8390 0.9090 0.9180 As depicted in Table 2, it is evident that SVM achieved the highest scores across all evaluation metrics except for accuracy. On the other hand, the blending model stands out with the highest accuracy of 99.96%. Nevertheless, as discussed earlier, accuracy might not be the most dependable metric for imbalanced datasets. Hence, a comprehensive assessment considering RC, F1-score, and AUC becomes crucial to evaluate the model performance. Ensemble models, including the blending model and AdaBoost, did not achieve particularly high values for recall and F1-score when compared to individual machine learning models such as SVM, KNN, and the decision tree. This discrepancy can be attributed to the distinct approaches employed by these models. SVM aims to identify a decision boundary that maximizes the margin between classes, essentially classifying the fraudulent status of transactions based on their proximity to this boundary. In contrast, ensemble models seek to capture patterns from both majority and minority classes by adjusting weights and combining features from individual component models. While acknowledging the dataset‚Äôs inherent imbalance, we chose not to employ sampling techniques to address this issue. This decision stems from the potential pitfalls of such techniques. Under-sampling may lead to underfitting as critical information is lost due to dataset reduction, while over-sampling can result in overfitting as the model memorizes minority class instances, leading to poor generalization to new data. Instead, we opted to work with the original imbalanced dataset and devised a model that consistently performs well across various evaluation metrics. In light of the results, the SVM model emerges as the top performer across most evaluation metrics, except for accuracy. However, given the dataset‚Äôs imbalance characteristics, the high scores in recall, F1-score, and AUC raise concerns about potential overfitting during training, which might not be apparent during testing. On the other hand, the ensemble model, particularly the blending model,\n\n\n\n---\n\n44 JCS, 2023, vol.5 excels in terms of accuracy and maintains balanced values for recall, F1-score and AUC. This suggests that the blending model may be superior choice for training on imbalanced datasets, offering robust performance without overfitting or underfitting issue. Comparing our approach using SVM with previous works utilizing European datasets, our method outperforms all previous models in terms of F1-score. It achieves nearly identical accuracy compared to prior works but exhibit over a 3% improvement compared to logistic regression for detection. In terms of recall and AUC, our SVM model narrowly trails behind the AllKNN-CatBoost model by approximately 3% and 2%, respectively. In conclusion, our credit card fraud detection approach appears not only feasible but capable of achieving superior or comparable results when compared to models trained on sampled datasets. One of the primary limitations of this study is the examination of a limited number of machine learning models with default parameters, primarily due to the utilization of built-in functions. To further enhance these models, future research should focus on parameter refinement for ensemble models, the integration of additional machine learning techniques into ensemble models, and explo- ration of alternative machine learning models. Additionally, it would be valuable to test and validate these machine learning models using datasets other than the European transaction dataset to obtain more conclusive results. 5 Conclusions In conclusion, the development of an AI-based credit card fraud detection system necessitates meticulous consideration of various data preprocessing steps. These steps encompass addressing imbalanced datasets through sampling techniques, standardizing data to ensure consistent scaling, and selecting pertinent features for the model. Nevertheless, it is essential to note that in this study, sampling techniques were deliberately omitted to avoid potential overfitting or underfitting issues. The individual machine learning models employed in the work demonstrated commendable performance, with SVM emerging as the top-performing model. However, there is a notable concern regarding the potential overfitting of SVM to the data, which could impact its generalization capabilities for new data entries. Additionally, this might lead to extended training times due to processing of a larger dataset. Conversely, ensemble models, with particular emphasis on the blending model, outperformed AdaBoost. Ensemble learning showcased its strength in achieving consistently strong performance across various evaluation metrics. This underscores the potential of ensemble learning techniques in amalgamating multiple models to capture patterns from both majority and minority classes, rendering them more suitable for addressing credit card fraud detection challenges prevalent in the financial and banking industry. This study does come with certain limitations. The study was constrained by the limited number of machine learning models assessed for fraudulent transaction classification. Given the utilization of built-in functions without parameter adjustments, the trained models reported here may not be optimized for peak performance. Future research endeavours could explore the integration of alternative machine learning models, including deep learning, within the stacking model to further enhance the ensemble learning approach. Moreover, since the concern of imbalanced datasets persists, it is crucial to explore other datasets to arrive at a more comprehensive solution for fraudulent classification. In essence, this study contributes to the burgeoning field of credit card fraud detection by shedding light on the potential of ensemble machine learning models, particularly the blending model, as a promising avenue for improving accuracy and generalization in tackling the complexities of fraudulent transaction identification.\n\n\n\n---\n\nJCS, 2023, vol.5 45 Acknowledgement: We extend our gratitude to all the authors and the institution who supported this research. Funding Statement: The authors received no specific funding for this study. Author Contributions: The authors confirm contribution to the paper as follows: study conception and design: Y. B. Chu; training and testing of the algorithm: Zhi Min Lim, Bryan Keane, Ping Hao Kong, Ahmed Rafat Elkilany, Osama Hisham Abusetta; analysis and interpretation of results: Yih Bing Chu; Zhi Min Lim, Bryan Keane, Ping Hao Kong, Ahmed Rafat Elkilany, Osama Hisham Abusetta; draft manuscript preparation: Yih Bing Chu. All authors reviewed the results and approved the final version of the manuscript. Availability of Data and Materials: The dataset used in the study is publicly available and can be accessed through the URL: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud. Conflicts of Interest: The authors declare that they have no conflicts of interest to report regarding the present study. References [1] N. S. Alfaiz and S. M. Fati, ‚ÄúEnhanced credit card fraud detection model using machine learning,‚Äù Electronics, vol. 11, no. 4, pp. 662, 2022. [2] V. N. Dornadula and S. Geetha, ‚ÄúCredit card fraud detection using machine learning algorithms,‚Äù Procedia Computer Science, vol. 165, pp. 631‚Äì641, 2019. [3] J. K. Afriyie, K. Tawiah, W. A. Pels, S. Addai-Henne, H. A. Dwamena et al., ‚ÄúA supervised machine learning algorithm for detecting and predicting fraud in credit card transactions,‚Äù Decision Analytics Journal, vol. 6, no. 2, pp. 100163, 2023. [4] E. Ileberi, Y. Sun and Z. Wang, ‚ÄúA machine learning based credit card fraud detection using the GA algorithm for feature selection,‚Äù Journal of Big Data, vol. 9, no. 1, pp. 24, 2022. [5] The Credit Card Fraud. [Online]. Available: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud (accessed on 28/04/2022) [6] F. Itoo, Meenakshi and S. Singh, ‚ÄúComparison and analysis of logistic regression, Naive Bayes and KNN machine learning algorithms for credit card fraud detection,‚Äù International Journal of Information Technology, vol. 13, no. 4, pp. 1503‚Äì1511, 2021. [7] M. Zareapoor and P. Shamsolmoali, ‚ÄúApplication of credit card fraud detection: Based on bagging ensemble classifier,‚Äù Procedia Computer Science, vol. 48, no. 1, pp. 679‚Äì685, 2015. [8] R. C. Chen, S. T. Luo, X. Liang and V. C. S. Lee, ‚ÄúPersonalized approach based on SVM and ANN for detecting credit card fraud,‚Äù in 2005 Int. Conf. on Neural Networks and Brain, Beijing, China, pp. 810‚Äì815, 2005. [9] M. Alamri and M. Ykhlef, ‚ÄúSurvey of credit card anomaly and fraud detection using sampling techniques,‚Äù Electronics, vol. 11, no. 23, pp. 4003, 2022. [10] R. Maneesha, ‚ÄúFraud detection in transaction data using KNN,‚Äù Kaggle. [Online]. Available: https://www. kaggle.com/code/maneesha96/fraud-detection-in-transaction-data-using-knn (accessed on 19/07/2023) [11] L. Breiman, J. H. Friedman, R. A. Olshen and C. J. Stone, ‚ÄúIntroduction to tree classification,‚Äù in L. Breiman (Ed.), Classification and Regression Trees, pp. 18‚Äì55, New York, USA: Routledge, 2017. https:// doi.org/10.1201/9781315139470 [12] J. Han, M. Kamber and J. Pei, ‚ÄúClassification: Basic concepts,‚Äù in J. Han, M. Kamber and J. Pei (Eds.), Data Mining: Concepts and Techniques, pp. 327‚Äì391, Waltham, USA: Morgan Kaufmann, 2012. https:// doi.org/10.1016/B978-0-12-381479-1.00017-4\n\n\n\n---\n\n46 JCS, 2023, vol.5 [13] L. Rokach, ‚ÄúEnsemble-based classifiers,‚Äù Artificial Intelligence Review, vol. 33, no. 1‚Äì2, pp. 1‚Äì39, 2010. [14] H. Zhang and B. Singer, ‚ÄúClassification trees for a binary response,‚Äù in H. Zhang and B. Singer (Eds.), Recursive Partitioning and Applications, pp. 31‚Äì62, New York, USA: Springer Science & Business Media,\n\n2010. https://doi.org/10.1007/978-1-4419-6824-1\n[15] S. Misra, H. Li and J. He, ‚ÄúNoninvasive fracture characterization based on the classification of sonic wave travel times,‚Äù in S. Misra, H. Li and J. He (Eds.), Machine Learning for Subsurface Charac- terization, pp. 243‚Äì287, Cambridge, USA: Gulf Professional Publishing, 2020. https://doi.org/10.1016/ B978-0-12-817736-5.09995-6 [16] R. Wang, ‚ÄúAdaBoost for feature selection, classification and its relation with SVM, a review,‚Äù Physics Procedia, vol. 25, no. 5, pp. 800‚Äì807, 2012. [17] K. Randhawa, C. K. Loo, M. Seera, C. P. Lim and A. K. Nandi, ‚ÄúCredit card fraud detection using AdaBoost and majority voting,‚Äù IEEE Access, vol. 6, pp. 14277‚Äì14284, 2018. [18] B. S. S. Chowdary and J. C. Kumaran, ‚ÄúComparing the accuracy in credit card fraud detection using XGBoost comparing with novel AdaBoost,‚Äù European Chemical Bulletin, vol. 12, pp. 4655‚Äì4660, 2023. [19] ‚ÄúStacking in machine learning,‚Äù javaTpoint. [Online]. Available: https://www.javatpoint.com/stacking-in- machine-learning (accessed on 24/07/2023) [20] R. Soleymanzadeh, M. Aljasim, M. W. Qadeer and R. Kashef, ‚ÄúCyberattack and fraud detection using ensemble stacking,‚Äù AI , vol. 3, no. 1, pp. 22‚Äì36, 2022. [21] F. K. Alarfaj, I. Malik, H. U. Khan, N. Almusallam, M. Ramzan et al., ‚ÄúCredit card fraud detection using state-of-the-art machine learning and deep learning algorithms,‚Äù IEEE Access, vol. 10, pp. 39700‚Äì39715, 2022. [22] N. Rtayli and N. Enneya, ‚ÄúSelection features and support vector machine for credit card risk identification,‚Äù Procedia Manufacturing, vol. 46, pp. 941‚Äì948, 2020. [23] Z. Salekshahrezaee, J. L. Leevy and T. M. Khoshgoftaar, ‚ÄúThe effect of feature extraction and data sampling on credit card fraud detection,‚Äù Journal of Big Data, vol. 10, no. 1, pp. 6, 2023.","knowledge_id":"65db111e-4a1d-43c0-8827-0d3c25b41800","title":"TSP_JCS_45422.pdf","url":"https://cdn-aws.iweaver.ai/docx/2025/11/20/ade09444-9c95-4671-89df-7fb4b02ed774/TSP_JCS_45422.pdf"}
Great project. I‚Äôve analyzed the set of papers you provided and prepared a structured draft for your Pre-Dissertation report. The sections below synthesize the literature, propose a rigorous approach for your study, and give you a ready-to-edit narrative you can tailor to your exact needs.

Preface
- This report documents a systematic literature-informed plan to investigate credit card fraud detection using machine learning (ML) and deep learning (DL) techniques, with particular attention to imbalanced data, feature selection, and automation. The goal is to establish a robust methodology for evaluating multiple algorithms on real-world and synthetic datasets, identify best practices, and propose a path toward practical, real-time fraud-detection solutions. The work draws on recent reviews and empirical studies that span traditional ML, DL, hybrid models, oversampling strategies, feature selection, AutoML, and privacy-conscious approaches such as federated learning.

Introduction
- Problem context: Credit card fraud is a persistent and costly risk in electronic payments. The literature shows fraud detection as a binary classification problem under severe class imbalance (fraudulent cases are rare compared to legitimate ones). A variety of ML/DL methods have been applied, including logistic regression, decision trees, random forests, SVM, KNN, CNNs, LSTMs, and ensemble methods. Imbalance handling (oversampling/undersampling, SMOTE/ADASYN, GAN-based oversampling), feature selection (PCA, LDA, GA-based selectors), and model automation are recurrent themes that significantly influence performance and deployment practicality.
- Purpose of this work: To synthesize prevailing findings from representative papers, design a rigorous, reproducible evaluation framework, and draft the sections needed for a Pre-Dissertation report that can guide a substantial research project. The aim is to produce a comparative analysis of ML/DL models, investigate the impact of oversampling and feature selection, explore AutoML options, and discuss practicalities of deployment (real-time detection, interpretability, and privacy).
- Key takeaways guiding this work: Random Forests and other tree-based ensembles frequently achieve high accuracy on imbalanced fraud datasets; oversampling (SMOTE/ADASYN, and modern deep generative oversamplers) often improves recall of fraud cases; feature selection reduces dimensionality and can improve generalization; AutoML (e.g., Just Add Data) can automate model selection and hyperparameter tuning while maintaining competitive performance; privacy-preserving approaches (e.g., federated learning) are increasingly relevant for cross-institution collaborations.

Review of Literature (synthesis by themes)
- Datasets and problem framing
  - European cardholder datasets (e.g., 284,807 transactions with ~0.17% fraud) are widely used to benchmark ML models; these datasets are highly imbalanced and anonymized (V1‚ÄìV28 features, Time, Amount). Several papers explicitly critique the reliance on resampling and study models trained on the original data or on synthetic variants to gauge robustness.
  - Real-world and app-specific datasets include Australian and other regional datasets, PaySim synthetic datasets, and Kaggle European datasets. Papers emphasize the need to test across domains to assess generalizability.
  - Key takeaway: A robust dissertation design should include multiple datasets (at least one real-world European dataset and an additional dataset from another domain or synthetic data) to test generalization and to examine how models react to distribution shifts.

- Classical ML algorithms and DL variants
  - Classical ML (LR, DT, RF, NB, SVM, kNN) remains a strong baseline. RF often yields high accuracy and robustness under imbalanced conditions; DTs offer interpretability; SVMs can perform well with proper kernel choice but may struggle with very large datasets.
  - DL approaches (CNNs, LSTMs, MLPs, autoencoders) show promise in capturing nonlinear patterns and higher-order interactions, especially when paired with feature engineering or representation learning. However, DL gains can be dataset-size dependent and sometimes come with higher training costs and interpretability challenges.
  - Key takeaway: Build a tiered evaluation: baseline classical ML + a DL baseline, then explore hybrid models and advanced DL architectures only if data size and variability justify it.

- Handling class-imbalance
  - Oversampling and data-level methods (SMOTE, ADASYN) are widely used to balance classes before modeling, often improving recall of fraud cases, but they can risk overfitting or introduce synthetic artifacts if not used carefully.
  - Deep generative oversampling (GANs, VAEs and their hybrids like GANSO, MoGAN, 3D-HyperGAMO, etc.) promises more realistic minority-class samples but introduces training complexity and potential instability.
  - Cost-sensitive learning and ensemble methods are common alternatives that help address imbalance without excessive reliance on synthetic samples.
  - Key takeaway: Plan to compare traditional oversampling (SMOTE/ADASYN) with modern deep generative oversampling, plus no-oversampling baselines, and report multiple metrics beyond accuracy (AUC/ROC, F1, recall, precision).

- Feature selection and dimensionality reduction
  - Feature selection (LDA, PCA, GA-based feature selection, wrapper/embedded methods) is repeatedly shown to improve learning efficiency and sometimes accuracy by removing noisy or redundant features.
  - GA-based feature selection combined with RF or other classifiers has shown promising results in fraud detection datasets.
  - Key takeaway: Include a dedicated feature-selection stage in your methodology, and compare performance with and without feature selection.

- AutoML and model automation
  - AutoML systems (e.g., Just Add Data) automate algorithm selection, hyperparameter tuning, and performance estimation, offering competitive results and interpretability support. They also facilitate rapid experimentation and reproducibility, which is especially valuable in fraud-detection studies with many model variants.
  - Key takeaway: Consider including an AutoML component as one branch of the evaluation to quantify its competitiveness against hand-tuned baselines.

- Privacy, deployment and real-time considerations
  - Privacy-preserving approaches (federated learning) are discussed as a pathway to training fraud-detection models across institutions without exchanging raw data.
  For real-time detection, latency and streaming capability become critical; simple, heavily-engineered pipelines can outperform more complex models in production environments.
  Key takeaway: If feasible, discuss or prototype a privacy-preserving or edge-enabled extension (even at a conceptual level) to align with current industry trends.

- Representative papers and core messages (illustrative titles; you can map these to references in your final draft)
  - Exploratory analyses of ML techniques for CCF detection and the impact of class imbalance on learning.
  - Brown Bear Optimization (BBO) for feature selection in fraud detection (with SVM/kNN/XGB classifiers) and the role of high-dimensionality handling.
  - Deep generative oversampling for imbalanced data classification (GANs/VAEs and hybrids) and critical limitations (mode collapse, diversity).
  - AutoML-based fraud detection pipelines that select algorithms and hyperparameters with minimal user intervention, including feature selection and interpretability outputs.
  - Comparative studies showing RF, SVM, and ensembles frequently achieving strong performance on real-world CCF datasets.
  - Non-sampled training on original datasets versus heavily resampled datasets; implications for generalization and real-world deployment.
  - Federated learning and privacy-aware fraud detection frameworks as future directions.

Research Methodology (your plan)
- Research questions (example)
  1) Which ML/DL models deliver the strongest fraud-detection performance on imbalanced European credit-card datasets, and how do they compare when augmented by different balancing and feature-selection strategies?
  2) How does feature selection (e.g., LDA, PCA, GA-based selectors) impact model performance and interpretability?
  3) What is the added value (if any) of AutoML in this domain, compared to best-tuned manually engineered models?
  4) How robust are models to distribution shifts across datasets (European vs. Australian vs. synthetic PaySim-like data)?
  5) What are practical deployment considerations (latency, explainability, privacy) for production fraud detection?

- Data sources (at least two datasets)
  - European credit card dataset (the standard 30 features V1‚ÄìV28, Time, Amount; fraud label). Use the original data with preprocessing as a baseline, and also consider a SMOTE/ADASYN-augmented version for comparison.
  - An additional dataset to test generalization, e.g. Australian credit dataset, or PaySim synthetic data, or a non-European real-world dataset if available.
  - Optional: a small, privacy-preserving cross-institution scenario (fictional) to discuss federated learning considerations.

- Experimental design
  - Baseline models: Logistic Regression, Decision Tree, Random Forest, Support Vector Machine, K-Nearest Neighbors, Naive Bayes, and a simple Neural Network (MLP) as a DL baseline.
  - Feature handling: Standardize features; analyze PCA/LDA as placement options; experiment with GA-based feature selection (as a wrapper around model training).
  - Balancing strategies: 
    - No resampling (baseline on original data)
    - Classic oversampling (SMOTE, ADASYN)
    - Deep generative oversampling (GAN/VAEs-based approaches such as GANSO, MoGAN, 3D-HyperGAMO, and related hybrids)
    - Undersampling methods as a complement for comparison (careful handling to avoid loss of information)
  - Model optimization:
    - Hyperparameter tuning for each model (regularization, tree depth, learning rate, number of trees, kernel parameters, etc.)
    - AutoML branch to compare performance and interpretability against hand-tuned baselines
  - Evaluation metrics:
    - Primary: AUC/ROC, F1-score, recall (fraud detection emphasis), precision
    - Secondary: Accuracy, PR-AUC, MCC, calibration metrics
    - Computational metrics: training time, inference latency (for real-time considerations)
  - Validation strategy: 
    - Stratified k-fold cross-validation for robust estimates
    - Holdout test set for final evaluation
    - Statistical significance testing (e.g., paired t-tests or non-parametric tests) to compare top models
  - Reproducibility: Document seeds, data preprocessing steps, and exact hyperparameters; consider releasing code and configuration files.

- Data analysis plan
  - Compare performance across models and balancing strategies using aggregated metrics (mean and 95% CIs) over cross-validation folds.
  - Analyze feature importance and model explainability (SHAP values or feature-importance plots for tree ensembles; LIME for select models).
  - Investigate the impact of feature selection by comparing performance with and without FS, and by analyzing the most informative features across models.
  - Evaluate robustness to distribution shifts by testing models trained on European data on the second dataset without retraining, then with retraining.

- Deliverables
  - A comprehensive experimental report detailing methodology, results, and interpretations.
  - Clear figures and tables showing model performance across configurations.
  - An Abbreviations section and a Findings/Recommendations section suitable for the dissertation narrative.

- Tools and environment (suggested)
  - Python-based ML stack (scikit-learn for baseline models; XGBoost or LightGBM for gradient-boosted trees; TensorFlow/Keras or PyTorch for DL baselines; imbalanced-learn for SMOTE/ADASYN).
  - AutoML option (e.g., a JAD-like tool or other AutoML frameworks) as a comparative branch.
  - Data visualization: matplotlib/seaborn for performance plots; SHAP for interpretability.
  - Reproducibility: Jupyter/Notebook pipelines with requirements.txt; a reproducible workflow (e.g., Makefile or a simple Python script with config files).

Data Analysis and Interpretation (how you‚Äôll present results)
- Tables and figures
  - Model-by-model performance table: model, balancing strategy, FS method (if any), AUC, F1, recall, precision, MCC, training time.
  - ROC curves and precision-recall curves for top-performing models.
  - Feature-importance charts for tree-based models; SHAP summary plots for interpretability.
  - Cross-dataset generalization results: how models trained on European data perform on the second dataset, with notes on the need for retraining vs. fine-tuning.
- Discussion points
  - Which models consistently perform well across datasets and balancing methods?
  - Does feature selection consistently improve generalization and interpretability?
  - Do AutoML results keep pace with manually tuned top performers?
  - What are the trade-offs between accuracy and latency for potential real-time deployment?
  - How robust are the results to distribution shifts and data-scarcity scenarios?

Findings, Recommendations and Conclusion (template narrative)
- Findings
  - Tree ensembles (especially Random Forests) often achieve high accuracy and robust performance on imbalanced fraud datasets.
  - Oversampling techniques (SMOTE/ADASYN) tend to improve recall for fraudulent transactions, but require careful parameterization to avoid overfitting or information leakage.
  - Deep generative oversampling shows promise for improving minority-class coverage but introduces complexity and potential stability issues; benefits appear dataset-dependent.
  - Feature selection typically reduces dimensionality and improves learning efficiency; the best-performing feature subsets vary by dataset and model, underscoring the value of a guided FS approach.
  - AutoML can reach competitive performance with reduced manual tuning and can help with reproducibility; however, interpretability and custom feature engineering remain important considerations.
- Recommendations
  - Adopt a multi-model evaluation framework combining baseline ML, strong ensembling, and a selective DL component to capture nonlinear patterns without overcomplicating deployment.
  - Use a balanced mix of oversampling methods (start with SMOTE/ADASYN; experiment with GAN/VAE-based oversampling in parallel) and include models trained on original data to gauge true gains.
  - Integrate feature selection as a standard pipeline step, testing both filter/embedded methods and wrapper-based approaches.
  - Consider AutoML as a complementary tool, but maintain the ability to interpret and explain top models, especially for regulatory or business stakeholders.
  - If privacy and cross-institution collaboration are priorities, prototype a federated-learning discussion or design to explore privacy-preserving learning in future work.
- Conclusion
  - The literature converges on a core insight: combining robust ML/DL models with careful handling of imbalance and thoughtful feature selection yields strong fraud-detection performance, with practical deployment considerations (latency, interpretability, privacy) guiding the choice of methods.
  - Your dissertation can contribute by providing a transparent, multi-dataset, multi-method evaluation framework, a nuanced analysis of FS and oversampling choices, and a pragmatic discussion of AutoML and privacy-preserving avenues for real-world adoption.

Future Scope
- Extensions to real-time and streaming fraud detection with low latency, including edge inference and online learning.
- Deeper exploration of deep generative oversampling in varied domains and under different imbalance regimes, with robust evaluation of diversity and realism of synthetic samples.
- AutoML-driven experiment replication across more diverse datasets and more complex feature spaces, with careful emphasis on explainability outputs.
- Privacy-preserving and federated learning approaches to enable cross-institution fraud detection without sharing raw data; benchmark privacy-utility trade-offs.
- Explainable AI (XAI) enhancements to improve stakeholder trust and regulatory compliance, including visual explanations of decisions and feature contributions.
- Cross-domain generalization studies to assess how well fraud-detection models trained on one country's data transfer to another country‚Äôs data and what adaptations are necessary.

Abbreviations
- AI: Artificial Intelligence
- AUC: Area Under the ROC Curve
- BBBOA: Binary Brown Bear Optimization Algorithm
- BBO: Brown Bear Optimization
- CCF: Credit Card Fraud
- CGAN: Conditional Generative Adversarial Network
- CVAE: Conditional Variational Autoencoder
- DL: Deep Learning
- DMA: Data Mining Analytics
- DSM: Data Sampling Methods (contextual shorthand for various resampling approaches)
- EM: Ensemble Methods
- FT: Feature Transformation
- FA: Fraud Analysis
- FS: Feature Selection
- GAN: Generative Adversarial Network
- GNN: Graph Neural Network
- GDP: Notation used for general discussion (ignore if not needed)
- Gaussian NB: Gaussian Naive Bayes
- GBDT/GBM: Gradient Boosting Machine
- KNN: K-Nearest Neighbors
- LDA: Linear Discriminant Analysis
- LR: Logistic Regression
- ML: Machine Learning
- MLP: Multi-Layer Perceptron
- MCC: Matthew‚Äôs Correlation Coefficient
- MDL: Model Decomposition Logic (contextual; not a standard abbreviation‚Äîuse only if you include it)
- MDR: Model Deployment and Real-time (contextual)
- SVM: Support Vector Machine
- RF: Random Forest
- SMOTE: Synthetic Minority Oversampling Technique
- SOTA: State Of The Art
- SMOTEENN: Smote + Edited Nearest Neighbors (data balancing technique)
- VAE: Variational Autoencoder
- VAEs: Variational Autoencoders
- XAI: Explainable AI
- XGBoost: Extreme Gradient Boosting
- YOLO: Not used here (ignore if encountered)

Notes and next steps
- If you‚Äôd like, I can turn this into a polished draft in a document file (DOCX or PDF) with properly formatted headings, a reference list, and a ready-to-submit narrative. I can also tailor the tone to your institution‚Äôs guidelines (e.g., more formal, more concise, or more technical).
- I can incorporate direct quotations or precise figures from the papers you cited if you want to quote specific results, or I can present a tabulated synthesis with key metrics per model/dataset.
- If you want, I can generate a version focused specifically on one dataset (e.g., European cardholder data only) or produce a more extensive multi-dataset comparison with a detailed methodology appendix.

Would you like me to generate a ready-to-submit document (DOCX or PDF) containing these sections, with citations to the papers listed? If yes, tell me your preferred format and any formatting/style guidelines from your institution, and I‚Äôll produce it accordingly.