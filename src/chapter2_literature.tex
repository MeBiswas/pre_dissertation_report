% !TeX root = ../main.tex

\begin{justify}
It’s well known that only a tiny fraction of credit card transactions are actually fraudulent—less than 0.2\% in most places—so any system that tries to detect these rare cases ends up dealing with an extremely lopsided dataset. A lot of past work relies on the European Credit Card Dataset \cite{kaggle_dataset}, which has a little over 280k transactions but only a few hundred fraud cases. Because most of the features in it are PCA components (V1–V28), along with Time and Amount, it protects privacy and has become the default choice for many researchers. You’ll find it in all sorts of studies: ensemble-learning experiments \cite{ChuEnsemble2023}, AutoML work \cite{Plakandaras2022}, genetic-algorithm feature selection \cite{ileberi2022}, comparisons of deep models \cite{Alarfaj2022}, anomaly detection surveys \cite{Hafez2025}, hybrid ML pipelines \cite{Gaav2025}, and several review papers \cite{IJARSCT2024_Review}. Still, depending on this one dataset has limits, because fraud looks different depending on the region, the bank, and the period of time being analyzed.

Across the literature, a lot of the usual machine-learning models show up—Logistic Regression, Naive Bayes, Decision Trees, Random Forests, SVM, and KNN. Random Forest often shows solid performance, especially when combined with sampling strategies like SMOTE, ADASYN, or mixed approaches \cite{Madhuryaa2022}. SVMs regularly perform well too; some studies even report over 90\% scores without doing any resampling on the original dataset \cite{ChuEnsemble2023}. When researchers compare KNN, Naive Bayes, and Logistic Regression, the results often shift depending on which sampling ratio or balancing method is used \cite{BinSulaiman2022}.

More recent papers lean heavily toward ensemble models. Tools like XGBoost, LightGBM, and Gradient Boosting consistently beat single-model approaches. LightGBM is especially strong when fine-tuned or combined with automated pipelines \cite{Plakandaras2022}. Blended and stacked models—mixing SVM, KNN, Decision Trees, and others—have also shown improvements, especially in precision–recall performance \cite{ChuEnsemble2023}. Several surveys report the same trend: ensembles generally catch more subtle fraud patterns \cite{IJARSCT2024_Review}.

Handling imbalance is a recurring issue everywhere. SMOTE and ADASYN tend to boost recall, though some authors warn that too much oversampling introduces noise or leads to overfitting \cite{Madhuryaa2022}. In a few cases, combining SMOTE with Random Forests or GA-based feature selection produces accuracy numbers close to 99.98\% \cite{ileberi2022}. Undersampling techniques and hybrids like SMOTE-ENN or SMOTE-Tomek also show up in the literature \cite{BinSulaiman2022}. GAN-generated synthetic fraud samples are another direction, though training GANs is still tricky \cite{Shirvan2025}. Some papers argue that strong models like SVM or CatBoost can work well even without resampling at all \cite{Sorour2024}. Cost-sensitive learning is also suggested as a realistic alternative because it reflects the actual money lost when fraud is missed \cite{Albalawi2025}.

A lot of improvement comes down to feature engineering and choosing the right inputs. Time-related features, spending patterns, and scaled Amount values often make a noticeable difference. Techniques like PCA, LDA, and embedded feature-selection approaches appear regularly \cite{BinSulaiman2022}. Genetic Algorithm-based selection sometimes improves generalization and reduces complexity \cite{ileberi2022}. AutoML setups generate competitive results with almost no manual tuning \cite{Plakandaras2022}. Many papers also point out the usefulness of prototype selection, noise cleaning, and scaling techniques such as Robust Scaler or Min-Max \cite{IJARSCT2024_Review}.

Something that comes up a lot in recent reviews is that some common evaluation mistakes inflate the performance numbers. Examples include using random train–test splits instead of time-based ones, applying SMOTE before splitting the data (which causes leakage), and relying only on accuracy or ROC-AUC even though neither is ideal for imbalanced settings \cite{Hafez2025}. A growing number of papers insist on better practices like temporal validation, careful preprocessing, and using metrics such as PR-AUC \cite{IJARSCT2024_Review}. These concerns show up across ensemble comparisons \cite{ChuEnsemble2023}, AutoML work \cite{Plakandaras2022}, and deep-learning studies \cite{Alarfaj2022}.

Even after all this research, a few gaps still stand out. Most studies depend heavily on one dataset, making it hard to know whether the results apply elsewhere. Time-aware validation is still rare even though fraud naturally changes over time \cite{Gaav2025}. Many models lack interpretability, which becomes a real issue in regulated financial settings \cite{Sorour2024}. Real-time performance requirements are often ignored, and privacy-preserving ideas like federated learning are only briefly mentioned in a handful of papers \cite{Albalawi2025}. Altogether, the literature suggests that better evaluation practices, more datasets, and hybrid approaches combining ML, ensembles, and stronger feature-selection methods are needed to move the field forward.
\end{justify}


\vspace{1cm}
\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabularx}{\textwidth}{|p{3.2cm}|p{3cm}|p{2.1cm}|X|p{3cm}|}
        \hline
        \textbf{Author(s) \& Year} & \textbf{Models / Approach} & \textbf{Dataset} & \textbf{Techniques Applied} & \textbf{Key Findings} \\ \hline

        Chu et al. (2023) \cite{ChuEnsemble2023} &
        SVM, KNN, DT, Blending, AdaBoost &
        European Dataset &
        Original dataset without sampling; ensemble blending &
        SVM exceeded 90\% metrics; blending improved stability; warns against oversampling. \\ \hline

        Plakandaras et al. (2022) \cite{Plakandaras2022} &
        AutoML (JAD) &
        European Dataset &
        AutoML pipelines, automated feature engineering &
        AutoML competitive with tuned models; improves reproducibility. \\ \hline

        Ileberi et al. (2022) \cite{ileberi2022} &
        GA + RF / GA + ANN &
        European Dataset &
        SMOTE; Genetic Algorithm feature selection &
        Achieved $\approx$99.98\% accuracy; GA improves model generalization. \\ \hline

        Alarfaj et al. (2022) \cite{Alarfaj2022} &
        RF, XGBoost, ANN, LSTM &
        European Dataset &
        Machine Learning + Deep Learning &
        DL slightly outperforms ML; PR-AUC shows imbalance challenges. \\ \hline

        Sulaiman et al. (2022) \cite{BinSulaiman2022} &
        Survey of ML techniques &
        Multiple Datasets &
        Overview of ML, privacy, imbalance &
        Highlights gaps in interpretability, privacy, evaluation. \\ \hline

        Sayande et al. (2024) \cite{IJARSCT2024_Review} &
        ML, DL, Ensemble Methods &
        Multiple Datasets &
        Review of ML algorithms &
        RF, XGBoost, CNNs strongest; real-time constraints noted. \\ \hline

    \end{tabularx}
    \caption{Summary of Key Studies in Credit Card Fraud Detection}
    \label{tab:literature_summary}
\end{table}