% !TeX root = ../main.tex

\begin{justify}
    
The amount of credit card transactions has increased due to the quick growth of digital payments and e-commerce, but it has also made it possible for more sophisticated fraud efforts. Even though fraudulent transactions make up a relatively small portion of all activity—typically less than 0.2\%—they are responsible for large financial losses on a global scale. Credit card fraud detection is a difficult challenge because of this severe imbalance, changing fraud trends, and real-time judgment restrictions.

Modern fraud tactics are too fast for conventional rule-based systems to keep up.  Recent research has shown that manual rules are unable to manage high-dimensional data, adjust to novel attack tactics, or retain accuracy when there is a significant class imbalance.  A more scalable option is offered by machine learning (ML) models, which enable real-time detection in large-scale payment settings by learning intricate transaction patterns.

Strong promise for fraud classification has been demonstrated by supervised machine learning approaches, including Logistic Regression, Naive Bayes, Decision Tree, Random Forest, SVM, k-Nearest Neighbors, XGBoost, and LightGBM.  In order to prevent optimistic biases, recent studies also stress the significance of feature engineering (especially Time and Amount), prototype selection, dimensionality reduction, data scaling (Robust or Min-Max Scaling), and imbalance-handling techniques like SMOTE, GAN-based augmentation, and cautious cross-validation.

\section*{Research Problem Statement}

Credit card fraud might look like a simple yes-or-no classification problem, but in practice it’s a lot messier. Fraud cases are extremely rare compared to normal transactions, scammers constantly change how they operate, and many advanced models are so hard to interpret that banks can’t fully trust them. On top of that, any system used in real financial settings has to make decisions almost instantly, which adds even more pressure.

A big issue researchers keep running into is that many models appear to “perform well” only because they label nearly everything as genuine. That pushes accuracy numbers up, but it completely fails at the real objective—catching the small number of fraudulent cases. Things get even trickier over time because fraud patterns don’t stay the same; what works today might be useless a few months later. And with stricter privacy rules and the need for clear explanations, complicated black-box models aren’t always acceptable in banking environments.

To deal with these challenges, researchers have tried a wide mix of methods—traditional ML algorithms, neural networks, and different types of ensemble models. Across this body of work, a few themes keep showing up: figuring out which features actually matter, automating parts of the modelling workflow, and dealing with class imbalance using techniques like oversampling, undersampling, SMOTE, and even GAN-generated synthetic data.

Even with all that progress, several key issues still remain:

\begin{itemize}
    \item \textbf {Class Imbalance:} Fraud cases are so rare that many models take the easy path and label nearly everything as legitimate. It boosts accuracy but misses the point entirely.
    \item \textbf {Concept Drift:} Scammers constantly adapt, so a model trained on older data gradually becomes less useful unless it’s updated regularly.
    \item \textbf {Model Interpretability:} Some advanced deep learning models work like black boxes. That’s a problem for banks, which often need clear explanations for any decision involving a customer.
    \item \textbf {Privacy and collaboration:} Banks could improve fraud detection by sharing trends with one another, but privacy regulations and competitive concerns make such collaboration difficult.
\end{itemize}

\section*{Research Objectives}

The main idea behind this study is pretty simple: I want to figure out a reliable way to judge how different machine-learning methods actually perform when it comes to spotting credit card fraud. Instead of just running a few models and reporting numbers, the goal is to build a process that someone else could follow and get consistent results

To do that, I’m trying to cover a few things:

\begin{itemize}
    \item One part of the work involves going through what other researchers have already done, getting a sense of which ML techniques seem promising, where they fall short, and what questions still haven’t been answered properly.
    \item I compare a bunch of models—older, well-known algorithms, ensemble methods, and deep-learning setups—and see how they behave under the same evaluation approach. The idea is to understand the differences without giving any one method an unfair advantage.
    \item There’s also the issue of class imbalance, which is unavoidable in fraud datasets. So I look at how various balancing techniques change model performance. That includes things like SMOTE, creating fake fraud samples with GANs, simple undersampling, and even cost-based adjustments.
    \item And finally, I want the whole process to be solid. That means using the right metrics (like recall, F1, AUC-PR), avoiding common mistakes such as leaking information from one part of the dataset into another, and making sure the validation respects the timeline of real transactions.
\end{itemize}

\section*{Significance of the Study}

This study tries to help both researchers and people who actually work with fraud detection every day. On the research side, it gives a clearer picture of how different machine-learning methods behave when the data is messy, uneven, or constantly changing because fraud patterns rarely stay the same for long.

From a practical point of view, the work is meant to make life a bit easier for banks, payment companies, and fraud teams. It talks about what kinds of models tend to work better, how the data should be prepared, and which balancing techniques actually help without slowing down the system or creating problems with regulations.

A lot of the ideas in this study build on earlier findings—things like why choosing the right features matters so much, how synthetic minority examples can help in rare-event problems, and why ensemble methods often end up performing more reliably than single models.
\end{justify}

% Add vertical space above the figure
\vspace{1.5cm}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/system-architecture.png}
    \caption{System Architecture for the Identification of Credit Card Fraud}
    \label{fig:system-architecture}
\end{figure}