% !TeX root = ../main.tex

This chapter presents the performance analysis of all machine learning models evaluated in this study. 
I put every model to test under preprocessing settings, and analyzed its behavior in terms of robustness to class imbalance, generalization capacity, and classification performance. 
A cross-model comparison, useful suggestions, and concluding observations round up the chapter.

% ============================================================
% Logistic Regression
% ============================================================
\section*{Findings from Logistic Regression}

Three variations of logistic regression (LR) were assessed: scaled data, SMOTE-balanced data, and GAN-balanced data.  This made it possible to examine the behavior of the linear model under various minority-sample distributions.

\subsection*{Scaled Data}
Strong and balanced performance was attained by the scaled-data LR model (ROC--AUC = 0.9722, precision = 0.7381, recall = 0.6596, F1 = 0.6966). 
Stable generalization was shown by cross-validation measures that were consistent across folds.  
There were no obvious indications of overfitting.

\paragraph{Interpretation}
The modified Time and Amount features and the evenly distributed PCA components both helped the model.  When trained on real data without artificial oversampling, its linear decision boundary proved adequate to capture fraud behavior.

\begin{figure}[H] \centering \includegraphics[width=0.8\textwidth]{lr_scaled.png} \caption{Confusion matrix for Logistic Regression on scaled data.} \end{figure}

\subsection*{SMOTE-Balanced Data}
Despite a minor improvement in ROC--AUC (0.9834), recall reached 0.9362, but accuracy fell to 0.0114, resulting in an ineffective F1-score (0.0225). 
Although cross-validation on balanced data looked great, it did not convert to actual validation data.

\paragraph{Interpretation}
LR overfitted to the artificial clusters generated by SMOTE.  
The decision boundary that was produced did not generalize to unbalance in the real world.

\begin{figure}[H] \centering \includegraphics[width=0.8\textwidth]{lr_smote.png} \caption{Confusion matrix for Logistic Regression on SMOTE-balanced data.} \end{figure}

\subsection*{GAN-Balanced Data}
GAN-balanced LR experienced the largest performance degradation (ROC--AUC = 0.8139, 
precision = 0.0052, F1 = 0.0104).  
Despite perfect performance on GAN-balanced folds, real validation performance collapsed.

\paragraph{Interpretation}
LR learned false decision boundaries as a result of GAN samples' excessive distortion of the fraud distribution.

\begin{figure}[H] \centering \includegraphics[width=0.8\textwidth]{lr_gan.png} \caption{Confusion matrix for Logistic Regression on GAN-balanced data.} \end{figure}

\subsection*{Summary}
\begin{itemize}
    \item Scaled-data LR performed best and was the only variant with practical utility.
    \item SMOTE and GAN severely harmed generalization.
    \item LR is unsuitable for datasets where synthetic oversampling significantly alters density.
\end{itemize}

% ============================================================
% Naive Bayes
% ============================================================
\section*{Findings from Na\"ive Bayes}

Naive Bayes (NB) was also evaluated under scaled, SMOTE-balanced, and GAN-balanced conditions.

\subsection*{Scaled Data}
NB generated a lot of false positives due to its exceptionally high recall (0.9149) and relatively low precision (0.0328).  
Similar recall-heavy behavior was seen in cross-validation data.

\paragraph{Interpretation}
NB overstated the minority likelihood and produced excessive fraud predictions as a result of feature dependence violations and overlapping PCA distributions.

\subsection*{SMOTE-Balanced Data}
SMOTE improved recall, but precision was still quite poor (0.0409).  
Although cross-validation results looked great, they were unable to generalize.

\paragraph{Interpretation}
NB overfit the synthetic SMOTE samples, which cluster tightly and distort the real distribution.

\subsection*{GAN-Balanced Data}
Performance collapsed entirely (ROC--AUC = 0.0486), with zero recall and precision on real data.

\paragraph{Interpretation}
GAN-produced samples were highly incompatible with NB’s distributional assumptions.

\subsection*{Summary}
Oversensitivity to overlapping distributions and artificial samples made NB unreliable under all circumstances.

% ============================================================
% Decision Tree
% ============================================================
\section*{Findings from Decision Tree Classifier}

Both scaled and SMOTE-balanced data were used to train decision trees.  
Because of severe overfitting in the early stages of development, a GAN model was purposefully avoided.

\subsection*{Scaled Data}
With reasonable recall (0.6383) and precision (0.4839), the scaled DT produced ROC--AUC = 0.8187.  
With a training ROC--AUC of 1.00, there was significant overfitting.

\paragraph{Interpretation}
Because of deep tree development (depth = 16), the model learned training structures and became unstable when the distribution shifted.

\subsection*{SMOTE-Balanced Data}
Precision fell precipitously (0.2244), but recall increased (0.7447).  
    Once more, cross-validation produced scores that were deceptively perfect.

\paragraph{Interpretation}
SMOTE amplified minority noise, worsening generalization of tree splits.

\subsection*{Summary}
\begin{itemize}
    \item Decision Trees are highly prone to overfitting in fraud detection tasks.
    \item Synthetic oversampling magnifies instability.
    \item DTs perform best as base learners within ensemble methods.
\end{itemize}

% ============================================================
% Random Forest Findings
% ============================================================
\section*{Findings from Random Forest}

All linear and probabilistic models were surpassed by Random Forest (RF).  
Validation F1 was 0.87, with competitive recall (0.79) and outstanding precision (0.97).  
Metrics from cross-validation were consistent but somewhat lower.

\paragraph{Interpretation}
Recall decreased by 18\% from cross-validation to actual validation data, indicating modest overfitting despite RF's great discriminative capacity.

% ============================================================
% XGBoost Findings
% ============================================================
\section*{Findings from XGBoost Classifier}

With F1 = 0.85 and ROC--AUC = 0.98, XGBoost outperformed Random Forest.  
Both recall (0.81) and precision (0.906) continued to be high.  
Though not as bad as DT or NB, moderate overfitting was noted.

\paragraph{Interpretation}
One of the most dependable models in this investigation was XGBoost, which profited from gradient boosting's capacity to represent nonlinear fraud patterns.

% ============================================================
% LightGBM Findings
% ============================================================
\section*{Findings from LightGBM}

LightGBM obtained strong F1 (0.7532), competitive recall (0.6170), and good precision (0.9667).  
It showed little evidence of overfitting and stayed steady throughout folds.

\paragraph{Interpretation}
LGBM was great for production settings because it was very reliable and generated the fewest false positives.

% ============================================================
% Cross-Model Comparison
% ============================================================
\section*{Cross-Model Comparison}

\begin{table}[H]
\centering
\caption{Comparison of All Models on Validation Data}
\begin{tabular}{|p{3.2cm}|c|c|c|c|p{3cm}|}
\hline
\textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{ROC-AUC} & \textbf{Remarks} \\ \hline

Logistic Regression (Scaled) & 0.7381 & 0.6596 & 0.6966 & 0.9722 & Best linear model \\ 
Naive Bayes (Scaled) & 0.0328 & 0.9149 & 0.0624 & 0.9549 & High FP rate \\ 
Decision Tree (Scaled) & 0.4839 & 0.6383 & 0.5505 & 0.8187 & Severe overfitting \\ 
Random Forest & 0.97 & 0.79 & 0.87 & 0.98 & Strong, mild overfitting \\ 
XGBoost & 0.906 & 0.81 & 0.85 & 0.98 & Strong, reliable \\ 
LightGBM & 0.9667 & 0.6170 & 0.7532 & 0.9763 & Highest precision \\ \hline

\end{tabular}
\end{table}


\paragraph{Overall Ranking (Based on F1, AUC, and Stability)}
\begin{enumerate}
    \item \textbf{XGBoost} – Best balance of precision, recall, F1, and generalization.
    \item \textbf{Random Forest} – Excellent performance but mildly overfits.
    \item \textbf{LightGBM} – Highest precision, stable, great for production.
    \item Logistic Regression (Scaled) – Reliable baseline, interpretable.
    \item Decision Tree – Unstable and high variance.
    \item Naive Bayes – Poor precision and highly unreliable.
\end{enumerate}

% ============================================================
% Recommendations
% ============================================================
\section*{Recommendations}

Based on all experiments, the following recommendations are made:

\begin{itemize}
    \item \textbf{Use XGBoost or LightGBM for real-world deployment}, owing to their 
    excellent performance, stability, and interpretability options (e.g., SHAP).
    \item \textbf{Avoid synthetic oversampling with linear and tree-based models}, as SMOTE and GAN 
    degraded generalization and distorted fraud patterns.
    \item \textbf{Use Logistic Regression only as a baseline}, not as a production model.
    \item \textbf{Avoid Naive Bayes entirely} for fraud detection due to extreme precision collapse.
    \item \textbf{Incorporate temporal validation pipelines} as fraud patterns drift over time.
    \item \textbf{Integrate cost-sensitive learning} to balance recall and false-positive costs.
    \item \textbf{Deploy ensemble or hybrid architectures} to combine strengths of RF/XGB/LGBM.
\end{itemize}

% ============================================================
% Conclusion
% ============================================================
\section*{Conclusion}

After working through the dataset and trying out different models, a few patterns gradually became obvious. The dataset itself is extremely uneven, so most of the simpler models—especially the ones that rely on clean linear boundaries or basic probability assumptions—had trouble dealing with the messy overlap in the data. They didn’t adapt well when synthetic samples were added either, which made their results even less reliable.

The tree-based models, on the other hand, kept showing up as the most dependable options. Random Forest was steady, and both XGBoost and LightGBM generally pushed ahead of everything else. XGBoost ended up performing the strongest overall, while LightGBM was surprisingly good at precision, catching fraud with fewer false alarms.

One thing I noticed throughout this process is that oversampling isn’t a magic fix. Methods like SMOTE or GAN-generated samples sometimes helped, but in many runs they actually made the models latch onto patterns that weren’t real. If they’re used at all, they need to be applied carefully and not just by default.

Putting all of this together, what really mattered was a mix of good preprocessing, thoughtful handling of the imbalance, and choosing models that can deal with complicated patterns. There’s still plenty of room to explore. It would be interesting to look at synthetic data that’s shaped more like real fraud behavior, or training methods that let different organizations learn from each other without exposing sensitive information. And since fraud changes over time, models that can adjust to those shifts would probably be far more useful in the long run.